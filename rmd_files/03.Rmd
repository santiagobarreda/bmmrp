\newpage

# Fitting Bayesian regression models with `brms`

In this chapter we're going to start to answer basic research questions with Bayesian regression models using the `brms` package in R. The model we'll use initially is not 'correct' for our data, but it is simple enough to work as an introduction to Bayesian regression models. In the next chapter we'll use `brms` to build models that are closer to 'correct' given the structure of our data. Before using a Bayesian regression model to investigate our data, we will explain what we mean by *regression model* and what specifically makes the models in this book *Bayesian*. We'll leave a discussion of the 'multilevel' aspect of our models for the following chapter. 

## What are regression models? {#c3-what-is-reg}

It's difficult to offer a precise definition for **regression** because the term is so broad, but regression models are very often models that help you understand systematic variation in the mean parameter ($\mu$) of a normal distribution. Actually, you can model variation in other parameters and use a variety of other probability distributions, but for now we will focus on models based on the normal distribution. Basically it goes like this:

* You have a variable you are interested in, $y$, which is is a vector containing $N$ observations. We can refer to any one of these observations like this $y_{[i]}$ for the $i^{th}$ observation. Although it's a bit atypical and, strictly speaking, not necessary, we're going to put the index variables associated with trial number ($i$) in brackets like this $y_{[i]}$. This is just to make it easier to identify, and to highlight the similarity to vectors in R (e.g., `mens_height[i]`).

* You assume that the random variation in your data is well described by a normal probability distribution. This is a mathematical function ($N(\mu,\sigma)$) that describes what is and is not probable based on two parameters. You also assume that the random variation in each observation is independent from the random variation in each other observation. This means, for example, that you can't really say why any two observations are above or below the mean.

* The mean of this distribution is either fixed, or varies in a systematic manner. The standard deviation of the error distribution is usually fixed, but can vary (more on this in chapter X). 

* The variation in the mean of this distribution can be understood using some other variables, and regression is a tool for modeling these relations. 

We can write our model more formally as in Equation \@ref(eq:31). This says that we expect that the tokens of the variable we are interested in are distributed according to ($\sim$) a normal distribution with parameters equal to $\mu$ and $\sigma$. Notice hat $y$ gets a subscript while $\mu$ and $\sigma$ do not. That means that we are modeling all of the observations with just these two parameters, while the value of $y$ changes for each observation based on the $i$ subscript. 

$$
\begin{equation}
y_{[i]} \sim N(\mu,\sigma)
(\#eq:31)
\end{equation}
$$

We're going to learn to read and write formal descriptions of our models. The relationship between statistical concepts and the formal notation used to represent them is very similar to the ability to play music and read musical notation. Someone who can play a song undoubtedly *knows* that song. However, in the absence of formal musical training that same person might not recognize sheet music representing the song. This person would also lack the vocabulary to discuss components of the song, and may find it difficult to learn to play new pieces. In the same way, most people have an excellent intuitive understanding of many statistical concepts (to be discussed in upcoming chapters) such as slopes, interactions, error, and so on. However, they lack the knowledge of formal statistical notation that enables a deeper understanding of and ability to generalize these concepts. As a result, learning to read/write this notation will help you describe your models efficiently, and understand the models used by other people more effectively.

Equation \@ref(eq:31) formalizes the fact that we think the *shape* of our data distribution will be like that of a normal distribution with a mean equal to $\mu$ and a standard deviation equal to $\sigma$. When you see this, $\mathcal{N}(\mu,\sigma)$, picture in your mind the shape of a normal distribution just like if you see this $y=x^2$ you may imagine a parabola. $N(\mu,\sigma)$ Really just represents that shape of the normal distribution, and the associated expectations about more and less probable outcomes. The above relationship can also be presented as in Equation \@ref(eq:32).

$$
\begin{equation}
y_{[i]} = \mu + \mathcal{N}(0,\sigma)
(\#eq:32)
\end{equation}
$$

Notice that we got rid of the $\sim$ symbol, moved $\mu$ out of the distribution function ($N()$), and that the mean of the distribution function is now 0. Under this interpretation, it is not the case that the entirety of the variable is random and unknowable. This representation breaks up our variable into two components:

1) A systematic component, the **expected value** $\mu$ of the variable $y$. 

2) A random component, the *error* $\mathcal{N}(0,\sigma)$, that causes unpredictable variation around $\mu$. 

Regression models separate observed variables into their **systematic** and **random** components. In this case, the systematic component is predictable for all observations in the data. The random component represents the *noise*, or *error* in our data, random variation around our expected values that isn't explained. This doesn't mean that it's inexplicable in general, it only means that we've structured our model in a way that doesn't let us explain it. In other words, a model like this thinks all variations from the mean is noise because it is structured in such a way that treats such deviations as noise. 

In regression models, we can try to understand variation in $\mu$ using predictor variables $\mathrm{x}$. These predictor variables co-vary (vary with) our $y$ variable, and, we think (or hope), help explain the variation in $y$. For example, in \@ref(eq:33) we're saying we think $\mu$ is equal to some combination of the three predictor variables $\mathrm{x}_{1}$, $\mathrm{x}_{2}$, and $\mathrm{x}_{3}$. For example, we might expect that the apparent height of a speaker is affected by their f0 ($\mathrm{x}_{1}$), vocal-tract length ($\mathrm{x}_{2}$), and resonance ($\mathrm{x}_{1}$). So, when we combine these predictors we think we can come up with a pretty good estimate of how tall someone will sound.

$$
\begin{equation}
\mu = \mathrm{x}_{1} + \mathrm{x}_{2} + \mathrm{x}_{3}
(\#eq:33)
\end{equation}
$$

The values of the predictor variables will vary from trial to trial, and are not fixed. In fact, often the whole point of running an experiment is to predict differences in observations based on differing predictor values. If we expect our predictors to vary from trial to trial, that means that the equation above should include $i$ subscripts indicating that the equation refers to the value of the predictors *for that trial* rather than overall. If we expect the predictors to change from trial to trial, naturally it is possible that $\mu$ may take on different values from trial to trial, and it therefore also needs an $i$ subscript. This update is reflected in equation \@ref(eq:34).  

$$
\begin{equation}
\mu_{[i]} = \mathrm{x}_{1[i]} + \mathrm{x}_{2[i]} + \mathrm{x}_{3[i]}
(\#eq:34)
\end{equation}
$$

The predicted value ($\mu_{[i]}$) for a given trial is very unlikely to be a simple combination of the predictors (as in equation \@ref(eq:34)), so that a *weighting* of the predictors will be necessary. We can use the symbol $\alpha$ for these weights as in equation \@ref(eq:35). For example, maybe $\mathrm{x}_{1}$ is twice as important as the other two predictors and so $\alpha_1=2$, while $\alpha_2=1$ and $\alpha_3=1$. Actually, maybe one predictor has a *negative* effect so that $\alpha_3=-1$. The 'weights' associated with each predictor are the **coefficients** (or parameters) of our model. Note that the weight terms ($\alpha$) do not get an $i$ subscript. This is because they do not change from trial to trial. The values of the *predictors* change from trial to trial, but the way that they are combined to produce an expected value for any given trial does not. The weights are a stable property of the model. 

$$
\begin{equation}
\mu_{[i]} = \alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]}  
(\#eq:35)
\end{equation}
$$

We can insert equation \@ref(eq:35) into equation \@ref(eq:32), which gives us equation \@ref(eq:36). At this point our model consists of an average value that has been broken up into three component parts and the random component represented by normally-distributed noise. 

$$
\begin{equation}
y_{[i]} =  (\alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]} ) + N(0,\sigma)  
(\#eq:36)
\end{equation}
$$

Often, $\varepsilon$ is used to represent the random component, the error term, as in equation \@ref(eq:37). Notice that the error term *does* get an $i$ subscript, as in $\varepsilon_{[i]}$. This is because the exact value of the error *does* change from trial to trial, even though the statistical characteristics of the error (i.e., $N(0,\sigma)$) do not. This is a typical way to express a *regression equation* or a *regression model*. **Fitting** a regression model to data basically consists of finding the 'best' values of its coefficients, $\alpha_1$, $\alpha_2$, and $\alpha_3$ and the parameter governing the error, $\sigma$, given our data and model structure. 

$$
\begin{equation}
y_{[i]} = \alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]}+ \varepsilon_{[i]}
(\#eq:37)
\end{equation}
$$

Notice that according to equation \@ref(eq:36), regression models do not require that our *data* be normally distributed, but only that the *random variation* in our data ($\varepsilon$) be normally distributed. In \@ref(eq:38), we see the sort of representation of our model that we will use in this book. This representation states the systematic and random components of our regression model separately, and it clearly and succinctly describes the structure of our model. In plain English this is:

>"We expect the average value of our variable to vary from trial to trial based on three predictors. The combination of these predictors is based on model specific coefficients that are static across trials. Our observations are expected to be randomly distributed around the mean value according to a normal distribution with a standard deviation equal to sigma".

$$
\begin{equation}
\begin{split}
\mu_{[i]} = \alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]} \\
y_{[i]} \sim \mathcal{N}(\mu,\sigma)
(\#eq:38)
\end{split}
\end{equation}
$$

## What's 'Bayesian' about these models? {#c3-whats-bayes}

In one common traditional approach to statistics, model parameters are estimated by trying to find the values that produce the largest value of a likelihood functions based on a theoretical probability distribution of interest to a particular problem (i.e., maximum-likelihood estimation,  as discussed in Chapter 2). We're not going to discuss this approach to statistical inference in any detail as there are hundreds, if not thousands, of books available on the subject (for reading on the subject see section X). Rather than dwell on these 'traditional' approaches to statistical inference, we're going to focus on what makes Bayesian inference *Bayesian*. Rather than estimating parameters using only information from likelihood functions (and data), Bayesian models estimate the **posterior probabilities** of parameters. To explain what posterior probabilities are we need to talk about joint probabilities again. We'll begin by stating something obvious: The probability of events $A$ and $B$ occurring is the same as the probability of $B$ and $A$ occurring.

$$
\begin{equation}
P(A \,\&\, B) = P(B \,\&\, A)
(\#eq:39)
\end{equation}
$$

Equation \@ref(eq:39) can be reformulated as in \@ref(eq:310) (see section \@ref(c2-joint)). 

$$
\begin{equation}
P(A|B)*P(B) = P(B|A)*P(A) 
(\#eq:310)
\end{equation}
$$

Recall that $P(A)$ and $P(B)$ are simply placeholders for the probability of some event. We can replace these with $P(y)$ and $P(\mu)$, which represent "the probability of observing your data" and "the probability of observing a certain parameter value" respectively. This is seen in equation \@ref(eq:311), which states that "the probability of the parameter and the data is the same as the probability of the data and parameter".

$$
\begin{equation}
P(\mu|y)*P(y) = P(y|\mu)*P(\mu)
(\#eq:311)
\end{equation}
$$

We can isolate $P(\mu|y)$ on the left hand side by dividing both sides by $P(y)$, resulting in equation \@ref(eq:312). When structured in this way, this is a common formulation of **Bayes theorem**, and it underlies Bayesian statistical inference. Note however, that all we did was state a basic principle of probability theory (equation \@ref(eq:39)) and then re-arrange some terms. 

$$
\begin{equation}
P(\mu|y) = \frac{P(y|\mu)*P(\mu)}{P(y)}
(\#eq:312)
\end{equation}
$$

Each of the components in \@ref(eq:312) has a name:

* $P(\mu)$ is the **prior probability** (of the parameter). This is the **a priori** probability of parameter values independent of the data $y$. This a priori expectation can come from world knowledge, previous experiments, common sense, or some combination thereof. For example, before you measure the height of adults in San Francisco, you know the average is not 4 feet and it is not 7 feet.

* $P(y|\mu)$ is the likelihood. This is the probability that the data $y$ would be observed or generated for particular values of $\mu$. The likelihood tells us the distribution of possible/credible parameter values given the data and probability model. If we assume our data is normally distributed, $P(y|\mu)$ can be thought of as the probability density of individual points given normal distributions as specified by $\mu$. [@@ are we talking about the likelihood for all the observations, i.e., the product from i to N? It seems like it, but if so, it's a product of the probability density values corresponding to the data points at each value of $\mu$]

* $P(y)$ is the **marginal probability** of the data. This is necessary to scale the numerator so that the posterior density has a total area under the curve equal to one. However, you will note that the marginal probability does not vary as a function of $\mu$. As a result, this does affect the posterior probability of different values of $\mu$. For this reason, and for computation reasons described later [@@ I don't know if we actually describe this anywhere, so please feel free to delete this bit, but it seems like it's maybe worth mentioning that you can sample from the posterior even if you don't calculate the denominator],you don't typically need to worry about the marginal probability. 

* $P(\mu|y)$ is the **posterior probability** of the parameter given the data, the probability model, and the prior. This is the **a posteriori** probability that the parameter is $\mu$ given your data $y$, the particular likelihood function you've chosen, and the prior model of the parameters. You get the posterior probability by combining the prior distribution and the likelihood, and in doing so incorporating your current observations into your prior beliefs. 

As noted above, 'traditional' models focus primarily (or exclusively) on how likely different conclusions are given your data. In contrast, Bayesian models focus on the posterior probability of different parameter values, that is on the combination of the likelihood and prior probabilities of parameters.

#### Prior probabilities

In a Bayesian model, every parameter whose value is being estimated needs a prior probability distribution to be specified. For example, imagine you are interested in estimating the mean of a set of values, $\mu$. You decide to use a Bayesian model and decide that $\mu$ will have a normal prior with a mean and standard deviation equal to $\mu_{prior}$ and $\sigma_{prior}$. To estimate this model you would need to provide fixed values for $\mu_{prior}$ and $\sigma_{prior}$, for example we could use $\mu_{prior}=3$ and $\sigma_{prior}=5$. Note that the parameters of the prior do not get prior distributions themselves. This is because we are not estimating values of $\mu_{prior}$ and $\sigma_{prior}$, but only of $\mu$.  

The use of prior probabilities is sometimes said to make Bayesian models inherently 'subjective' but this concern is a bit overblown in most cases. First, in cases where you have plenty of data (as is often the case for experiments), prior probabilities often have little to no effect on outcomes. This is because when you have many observations, the posterior probability of parameters is dominated by the likelihood (as will be discussed in \@ref(c3-posterior)). Second, a researcher will always use 'common sense' (i.e. their prior expectations) to interpret their data. For example, if a listener reported that all adult males were 90 cm tall a researcher would have to wonder if this subject understands height in centimeters or if they were carrying out the experiment in good faith. So, even when they do not explicitly assign prior probabilities to parameter values, researchers still often use their expectations to 'screen' results in a manner broadly consistent with the use of prior probabilities in Bayesian reasoning. A Bayesian model simply requires that you build your expectations into your model. It *formalizes* it, makes it *definable* and *replicable*. Finally, every model involves arbitrary decisions which can substantially affect our results so that the design of a model can never said to be strictly 'objective'. As a result,there is no particular reason to worry about the objectivity involved in establishing a prior in Bayesian modeling without also worrying about the objectivity involved in model building more generally. 

#### Posterior distributions {#c3-posterior}

The calculation of posterior distributions involves the combination of the likelihood function with the prior probability distribution and the marginal probability. The marginal probability does not affect the 'shape' of the posterior distribution, and exists to scale the posterior so that the area under the curve is equal to one (to satisfy the requirements of basic probability theory). As a result, we will focus on the combination of the likelihood and prior probabilities. 

The combination of the likelihood and prior probability distributions is straightforward conceptually: You multiply the values of the two functions at each x-axis location. This works because we are interested in the *joint* probability of the likelihood and the prior, reformulated as the product of a conditional probability (the likelihood) and a marginal probability (the prior). The resulting curve then represents the **joint density** of the two distributions. In Figure \@ref(fig:F31) several likelihoods and priors are combined, showing the effects of variations in these on posterior distributions. Each column of figure \@ref(fig:F31) differs in terms of the number of observations used to calculate the likelihood (n = 3, 10, 675), and rows differ in terms of the standard deviation of the prior probabilities of the mean with $\mu$ equal to 180 cm and $\sigma$ = 100, 15, 1. In each case, the calculation of the likelihood assumes that the data has the same standard deviation as our apparent height data (7.8 cm). In each plot, all curves have been scaled to have the same height in the figure. This is only to make the figures visually interpretable but does not affect any of the points made below. 

The different standard deviations used for the prior probabilities of $\mu$ encode different levels of prior belief regarding what heights of adult male speakers from the US. The top, middle, and bottom rows represent what can be referred to as **vague**, **weakly informative** and **informative** priors. The boundaries between these kinds of priors is fuzzy. However, they can be distinguished as follows. A vague prior provides almost no information about the expected range of plausible results, a weakly informative prior provides some information, and an informative prior provides a lot. You can think of these as three different approaches to using prior information. [@@ "almost no information", "some information", and "a lot" don't seem any less fuzzy to me, tbh]


```{r F31, echo = FALSE, fig.height = 5.5, fig.width = 8, fig.cap='Demonstration of the effect of different prior probabilities and number of observations on likelihoods on posterior distributions.', cache = TRUE}

################################################################################
### Figure 3.1
################################################################################

library (bmmb)
# load and subset experimental data
data (height_exp, package = "bmmb")
men = height_exp[height_exp$C_v=='m',]
men = men[men$R=='a',]
mens_height = men$height

par(mfcol = c(3,3), mar =c(.1,.25,.1,.25), oma = c(4,3,3,1))  

x = seq (135, 210, 0.05)
####
likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 3 ) )
prior = dnorm (x, 180, 100) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='Density',lwd=2,yaxs='i', 
      xlim = c(140, 210), ylim = c(0,1.1),xlab='',xaxt='n',cex.axis=1.3,yaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side=3,outer=FALSE,text="Observations = 3",line=1)
mtext (side=2,outer=FALSE,text=expression(paste("Prior ", sigma, " = 100")),line=1)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 3 ) )
prior = dnorm (x, 180, 15) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(140, 210), ylim = c(0,1.1),xaxt='n',cex.axis=1.3,yaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side=2,outer=FALSE,text=expression(paste("Prior ", sigma, " = 15")),line=1)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 3 ) )
prior = dnorm (x, 180, 1) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(140, 210), ylim = c(0,1.1),cex.axis=1.3,yaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side=2,outer=FALSE,text=expression(paste("Prior ", sigma, " = 1")),line=1)

####
likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 10 ) )
prior = dnorm (x, 180, 100) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='Density',lwd=2,yaxs='i', 
      xlim = c(160, 185), ylim = c(0,1.1),xlab='',yaxt='n',xaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side=3,outer=FALSE,text="Observations = 10",line=1)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 10 ) )
prior = dnorm (x, 180, 15) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(160, 185), ylim = c(0,1.1),yaxt='n',xaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 10 ) )
prior = dnorm (x, 180, 1) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(160, 185), ylim = c(0,1.1),yaxt='n',cex.axis=1.3)
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

####
likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 675 ) )
prior = dnorm (x, 180, 100) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='Density',lwd=2,yaxs='i', 
      xlim = c(170, 183), ylim = c(0,1.1),xlab='',yaxt='n',xaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side=3,outer=FALSE,text="Observations = 675",line=1)
legend (176,.9,legend=c("Prior","Likelihood","Posterior"),lwd=3,col=c(2,1,4),bty = "n",cex=1.2)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 675 ) )
prior = dnorm (x, 180, 15) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(170, 183), ylim = c(0,1.1),yaxt='n',xaxt='n')
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

likelihood = dnorm (x, mean (mens_height), sd (mens_height) / sqrt ( 675 ) )
prior = dnorm (x, 180, 1) ; posterior = likelihood * prior
plot (x, likelihood / max(likelihood), type = 'l', ylab='',lwd=2,yaxs='i', 
      xlim = c(170, 183), ylim = c(0,1.1),yaxt='n',cex.axis=1.3)
grid()
lines (x, prior / max(prior) ,lwd=2,col=2)
lines (x, posterior / max (posterior),lwd=2,col=4)

mtext (side = 1, outer = TRUE, text = "Height (cm)", line = 2.5)

```

In the top row of Figure \@ref(fig:F31) we see the influence of a *vague* or *diffuse* prior on inference. Since this distribution has a standard deviation of 100 and a mean of 180, this means that basically all heights from zero to 400 cm are plausible a priori. A prior probability this wide is only going to place very minimal constraints on the posterior probabilities. This is reflected in the top row of figure \@ref(fig:F31) where even in the case of only three observations the posterior probability almost exactly matches the likelihood. So, in the presence of very weak prior beliefs, the most credible values for your parameters a posteriori (after incorporating your data) will be dominated by the most likely values. This makes sense.

In the middle row of Figure \@ref(fig:F31) we see the influence of a *weakly informative* prior. This distribution has a standard deviation of 15 and a mean of 180. The standard deviation was set to twice the value of the standard deviation of adult male heights in the United States (cite). As a result, it places reasonable constraints on our expectations but it will not strongly influence results within that reasonable range. We see that in this case, the likelihood still dominates the posterior even when only five observations are available. 

In the bottom row of Figure \@ref(fig:F31) we see the influence of a *informative* prior. This distribution has a standard deviation of 1 and a mean of 180. A prior distribution this narrow basically says that we are *sure* that the average height is around 180 cm. This is because the prior probability of any value outside of 178-182 cm is nearly zero, meaning that we do not believe values outside this bound before collecting our data. We can see that under these conditions, the prior distribution actually can have a strong effect on the posterior, especially in cases with few observations. However, with a large enough sample size the likelihood can still come to dominate even the narrowest prior distributions. 

### Characteristics of posterior distributions

If we focus on the general characteristics of posterior distributions with respect the the characteristics of priors and likelihoods, a pattern emerges. Specifically, the posterior is a mix of the prior and likelihood which takes their relative 'variances' into account. We know (see section \@ref(c2-chars-of-likelihoods)) that the width of the likelihood function is dependent on the underlying error in the data and on the sample size. Greater error increases the width of the likelihood, since a wider spread of data will be compatible with a wider range of parameter values. On the other hand, more observations will tend to decrease the width of the likelihood, since it consists of the product of density values across all the observations in a data set. So, we see that the characteristics of the posterior distribution will depend simultaneously on the 'width' of the prior distribution, the amount of noise in your dependent variable, and the size of the sample involved in the calculation of your likelihood. 

A consequence of the 'merging' of prior information with the likelihood is that posterior distributions can be **shrunk** towards the prior. Recall from chapter 2 that the maximum likelihood estimate of a parameter can be thought of as the 'best' parameter in that it is the parameter that makes your data as probable as it can be given the model structure. In figure \@ref(fig:F31) we see that in some cases the posterior distribution is not exactly like the likelihood and has been *pulled* towards the prior. This *pull* is referred to as **shrinkage**, because it tends to *shrink* the magnitude of effects by pulling them towards the mean (which is often zero). Broadly speaking, deviations from prior expectations are maintained when there is good enough evidence for them, and shrunk when there is not. What constitutes 'enough' evidence is based on the structure of the model and the nature of the data. As seen in figure \@ref(fig:F31) a wide enough prior will not meaningfully affect estimates even for extremely small sample sizes. You may wonder, is shrinkage a good thing? It turns out that shrinkage can help models arrive at more reliable parameter estimates by reducing weakly supported values that deviate substantially from prior expectations (cite). This will be discussed further in the following chapter.

[@@ Is it *shrinkage* with just a prior and likelihood? I guess I tend to think of shrinkage only in the context of multilevel models, where partial pooling "shrinks" estimates toward a mean parameter being estimated - I'm kind of just thinking out loud here, and the more I do, the more I think I'm okay with this use of "shrinkage" ... anyway, let's talk about it]

## Sampling from the posterior using STAN and `brms` {#c3-sampling}

We want to understand the posterior distribution of parameters. How do we get this information? For very simple models, posterior distributions can be derived **analytically**, that is by finding exact solutions to a series of equations. However, for more complicated models, such as the ones we'll discuss in this book, it can be difficult (if not impossible) to understand the characteristics of the posterior distributions of parameters in this way. As a result, these questions are answered **numerically** using software that samples from posterior distributions using one or more algorithms designed to do so as accurately and efficiently as possible. Given enough samples from the posterior distribution, we can estimate the properties of the distribution (just as we could by sampling any other variable).

Many different software approaches to sampling from posterior distributions have been developed through the years including *winBUGS*, *JAGS*, *PyMC*, and *STAN*. The software used in this book is **STAN**. We use this because it is (relatively) fast, reliable, extremely flexible, and widely adopted. However, the modeling and statistical principles explained in this book apply generally to all Bayesian models regardless of the software used to fit them, and the general concepts extend to linear modeling more generally. 

One downside with working with STAN directly is that you need to write your own models. This is not too difficult, but it is not particularly *easy* either, and it can be a bit time consuming, especially for complicated models. In this book we will rely on the `brms` package to use STAN. `brms` effectively simplifies the use of STAN by making the specification of highly-efficient models very simple, and providing us with a great deal of flexibility in doing so. It also includes many helper functions that make working with Bayesian models very convenient. So, even though we will use `brms` for simplicity, everything we discuss in this book could be used to directly write your own models for STAN (or any other statistical software). 

In order to sample from the posterior distribution using software like STAN, the user provides data and a description of a model which specifies: 

  * The relations between the variables in the data. For example, what is the dependent variable? What are the independent variables? How do these relate? 
  
  * The nature of random variation in the model. To this point we have only discussed single sources of normally-distributed noise in our models. 
  
  * Prior distributions for all estimated parameters.

Given this information, STAN samples from the posterior distribution and returns arrays containing these samples, rather than looking for the single 'best' estimate. For each parameter, STAN does a more-complicated version of the following algorithm:

1) Pick a random value for the parameter (i.e., $\mu_{tmp}$ = 173 cm).

2) Calculate the posterior probability for the current estimate of $P(\mu_{tmp}|y)$.

3) If the posterior estimate meets some criteria (e.g., it is better than the last one, it is not too low, etc.), then the value of $\mu_{tmp}$ is recorded, and becomes $\mu_{estimate}$. If not it is just discarded.

4) Go back to step 1. 

[@@ how confident are you that this is actually what STAN does? I know that this is a pretty reasonable description of the Metropolis algorithm, but Hamiltonian Monte Carlo is, as far as I know, *way* more complicated that this, and STAN has a number of fancy things built on top of plain Hamiltonian Monte Carlo...]

The result of this process is a **chain** of estimates of $\mu_{estimate}$, and any other parameter you are estimating in your model. This *chain* is a vector containing the sequence of samples from the posterior distribution of $\mu_{estimate}$, which together tell us about the characteristics of the parameter it represents. Incredibly, under a very reasonable set of conditions the process below will result in a distribution of $\mu_{estimate}$ that will converge on the posterior distribution of $\mu$ given your data and model structure (including prior probabilities). 

## Estimating a single mean with the `brms` package

### Data and Research Questions

We're going to use the same experimental data we looked at last chapter: The height judgments collected for the adult male speakers in our experiment. For more information on the experiment, see section \@ref(c1-methods). Below we load the book package and subset our experimental data to only include those rows involving adult male speakers. In addition, we will focus only on the natural productions (the actual resonance, `men$R=='a'`), excluding those trials involving the manipulated 'big' resonance level.

```{r echo=FALSE, warning=FALSE,message=FALSE}
# load book package and brms
library (bmmb)
library (brms)

# load and subset experimental data
data (height_exp)
men = height_exp[height_exp$C_v=='m',]
men = men[men$R=='a',]
mens_height = men$height
```

We're going to revisit the research questions posed at the beginning of Chapter 2:

  Q1) How tall does the average adult male from the US sound?

  Q2) Can we set limits on credible average apparent heights based on the data we collected?

However, this time we are going to approach these questions using a Bayesian regression model using `brms` (and STAN).

### Description of the model

We're beginning with a model that treats all of our data as random deviations drawn from a single, undifferentiated normal distribution. Our model for a single group of normally distributed values can be thought of in several different ways. In \@ref(eq:313a) the value of your dependent variable for any given trial ($y_{[i]}$) is thought of as being a normally-distributed variable with a trial-specific mean of $\mu_i$, and a fixed standard deviation $\sigma$.

[@@ it seems confusing to me to talk about a "single, undifferentiated normal distribution" and then immediately show a model with $\mu$ indexed by trial]


$$
\begin{equation}
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma)
(\#eq:313a)
\end{equation}
$$

We can also think of this model as in \@ref(eq:313b), which says that your dependent variable is the sum of some of some average *expected value* for that trial, ($\mu_{[i]}$) and some specific random error for that trial ($\varepsilon_{[i]}$). The random error is expected to be normally distributed with a mean of 0 and some unknown standard deviation (as in: $\varepsilon_{[i]} \sim N(0,\sigma)$). 


$$
\begin{equation}
y_{[i]} = \mu_{[i]} + \varepsilon_{[i]}
(\#eq:313b)
\end{equation}
$$

In general, we use regression models to understand orderly variation in $\mu_{[i]}$ from trial to trial by breaking it up into predictors ($\mathrm{x}_{1}, \mathrm{x}_{2},...$) that are combined using based on weights as determined by the model coefficients ($\alpha_1, \alpha_2,...$). However, in this case we expect the value of $\mu_{[i]}$ to actually be equal for all trials. When we are only trying to estimate a single average, we don't have any predictors to explain variation in $\mu_{[i]}$. In fact, our model structure suggests we expect no variation in $\mu_{[i]}$ from trial to trial. However, mathematically we can't just say 'we have no predictor' since everything needs to be represented by a number. As a result, we use a single 'predictor' $\mathrm{x}$ with a value of 1 so that our regression equation is as in \@ref(eq:314)). Now, our model is trying to guess the value of a single coefficient ($\alpha_1$), and we expect this coefficient to be equal to $\mu_{[i]}$ since it is being multiplied by a 'predictor' with a constant value of 1.
 

$$
\begin{equation}
\mu_{[i]} = \alpha_1*1
(\#eq:314)
\end{equation}
$$

This kind of model is called an **Intercept only** model. Regression models are really about representing *differences*, differences between groups and across conditions. When you are encoding differences, you need an overall reference point. For example, saying that something is '5 miles north' is only interpretable given some reference point. The 'reference point' used by your model is called your 'Intercept', and it is the center of your model's universe. At this point our model consists *only* of a single reference point, and the $\alpha_1$ parameter reflects its value (as shown in Equation \@ref(eq:314)). As a result, the $\alpha_1$ coefficient is called the 'Intercept' in our model. When a coefficient is just being multiplied by a 'fake' predictor that just equals one, we can omit it from the regression model (but its still secretly there). So, our model investigating the apparent heights of adult males from Michigan can be formalized like this:

$$
\begin{equation}
\begin{split}
height_{[i]} \sim N(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = Intercept \\
\end{split}
(\#eq:315)
\end{equation}
$$

Put in plain English, each line in the model says the following:

* We expect that apparent height for a given observation $i$ is normally distributed according to some trial-specific expected mean value and some unknown (but fixed) standard deviation.

* The expected value for any given trial ($\mu_{[i]}$) is equal to the intercept of the model for all trials. This means its fixed and we have the same expected value for all tokens.

The model also implicitly says that the error, the random variation around $\mu$, is drawn from a normal distribution with a mean of 0 and a standard deviation of $\sigma$. This distribution represents all deviations in apparent height around the mean apparent height for the sample ($\mu_{[i]}$). In other words, the error for this model is expected to look like:

$$
\begin{equation}
\varepsilon_{[i]} \sim N(0,\sigma)
(\#eq:316)
\end{equation}
$$

We can rearrange the terms in (\#eq:313b) to isolate the random term on the left side. When we do this, we see that **error** is what we call the difference between the value of an observation and the expected value for that observation. 

$$
\begin{equation}
\varepsilon_{[i]} = y_{[i]} - \mu_{[i]} 
(\#eq:317)
\end{equation}
$$

In practice, you never know the true expected value, the *real* exact parameter for whatever distribution you are working with. Instead, you work with an estimate of the predicted value $\hat{\mu}$. As a consequence, you do not have access to the exact errors but instead to estimated errors $\hat{\varepsilon}$. Estimated errors are called **residuals**. 

$$
\begin{equation}
\hat{\varepsilon}_{[i]} = y_{[i]} - \hat{\mu}_{[i]}
(\#eq:318)
\end{equation}
$$

As noted in section \@ref(c3-what-is-reg), regression models assume that the random error, the unpredictable deviations about the expected value across trials, are independent and identically distributed. We can now be more specific and say that we expect that our model *residuals* to be independent of each other. This assumption is obviously violated for this data since we have multiple observations from each listener, each of which had their own tendencies (as discussed in section \@ref(c2-conditional)). For this reason, we can say that this model is 'wrong'; it is built in such a way that we know it is not a good fit for our data. We will discuss this, and the problems it causes, in the following chapter.

### The model formula

Model structures are expressed in R using a very specific syntax. Think of writing a model formula as a sub-language within R. Generally, model formulas in R have the form:

`y ~ predictors`

The variable we are interested in understanding ($y$) goes on the left hand side of the $\sim$, and on our predictors go on the right hand side. Notice that the random term ($\varepsilon$) is not included in the model formula. The formula above can be read as 'y is distributed according to some predictor', which really means "we think there is systematic variation in our y variable that can be understood by considering its joint variation with our predictor variable(s)."

For intercept only models, the number `1` is included in the model formula to indicate that a single constant value is being estimated (as in \@ref(eq:314)). As a result, our model formula will have the form seen below. This model could be said out loud like "we are trying to estimate the mean height" or "we are predicting mean height given only an intercept".

`height ~ 1`

### Fitting the model: Calling the `brm` function

The `brms` package contains the `brm` function, which we will use to fit our models. The `brm` function takes a model specification, data, and some other information, and fits a model that estimates all the model parameters. Unless otherwise specified, `brm` assumes that the error component ($\varepsilon$) of your model is normally distributed. The first argument in the function call is the model formula, and the second argument tells the function where to find the data (a dataframe called `men`). The other arguments tell the function to estimate a single set of samples (chains = 1) using a single processor on your CPU (cores = 1). These arguments will be discussed in more detail later.

```{r, warning = FALSE, cache = TRUE, collapse = TRUE, eval = FALSE}
# To ensure predictable results in examples, we will using the same random  
# seed throughout, and resetting it before model fitting.  
set.seed (1)
model = brms::brm (height ~ 1, data = men, chains = 1, cores = 1)

## Compiling Stan program...
## Start sampling
## 
## SAMPLING FOR MODEL '03859e54349182b6cd9cd51aa7ca25d3' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.103 seconds (Warm-up)
## Chain 1:                0.057 seconds (Sampling)
## Chain 1:                0.16 seconds (Total)

```
```{r, include = FALSE}
model = readRDS ('../models/3_model.RDS')
```

By default, `brms` takes 2000 samples, throwing out the first 1000 samples and returning the last 1000. The first 1000 samples are the **warmup**, the time the model uses to find appropriate parameter values for the model [@@ I think warmup in STAN is (also?) used to tune the sampling algorithm]. The output above shows you that the sampler is working, and tells you about the progress as it works. This is a small amount of data and a simple model so it should be pretty fast. This is the only time we will be actually fitting a model in the code chunks. For all of the models discussed in this book, you can fit any of these models yourself locally using the code provided, or you can download them directly from the book GitHub repo using the code below. 

```{r, eval = FALSE}
# Download it from the GitHub page:
model = bmmb::get_model ('3_model.RDS')
```

### Interpreting the model: the print statement

Typing the model name into the console and hitting enter prints the default `brms` model print statement:

```{r, collapse = TRUE, eval = FALSE}
# inspect model
model
```

The first part provides you with some basic information and part tells you some technical details that we don't have to worry about for now (though some are obvious).

```{r}
## Family: gaussian 
##  Links: mu = identity; sigma = identity 
##Formula: height ~ 1 
##   Data: men (Number of observations: 675) 
##  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;
##         total post-warmup draws = 1000
```

Next we see estimated effects for our predictors, in this case only an intercept. This is an estimated **population-level effect** because it is shared by all observations in our sample, and it is not specific to any particular subset of observations.

```{r}
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   173.80      0.31   173.20   174.38 1.00     1038      598
```

The information above provides the mean (Estimate) and standard deviation (Est. Error) of the posterior distribution of $\mu$ (`Intercept`). The values of `l-95% CI` and `u-95% CI` represent the upper and lower 95% **credible interval** of the posterior distribution. An $x\%$ credible interval of a parameter is an interval such that the parameter has an $x\%$ chance ($0.x$ probability) of falling inside the interval. `brm` calculates credible intervals using quantiles so that the `l-95% CI` and `u-95% CI` represent 2.5% and 97.5% quantiles of the posterior samples of a parameter. Based on its 95% credible interval, we see that there is a 95% probability that $\mu$ is between 173.2 and 174.4 cm given our data and model structure. Our model also provides us an estimate of the error standard deviation($\sigma$), under 'Family Specific Parameters: sigma'. This estimate closely matches our sample standard deviation estimate (`sd(mens_height)`) of 7.76 cm. 

In addition, we also get a 95% credible interval for this parameter (2.5% = 7.38, 97.5% = 8.21). Although our focus is often on estimation mean parameters, it is very imprtant to keep in mind that our model in \@ref(eq:315) involves the estimations of *two* parameters, $\mu$ and $\sigma$.

```{r}
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     7.78      0.22     7.38     8.21 1.00     1060      736
```

This last section is just boilerplate and contains some basic reminders which will generally always look the same.

```{r}
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

### Seeing the samples

In section \@ref(c3-sampling) we discussed that Bayesian modeling software (like STAN) takes *samples* of the posterior distributions of parameters given the data and model structure. It is helpful to see that our model is really just a series of **posterior samples**. Compact description of our models, such as the one in the print described above, are just summaries of the information contained in the posterior samples. Below we get the posterior samples from the model we fit above, in the form of a matrix. As expected, we have 1000 samples of each parameter. The first column represents the model intercept (`b_Intercept`), the middle column is the error (`sigma`). The third column (`lp__`) is the log posterior density, to be discussed in section X. 

```{r, collapse = TRUE}
# get posterior samples from model
samples = brms::as_draws_matrix (model)
head (samples)
```

We can plot the individual samples for the mean parameter on the left in figure \@ref(fig:F32), and on the right we can see a histogram of the samples. 

```{r F32, fig.width = 8, fig.height = 3.5, fig.cap = "(left) Individual samples from posterior distribution of the model intercept paramter. (right) A histogram of the samples on the left. The curve shows a normal distribution with a mean of 173.8 and a standard deviation of 0.31.", echo = FALSE}

################################################################################
### Figure 3.2
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot (samples[,1], xlab = 'Sample number',ylab = 'Height (cm)',col=teal,pch=16)
abline (h = mean (samples[,1]), lwd=4,col=yellow)
hist (samples[,1], freq = FALSE, breaks = 15,main='',xlab='Height (cm)',col=maroon,ylim=c(0,1.3))
curve (dnorm (x, 173.80, 0.31), xlim = c(172,175), lwd=4,add=TRUE,col=darkorange)
```

Recall that our model output provides information about 95% credible intervals for the mean parameter:

```{r, eval = FALSE}
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   173.80      0.31   173.20   174.38 1.00     1038      598
```

We know that these simply correspond to the 2.5% and 97.5% quantiles of the posterior samples. We can confirm this by checking the quantiles on the vector containing our posterior samples and see that these exactly correspond to the values of `Estimate`, `l-95% CI`, and `u-95% CI` in the model print statement above. 

```{r, collapse = TRUE}
quantile (samples[,"b_Intercept"], c(.025, .975))
```

One of the great things about Bayesian models is that you can make your own summaries of the posterior samples, summarize them in several ways as required, and ask different questions easily. For example, there is no special status for the 2.5 and 97.5% quantiles, and we can easily check the values of other ones:

```{r, collapse = TRUE}
quantile (samples[,"b_Intercept"], c(.25, .75))
```

We can also use the posterior distribution to find the probability that the mean parameter is over/under any arbitrary value:

```{r, collapse = TRUE}
mean (samples[,"b_Intercept"] < 174)
```

Let's take a second to think about why this works. Recall that the probability is the odds that something will occur, relative to all other outcomes. Our vector `samples[,"b_Intercept"]` represents 1000 observations of a random variable, 1000 possible values of the average apparent height of adult males from the US. If we find the total number of these observations that were below 174 cm and then divide by the total number of observations (1000), we are calculating the probability of observing a mean estimate below 174 cm. As a result, the calculation above says that there is a 0.75 probability (a 75% chance) that the mean apparent height of adult male speakers in this population is under 174 cm, given our data and model structure. We come to this conclusion by finding that 75% of the posterior samples of the parameter of interest are below 174 cm.

### Getting the residuals

We can get the model residuals using the `residuals` function. By default it returns a data frame where one row corresponds to each observation in your data, and the different columns provide you information about the estimate. You will notice that you get credible intervals for each of the estimated residuals. This is because there is one prediction for each set of posterior samples. Since we have 1000 posterior samples that means we have 1000 slightly different predicted values for each observation and therefore 1000 slightly different estimated errors for each observation. By default the distribution of residuals is presented, but you can get the individual estimates themselves by calling `residuals(model,summary=FALSE)`.

```{r, collapse = TRUE}
model_residuals = residuals (model)
head (model_residuals)
```

In the left panel of figure \@ref(fig:F32.5) we show a histogram of our residuals and compare this to the standard deviation estimated for the error distribution in our model (`sigma = 7.78`). It is no surprise that these match because this is precisely what the sigma parameter in our model is an estimate of, the standard deviation of the residuals. Since our model consists of only an intercept, a single expected value ($\mu$) for all instances of the variable, all variation around the mean constitutes error. We can compare this by comparing our residual estimates to the centered data (right panel of figure \@ref(fig:F32.5)) and seeing that these are basically the same.

```{r F32.5, fig.height = 3, fig.width = 8, fig.cap='(left) Histogram of the residuals of our model. (right) A comparison of our residuals and centered height judgments shows that these are the same thing.', echo = TRUE}

par (mfrow = c(1,2), mar = c(4,4,1,1))
hist(model_residuals[,1],main="", col = bmmb::cols[13], freq=FALSE)
curve (dnorm (x,0,7.8), xlim=c(-40,25),add=TRUE,lwd=2,col=deepgreen)

centered_height = mens_height - mean(mens_height)
plot (model_residuals[,1], centered_height,lwd=2,col=4,cex=1.5)
abline (0,1,col=2,lwd=2)
```

## Checking model convergence

Our parameter estimates are based of a set of samples from the posterior distribution of a parameter. As with any other inference based on samples, our parameter estimates will be unreliable if we don't have enough samples, or if our samples do not represent the population we are trying to understand. For this reason, it's important to look at the **ESS** values (the *effective sample size*), and the **Rhat** values provided for `brm` model print statements. ESS tells you approximately how many *independent* samples you have taken from the posterior. Bulk ESS tells you how many samples the sampler took in the 'thick' part of the density, and Tail ESS reflects how much time the sampler spent in the 'thin' part, in the tails of the distribution. Rhat tells you about whether your 'chains' have converged (more on this later). As noted in the 'boilerplate' at the end of the `brm` model print statement, values of Rhat near 1 are good, and values higher than around 1.1 are a bad sign. Ideally we would like several hundred samples (at least) for mean estimates, and thousands to be confident in the 95% confidence intervals. You may sometimes fit a model and get a warning message like this:

```{r}
## Warning messages:
## 1: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and  
## medians may be unreliable. Running the chains for more iterations may help. See:
## http://mc-stan.org/misc/warnings.html#bulk-ess
## 2: Tail Effective Samples Size (ESS) is too low, indicating posterior variances 
## and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess
```

That is `brms` telling you that you need to collect more samples in order to be confident in your parameter estimates. To get more samples we can run the model longer, or we can use more **chains**. A chain is a separate set of samples for your parameter values. A model can be fit in parallel across several cores resulting in several independent chains. Since these chains are all supposed to be sampling from the same posterior distribution, their samples can be merged across chains after sampling. There is a fixed number of samples a single core of your computer can take in a fixed amount of time. When you do this across $N$ cores, you can get (approximately) $N$ times as many samples in the same amount of time. Since many computers these days have 4-8 (or more) cores, we can take advantage of parallel processing to fit models faster. Before fitting a model across multiple cores, you should confirm how many you have. You can use the following command (you may need to install the `parallel` package):

```{r, eval = FALSE}
parallel::detectCores()
```

The example code throughout this book will use four cores to fit models. If you only have four total cores on your computer, you should change the models to use 2-3 chains and cores so that your computer can take care of other necessary functions while you fit your model(s). One thing to keep in mind is that these models can be computationally intensive to fit. As the data sets become larger and the models become more complicated, more powerful computers are needed in order to fit a model in a reasonable amount of time. Below, we re-fit our initial model but run it on four chains, and on four cores at once.

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
model_multicore =  
  brms::brm (height ~ 1, data = men, chains = 4, cores = 4)
```
```{r, eval = FALSE}
# Or download it from the GitHub page:
model_multicore = bmmb::get_model ('3_model_multicore.RDS')
```
```{r, include = FALSE}
model_multicore = readRDS ('../models/3_model_multicore.RDS')
```

We can print the model below, and can see that using four chains has substantially increased our ESS, without taking up much more computing time. Towards the top of our print statement we see that `4 chains` have collected `total post-warmup samples = 4000`. This means our model has 4000 samples for every parameter in the model. However, for some parameter we have only about 3000 'effective samples'. This means some of our samples are basically dead weight, taking up space and slowing down future computations for no good reason. The discrepancy between the number of samples and the 'effective' number of samples is due to something called **autocorrelation**, the self-similarity of nearby observations in a series of observations. Sometimes consecutive samples can be too similar and so don't given you that much *independent* information. When this happens you end up with less information about a parameter than you might think based on the number of samples you have. Think of it like measuring the temperature of a place to get an idea of its average annual weather. Measurements need to be well separated in order to be really independent. If you were to measure the temperature every 5 minutes these measurements would have a high autocorrelation, and would not give you a good impression of the range of temperatures that place tends to experience in a calendar year.

```{r, collapse = TRUE}
# inspect model
model_multicore
```

One way to increase the ESS without increasing the total final sample size is to run longer chains and keep only every $Nth$ one. This strategy is called **thinning**. This lets your models be smaller while containing approximately the same information. To do this you have to change the `iter`, `warmup` and `thin` parameters when you fit your model. Default behavior is that the models you fit keep every sample after the `warmup` is done, up to the `iter` maximum. So if `iter=3000` and `warmup=1000` you will end up with 2000 samples. After this, you keep only one every `thin` samples. Basically, you will end up with $(iter-warmup) / thin$ samples per chain. If you are doing this across `Ncores` cores, then you will end up with $((iter-warmup) / thin) \times Ncores$ samples in total. Below, we ask for 3000 samples per chain. Since the warmup is 1000 this means we will keep 2000 post warm-up. However, since `thin=2`, we will keep only 1000 of these. Finally, since we are fitting the model on four cores, we will end up with 5000 samples in total (i.e. $((3000-1000) / 2) \times 4$). 

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
model_thinned =  
  brms::brm (height ~ 1, data = men, chains = 4, cores = 4,
       warmup = 1000, iter = 3000, thin = 2)
```

```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_thinned = bmmb::get_model ('3_model_thinned.RDS')
```
```{r, include = FALSE}
model_thinned = readRDS ('../models/3_model_thinned.RDS')
```

We inspect the model print statement and see that despite having the same number of samples as the `model_multicore`, the ESS for this model is higher than for the previous model, in particular for the `sigma` parameter.

```{r, collapse = TRUE}
# inspect model
model_thinned
```

## Specifying prior probabilities

In section \@ref(c3-whats-bayes) we mentioned that in Bayesian models all estimated parameters *must* have prior probability distributions specified for them. And yet, to this point we've been fitting models without explicitly specifying prior probability distributions for the parameters. It turns out that if you don't specify prior probabilities for your parameters, `brm` will use its own default priors for parameters given the characteristics of your data. We can use the function `get_prior` in `brms` to see what the default priors are for our model, and to see which parameters in our model require priors. Of course, we should know this based on the structure of our model but this method is useful to help verify our expectations. 

Below we can see that our model requires priors for our two estimated parameters, the `Intercept` ($\mu$) and `sigma` ($\sigma$) parameters, and that these have been given `default` values. The default values use a t distribution (`student_t()`), which we will discuss in section X.  

```{r}
brms::get_prior (height ~ 1, data = men)
```

`brms` makes it easy to specify prior probabilities for specific parameters or whole 'classes' of parameters. Setting priors for entire classes of parameters is faster for you and makes the model run faster. Right now, our model only includes the following classes of parameters:

-   `Intercept`: this is a unique class, only for intercepts.
-   `sigma`: this is for the standard deviation of our error parameters. Our model only has one for now, `sigma` ($\sigma_{error}$), but it will have more later.

We're going to set *weakly informative* prior probabilities for our parameters. To set these you have to use what you know about your variables and the world in general. Since we know that the average male over 20 in the US is 176 cm tall (cite@@), this seems like a reasonable prior expectation for how tall the adult males in our sample will sound. We also know that the standard deviation of adult male heights in the US is 7.5 cm, and will double this for our priors. This is to account for that fact that there may be more variation in how tall people 'sound' compared to how tall they *are*. The code to set the priors for our model looks like this:

```{r, eval = FALSE}
  prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
            brms::set_prior("normal(0, 15)", class = "sigma"))
```

The code above tells our model to use a normal distribution with a mean of 176 and a standard deviation of 15 (`normal(176, 15)`) for the prior distribution of the Intercept. Around 90-95% of the mass of normal distributions is within two standard deviations of the mean. This means that we are saying that we expect, a priori, that the intercept should be between around 146 (176-15x2) and 206 Hz (176+15x2). This is too broad, but at least places the supposed outcomes within reasonable human ranges. The random error, `sigma`, was given a prior with a normal distribution with a mean of 0 and a standard deviation of 15 (`normal(0, 15)`). This is likely an overestimation of the magnitude of the random error in this data. However, it is likely to be in the ballpark. Our prior specifies a normal distribution centered at 0 for the standard deviation. Since standard deviations, like variances, can only be positive the sampler (STAN) used by `brm` ignores the negative half and uses only the positive half of the prior distribution. This prior basically says that we expect the average variation around the mean to be less than 30 cm, which it is very likely to be. 

The left panel in figure \@ref(fig:F33) compares the normal distribution we used (blue line) to a histogram of our height judgments. As we can see, the prior distribution we used for the intercept is much broader (more vague) than the data distribution so that it will have basically no effect on our results (but will help our model fit properly). The right panel compares the prior for the standard deviation parameters to the absolute value of the centered apparent heights. This presentation shows how far each observation is from the mean apparent height (at 174 cm), and again we see that most of these deviations are in the thicker part of the prior density. As a result, neither of these priors is going to have much of an effect on our parameter estimates given the size of our sample (see figure \@ref(fig:F31).

```{r F33, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = "(left) A comparison of the densities of the prior for our intercept, compared to a histogram of our height judgments for male speakers. (right) The distribution of absolute deviations from the mean height judgment, compared to the prior distribution for the error parameter in our model."}

################################################################################
### Figure 3.3
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))

## plot prior for mean
hist (mens_height, xlim = c(140,200), freq = FALSE, col = deepgreen, ylab = 'Density', xlab = 'Heigh (cm)',
      ylim = c(0,0.08), main="")
curve (dnorm(x,176,15),lwd = 2, col = 4, add = TRUE)

## plot prior for standard deviations
hist (abs (mens_height - mean (mens_height)), freq = FALSE, col = yellow, main="",
      ylab = 'Density', xlab = 'Heigh (cm)',ylim = c(0,0.12), xlim = c(0,50))
curve (dnorm(x,0,15),lwd = 2, col = 4, add = TRUE)

```

We can update the description of our model to include the specification of prior distributions for each estimated parameter, as in \@ref(eq:317). In the future, our model descriptions will always include these. Our model specification now makes it clear: We expect height judgments to be normally distributed, we expect the mean to always equal the intercept, and we have specific distributions in mind for all estimated model parameters (i.e., $\mu, \sigma$). 

$$
\begin{equation}
\begin{split}
\\
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} \\
\\
\textrm{Priors:} \\
\mathrm{Intercept} \sim N(176, 15) \\
\sigma \sim N(0, 15) \\ 
\end{split}
(\#eq:317)
\end{equation}
$$

We can fit this new model below using the new lines which specify the prior distributions for our parameters. 

```{r, eval = FALSE}
# Fit the model yourself, or
set.seed (1)
model_priors =  
  brms::brm (height ~ 1, data = men, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))
```

```{r, include = FALSE, eval = FALSE}
model_priors = bmmb::get_model ('3_model_priors.RDS')
```
```{r, include = FALSE}
# load downloaded model from working directory
model_priors = readRDS ('../models/3_model_priors.RDS')
```

We can use the `short_summary` function in the `bmmb` package to get a 'smaller' version of the model print statements. These shorter versions are not a replacement for the complete statement as they omit important information about model fit. However, they do help us to compare models while minimizing redundant information in this text, thereby making a more efficient use of space on the page. If we compare the output of `model_thinned`:

```{r, collapse = TRUE}
# inspect model
bmmb::short_summary (model_thinned)
```

To that of the model where we specified wider priors (`model_priors`), we see that there is no noticeable effect on our results. This is because the prior matters less and less when you have a lot of data, and because we have set wide priors that are appropriate (but vague) given our data. Although the priors may not matter much for models as simple as these, they can be very important when working with more complex data, and are a necessary component of Bayesian modeling.

```{r}
bmmb::short_summary (model_priors)
```

## The log posterior density (`lp`) {#c3-log-posterior}

When we get our model samples with the `as_draws_matrix` function, in addition to our mean and standard deviation estimates we get an estimate of something called `lp__`. This is a measure of the (unnormalized) **log posterior density**, basically, the overall posterior probability of your model parameters given your data. 

```{r}
samples = data.frame (brms::as_draws_matrix (model_priors))
head (samples)
```

In \@ref(fig:312) we presented the posterior probability of a $\mu$ parameter given your data and priors. We can generalize this to encompass all of our model parameters, represented by $\theta$ in \@ref(fig:312). Now, \@ref(fig:318) represents the posterior probability of all our model parameters given our data.

$$
\begin{equation}
P(\theta|y) = \frac{P(y|\theta)*P(\theta)}{P(y)}
(\#eq:318)
\end{equation}
$$

The *unnormalized* posterior density ignores the marginal distribution ($P(y)$) and so returns a density that is proportional to the posterior density up to some constant, as in \@ref(fig:319). By saying that it is defined up to a proportional constant, this is what we mean. We cannot, or do not bother, calculating the exact posterior density. However, we can calculate some value, let's say $x$, where the true posterior density is equal to $x \times C$ where $C$ is some unknown constant. When we model the unnormalized posterior density we take this approach, we try to estimate $x$ ($P(\theta|y)$) while ignoring $C$ ($1/P(y)$).

$$
\begin{equation}
P(\theta|y) \propto P(y|\theta)*P(\theta)
(\#eq:319)
\end{equation}
$$

If we log-transform both sides of \@ref(fig:319), we get the log-posterior density, shown in \@ref(fig:320).

$$
\begin{equation}
log (P(\theta|y)) = log (P(y|\theta)) + log(P(\theta))
(\#eq:320)
\end{equation}
$$

Let's pause to think about what \@ref(fig:319) means and why it is a useful measure. First, let's talk about $P(y|\theta)$ which is the model likelihood: The joint probability of the data given the values of the parameters. For example, we can think about the probability of observing each of the observations in our height data given the posterior means estimates of $\mu$ and $\sigma$ provided by `model_priors`. Below, we see the values of the probability density above the first six observations, and then the logarithm of these values below.

[@@ here is where `height` is invoked, but apparently undefined, at least in this chapter, or maybe it's just because I'm trying to knit using plain on knitting]

```{r, collapse = TRUE}
head (dnorm (mens_height, 173.8, 7.78))

head (dnorm (mens_height, 173.8, 7.78, log = TRUE))
```

To find the joint probability of the data given the model parameters, we can sum the log-densities as seen below. This provides us an estimate of the likelihood of the model given all parameter values ($log (P(y|\theta))$). 

```{r, collapse = TRUE}
sum (dnorm (mens_height, 173.8, 7.78, log = TRUE))
```

When we add the logarithm of the priors for our parameters to this. By combining our priors and likelihoods in this way, we are calculating the *unnormalized* posterior density. 

```{r}
sum (dnorm (mens_height, 173.8, 7.78, log = TRUE)) + 
  dnorm (173.8,176,15,log=TRUE) + 
  dnorm (7.78,0,15,log=TRUE)+log(2) 
```

We can see that our calculation is very close to the average value of the `lp__` estimates provided by our model. 

```{r}
mean (samples$lp__)
```

The log posterior density, or values related to it, will come up again in chapter X when we discuss model comparison. This is because the value captures the posterior probability of the parameters given the data. Broadly speaking, models with a higher posterior probability are more believable because they are more likely to generate the observed data, and/or they have parameter values that are more probable a priori. 

## Answering our research questions

Finally, let's return again to the research questions we posed initially in chapter 2, and again at the beginning of this chapter:

Q1)  What is the average apparent height  of the whole *population* likely to be?

Q2)  Can we set bounds on likely mean f0 values based on the data we collected?

We can consider the answers to these questions provided by our final model, `model_priors`. Usually, parameters should be reported with *at least* the mean/median and standard deviations of the posterior distribution, in addition to some useful credible interval (e.g. 50%, 95%) around that parameter. Based on the result of our final model, an answer to each question might be something like this: 

A1) Based on our model the average apparent height for adult males is likely to be 174 cm. In a paper we might report this like: "The mean height is 174 cm (s.d. = 0.3, 95% CI = 173.2, 174.7)".

A2) Yes we can. There is a 95% probability that the population mean is between 173.5 and 174.7 given our data and model structure. In other words, 95% of the posterior density is concentrated between the values of 173.2 and 174.4. 

Notice that our answers correspond closely to what we concluded at the end of last chapter, that "the average male speaker is *most likely* to sound about 174 cm tall. We can also conclude informally based on Figure \@ref(fig:F28) that the most likely mean values fall between (approximately) 173 and 174.5 cm". The reason for this correspondence is because we made our inferences at the end of chapter 2 using only the likelihood and, due to the shape of the prior and the number of observations in our data, the shape posterior distribution of our model is being dominated by the likelihood. 

## Frequentist corner

In frequentist corner, we're going to compare the output of `brms` to some more 'traditional' approaches. We're not going to talk about the traditional models in any detail, the focus of this section is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in the different models. If you are already familiar with these approaches, these sections may be helpful. If not, some of the information provided here may not make much sense, although it may still be helpful. If you want to know more about the statistical methods being discussed here, please see section X for a list of suggested background reading in statistics.  

### One-sample t-test vs. intercept-only Bayesian models 

A one sample t-test helps investigate whether the mean of a set of observations is consistent with a true underlying value of zero. The t-test answers questions by investigating the likelihood of parameters given the data (priors and posteriors are not involved). In addition, t-tests are calculated using analytic (exact) methods, and do not involve sampling from distributions. The t-test assumes that all of your observations are independent, so it is not appropriate to use for this data. However, we just want to compare it to our initial Bayesian model (which made the same assumptions). We can apply a one-sample t-test to our vector of apparent-height judgments:

[@@ I think you're conflating frequentist *testing* and maximum likelihood estimation. Tests in MLE approaches typically (always?) rely on frequentist testing assumptions, but most frequentist testing does not rely on MLE. For example, the variance estimator in the denominator of a t-test is explicitly *not* the maximum likelihood estimate of the variance - it's the unbiased estimate (dividing by $n$ produces the maximum likelihood estimate, dividing by $n-1$ produces the unbiased estimate). I like the idea of explicitly comparing different approaches at the ends of chapters, but I think I'm inclined to avoid doing so in a "frequentist vs Bayesian" way, or at least I think that we should minimize our focus on that distinction, per se. I need to think about this some more, and we should talk about it.]

```{r}
t.test (mens_height)
```

Notice that the interval provided around the mean (`95 percent confidence interval: 173.2010 174.3744`) corresponds very well to the 95% credible interval around the intercept provided in `model` (`173.20, 174.38`). The reason they align so well is because both models have the same general structure and make similar assumptions. In addition, our Bayesian estimate is being dominated by the likelihood, and the more 'traditional' t-test *only* considers the likelihood. As a result, although the Bayesian model reports posterior probabilities, these can provide very similar responses as likelihood-based approaches. 

### Intercept-only ordinary-least-squares regression vs. intercept-only Bayesian models 

Ordinary-least-squares (OLS) regression is an approach to fitting regression models using likelihoods (without priors or posteriors). OLS regression assumes that your residuals are independent and that your error variation is normally distributed. We can fit an OLS model using the `lm` (linear model) function in `R`, using the same model formula we used for our Bayesian models. 

```{r}
ols_model = lm (mens_height ~ 1)
summary (ols_model)
```

Again, we see a close similarity to our initial `model`. The `Std.error` for the Intercept above (0.2998) corresponds closely to the `Est.error` of the intercept below (0.31), and the `Residual standard error` above (7.764) corresponds closely to the `sigma` estimate below (7.78). Just as with the t-test, these similarities are due to the models having the same general structure, making the same assumptions, and being dominated by the likelihood of the parameters.  

```{r}
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   173.80      0.31   173.20   174.38 1.00     1038      598
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     7.78      0.22     7.38     8.21 1.00     1060      736
```

