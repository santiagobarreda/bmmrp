\newpage
```{r, include = FALSE}
knitr::opts_chunk$set(
  dpi = 300, dev = "jpeg", collapse=TRUE
)
```

# Inspecting a 'single group' of observations using a Bayesian multilevel model {#c4}

In the last chapter we built a Bayesian regression model suitable for inspecting the average of a single group of observations. However, as we noted multiple times this model was 'wrong' for the structure of our data. The reason for this is that this model did not properly account for the *repeated measures* structure in our data. To properly handle repeated measures data, we need a *multilevel model*. In this chapter we will explain what we mean by 'repeated measures' and 'multilevel', in addition to fitting our first proper multilevel Bayesian model using `brms`.

## Multilevel models and repeated measures data {#c4-multilevel}

Depending on the field of study, experiments often produce **repeated measures data**, data where multiple observations come from the same experimental unit (e.g., person, as in the data used in this book). As a practical matter, setting up experiments is often time consuming such that it makes more sense to collect, for example, 100 observations from each of 50 different people, rather than 1 observation from each of 5000 different people. In addition, collecting more than one measurement from each source can go a long way towards reducing uncertainty in a model. However, the statistical analysis of repeated measures data requires models that take the repeated nature of the measurements into account. Treating repeated measures data as if it were *not* repeated measures data can cause problems for the inferences we make using statistical models. 

For example, if we have single samples of speech from each of 50,000 adult males from Los Angeles, we may be confident that we can reliably estimate the speech characteristics of male speakers from Los Angeles. But what if our 50,000 samples were from only five different speakers? It's obvious that this makes data less reliable for making inferences about Los Angeles in general, but the 10,000 observations from each speaker likely provide excellent information about the five people we measured. The repeated observations of a limited number of subjects may give you good information about the specific subjects observed, but it may also be less useful to make inferences about the population more generally. 

In general, the reason repeated-measures data can cause problems when not modeled appropriately is because the observations within each experimental unit are not independent: Multiple measurements from the same person are probably going to be similar to each other. For example, one listener may tend to perceive larger apparent sizes than another listener does, which can give us a warped perspective regarding the degree and nature of the variability in a set of observations of apparent size judgments. 

We're going to consider the same data we discussed in Chapter 2, the apparent height judgments made for the adult male speakers in our experiment in the actual, unmodified resonance condition (see section \@ref(c1-exp)). Below we load and subset our full data to include only these trials:

```{r}
library (bmmb)
data (exp_data)
men = exp_data[exp_data$C_v=='m',]

# create vector of height judgments
mens_height = men$height
```

Figure \@ref(fig:F4-1) presents the height judgments collected for adult male speakers in our experiment, organized in three different ways. In the right panel we see the marginal distribution of the data with no differentiation made between listeners. This boxplot shows the individual observations of our vector $height_{[i]}$ around the overall mean $\mu$ with a standard distribution equal to $\sigma_{[total]}$. In the middle panel we see each listener's height judgments in a different boxplot. Each listener has a different mean value which we will refer to as $L_{[n]}$ for listener $n$, $L_{[\bullet]}$ for any individual unspecified listener, and $L_{[\;]}$ with an empty subscript when referring to all listener mean values. Each listener also has a distribution of height judgments around their average value. For the sake of simplicity, we will assume that the standard deviation of these distributions is equal to $\sigma_{[within]}$ for all listeners. Finally, in the left panel we see the distribution of individual speaker averages ($L_{[\;]}$). This distribution is similar to considering the distribution of thick bars inside each individual boxplot (although these bars represent the medians and not the means). We can call the standard deviation of this distribution $\sigma_{[between]}$, since it represents between-listener variation in average height judgments.

```{r F4-1, fig.height=3, fig.width=8, fig.cap = "(left) Boxplot of distribution of average height judgments made by each listener in the experiment for adult male speakers. (middle) Individual boxplots of height judgments for adult male speakers, for each listener. (right) The marginal distribution of the height judgments shown in the middle plot.", echo = FALSE}

################################################################################
### Figure 4.1
################################################################################

par (mar = c(4,.1,.5,.1), mfrow = c(1,1), oma = c(0,4.2,0,0))
layout (m=matrix(1:3,1,3), widths = c(.15,.7,.15))
boxplot (tapply(men$height,men$L,mean), xlab = "", ylab="Height (cm)",
         col = cols, ylim = c(140,195), width = 2,cex.axis=1.3)
abline (h=)
boxplot (height ~ L, data = men, xlab = "Listener", ylab="",yaxt='n',
         col = cols[-1], ylim = c(140,195), cex.lab=1.3,cex.axis=1.3)
grid()
abline (h = mean(mens_height), lwd=2, lty=3)
boxplot (height ~ L, data = men, col = cols[-c(1,8)],add=TRUE,yaxt='n',xaxt='n')
boxplot (men$height, ylab="",yaxt='n', col = cols[8], ylim = c(140,195), width = 2)
mtext (side=2,outer=TRUE, text="Height (cm)", cex=1, line=2.9,adj=.55)
```

A single normal distribution has a single fixed standard deviation. A visual inspection of the boxplots in figure \@ref(fig:F4-1) suggests that $\sigma_{[total]}$, $\sigma_{[within]}$, and $\sigma_{[between]}$ may not all be equal. Conceptually, they are not the same thing: $\sigma_{[total]}$ measures variation among both listeners *and* observations, $\sigma_{[within]}$ measures variation conditional on a particular listener, and $\sigma_{[between]}$ measures variation between listener averages while ignoring random variation in observations within listeners. So, we potentially have three different standard deviations, and three different sources of variation, in our data based on how we have re-conceptualized the variation. The final model we used to analyze our data in the last chapter looked like this:

$$
\begin{equation}
\begin{split}
\\
height_{[i]} \sim N(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} \\
\\
\textrm{Priors:} \\
\mathrm{Intercept} \sim N(176, 15) \\
\sigma \sim N(0, 15) \\ 
\end{split}
(\#eq:41)
\end{equation}
$$

This model above estimates $\sigma_{[total]}$ ($\sigma$ in the model above) but not $\sigma_{[within]}$ or $\sigma_{[between]}$. As a result, this model treats all deviations from the mean as random and therefore acts as if systematic between-listener variation did not exist. However, it is clear that listeners show consistent tendencies in the heights they tend to report, so that systematic between-listener variation ($\sigma_{[between]}$) should not necessarily be treated as random, unpredictable error relative to a single population mean. For example, a height response of 181 cm may be 7 cm above the overall mean, but is perfectly average for listener 10. From the perspective of the model above, an observation of 181 cm is +7 above the mean regardless of who produced it. All variation about the mean is noise. However, if you know listener 10 provided a response then a response of 181 cm should not be surprising and would be perfectly 'on target' for this listener. So, an observation of 181 would be an error of 0 cm when provided by listener 10. 

If we consider our data *conditional* on the listener who provided it, it is variation from that listener's average ($L_{[\bullet]}$) value that should be considered (observation-level) *noise*, and not all deviations from the mean ($\mu$). So, from the perspective of a model that includes information about listeners the *real* random error is not $\sigma_{[total]}$ but $\sigma_{[within]}$. This is because our model would 'know' why the boxplots in the middle panel of figure \@ref(fig:F4-1) have different means (because different listeners provided the judgments), but not why the boxplots have an internal distribution (i.e. why listeners provided a range of height judgments).  

[@@ I see that the next section is about modeling variation at two levels, but I think the above paragraph should be revised a bit, since it makes it sound like the listener-level variation isn't noise. I added "observation-level" in parentheses, but then I saw the bit about "real random error" and I wasn't sure what, exactly, to write - the variation between listeners is also real random variation, isn't it?]

### 'Levels' of variation {#c4-levels}

We can think of repeated measures data as having multiple 'levels' of variation, as shown in figure \@ref(fig:F4-2). For our experimental height data, our 'levels' are: 

* The 'lower' level: Is the distribution of the *data* itself, our individual observations and data points. We expect that each listener will produce judgments around their specific average ($L_{[\bullet]}$), with, again for the sake of simplicity, a standard deviation equal to $\sigma$ for all speakers (referred to as $\sigma_{[within]}$ above). Clearly, there may be many good reasons for listeners' height judgments to vary from their average. However, our model cannot explain this and so this is treated as 'error' by our model. 

* The 'upper' level: The distribution of *parameters* between your sources of data. In this case this is the average height reported by a given listener $L_{[\bullet]}$. The speaker averages can be thought of as random variables, since we also cannot explain exactly why listener averages deviate from the overall average, so any given speaker's $L_{[\bullet]}$ is unpredictable a priori. We can assume that this upper-level distribution is also normal with a mean of $\mu_L$ and a standard deviation equal to $\sigma_L$ (referred to as $\sigma_{[between]}$ above).  

A **multilevel model** is a special kind of regression model that is able to simultaneously model random variation at multiple levels. A multilevel model fit to our height judgments would be able to estimate the values of $L_{[\bullet]}$ for all listeners, the variation in listener means ($\sigma_L$), and the random within-listener variation ($\sigma$), all at the same time. The structure of such a multilevel model can be compared to the structure of the 'unilevel' model we fit to our height judgments in Chapter 3 (and seen in \@ref(eq:41)). In our 'unilevel model' there is only one level and one distribution: The data distribution. In the simple multilevel model we just introduced, there are two levels: the distribution of *parameters* above, and the distributions of *data* below. 

```{r F4-2, echo = FALSE, out.width = "50%", fig.cap = "A comparison of a statistical model with only a single level of variation to the structure of a multilevel model."}

################################################################################
### Figure 4.2
################################################################################

knitr::include_graphics("../images/unilevelvsmultilevel.png")
```

Model parameters describe some tendency in the data. For now we are just talking about average values. So, our models describe listeners in terms of their average response, which it encodes using parameters related to the listener means. As a result, when we say "the distribution of parameters" to refer to the distribution of $L_{[\bullet]}$, we are really saying "the distribution of listener characteristics". So, when we say "multilevel models model variation at the data level and the parameter level", we are basically saying "multilevel models allow us to model variation of data within listeners, and of parameters/characteristics between listeners".

## Strategies for estimating factors with many levels {#c4-strategies}

Our model so far has only included an intercept. However, it seems that our model should include coefficient that would allow us to model variation in apparent height judgments across different listeners. R treats nominal, categorical predictors as *factors* and assumes that each different label represents a different group. Each value of a factor is called a *level*. Although we can indicate listeners using numbers, we are not treating this predictor as numerical or otherwise quantitative (i.e., Listener 1 isn't one less than Listener 2). Instead, we will treat listener (`L`) as a factor, and the individual listeners in the experiments are its levels. As far as our models are concerned, participant/speaker/subject/listener has no special status as a predictor and it is just a factor with many levels. 

In order to estimate a separate effect for each listener we need to add 15 parameters to our model, $L_{[1]},...,L_{[15]}$, one for each listener. Our data could then include a predictor for each listener coefficient that would equal one or zero based on whether the row was contributed by that listener or not. For example, in \@ref(eq:42) we see a line consisting of a set of 15 predictors associated with the $L$ coefficients, all multiplied by zero save for the second one. The effect of this, seen in the second line in \@ref(eq:42), is to only have the second listener coefficient contribute to the value of the predicted value for observation $i$.  

$$
\begin{equation}
\begin{split}
\mu_{[i]} = Intercept + L_{1}*0 + L_{2}*1 + L_{3}*0 +\ldots + L_{15}*0 \\
\mu_{[i]} = Intercept + L_{2}*1  \\
\end{split}
(\#eq:42)
\end{equation}
$$

A careful consideration of \@ref(eq:42) suggests that the $L_{[\bullet]}$ coefficients can't equal the average height reported by each listener. For example, if the overall mean (the intercept) is 170 cm and listener three reports heights of 175 cm on average, the equation above would predict a height response of 345 cm ($Intercept + L_{[3]}=354$) for this listener. Clearly that is not how the model should be working. Recall that regression models encode *differences* rather than absolute values. Our model already represents the overall data average using the intercept parameter. Thus, the listener predictors only need to contain information about *differences* between the listener average and the intercept, i.e. the *effect* for that listener. So, if the overall average is 170 cm and listener three reports an average height of 175 cm, then $L_{[3]}=5$ and $Intercept + L_{[3]}=175$. For this reason, multilevel models usually set $\mu_L$ equal to zero and model listener (or group) specific variation around zero, rather than the absolute values of listener (or group) averages.     

If every single factor level were to get an independent parameter represented in our regression equation, these would become very long and difficult to interpret. For example, we may want to include a factor with dozens or even hundreds of levels. Instead, we can treat the effects associated with each level of a factor as a vector. So, rather than use two separate predictors, $L_1$ and $L_2$, to represent the first and second listener effects, we will use $L_{[1]}$ and $L_{[1]}$, the first two elements of a vector of length 15. In order to know which listener contributed to each trial, we also need an index variable, a variable that equals a number from one to 15 that indicates which listener parameter to use on that trial. This is the `L` column in our `exp_data` data. For example, below we convert our listener vector to a factor and check out the order of the levels. 

```{r}
L = as.factor (men$L)
levels(L)
```

When you give `brms` a factor as a predictor, it uses the property of factors as numbers (discussed in section \@ref{c1-categorical}) to index the predictor vector. So, when you see the order of the levels provided by R, that is the order of the coefficients in the model that represent the levels of that factor. When we then use this listener vector to estimate the average height reported by each listener, the ordering of the levels is maintained in the listener averages. 

```{r}
# find average reported height for each listener
listener_means = round(tapply (men$height, L, mean),1)
listener_means
```

If we center the listener_means vector above, this is basically equivalent to our vector of listener effects $L_{[\;]}$. 

```{r}
listener_effects = round (listener_means - mean (listener_means),1)
listener_effects
```

Imagine that we knew some data was provided by the second, third and first listeners. We could represent these effects in the proper order as below:

```{r}
listener_effects[c(2,3,1)]
```

The `L` column in our data tells us which listener provided our data for each data point we have. So, if we index our effects vector with the `L` vector, we end up with a vector that shows you which listener effect applies for each observation in our data. Below, we see that the first six elements of `L` indicate listener 01, and as a result the first six elements of `listener_effects[L]` would represent the coefficient for listener 01. 

```{r, eval=FALSE}
head (L)
head (listener_effects[L])
```

Unfortunately, R names the vector of coefficients in your model using the same name as your predictor. So, the vector of listener effects in your model won't have a nice name like `listener_effects`. Instead, we have a predictor called `L` which represents listeners, *and*, we have a set of coefficients called $L$ in our model that represent the effects for each of our listeners. As a result, our regression equations include terms that look like this, $L_{[L_{[i]}]}$, which means: "Coefficient $L$, and the level is the one indicated by the `L` predictor for trial $i$". This term is now included in our prediction equation in \@ref(eq:43). For example, in the code below we the equivalent notation in terms of our `listener_effects` vector, where we arbitrarily set $i=300$ to show the listener affect associated with the 300th trial in our data.

```{r, eval = FALSE}
i = 300
L[300]
listener_effects[L[i]]
```

When you see terms like these $L_{[L_{[i]}]}$ in your models, this is nothing more than a more compact way of representing the same model structure shown in \@ref(eq:42). The regression equation in \@ref(eq:42) says that the expected value for any given trial is equal to the intercept plus the listener effect specific to the listener that contributed that trial (as indiciated by the `L` vector). 

$$
\begin{equation}
\begin{split}
\mu_{[i]} = Intercept + L_{[L_{[i]}]} \\
\end{split}
(\#eq:43)
\end{equation}
$$

We know that the 15 values of $L$ are related by being different listeners in our experiment. The way you tell your model that these coefficients are related is by treating them as levels of the same factor rather than as unrelated coefficients. One way to think of factors is that they represent 'batches' of coefficients in your model that are thematically or conceptually related. Once you encode the fact that there are 'batches' of parameters in your model, you have several options with respect to how these batches are estimated. Specifically with respect to our height data, we have several options regarding how we estimate the value of our listener parameters $L_{[\bullet]}$. Gelman and colleagues (CITE) discuss three general ways that parameters related to levels of a factor can be estimated: 1) *Complete pooling*, 2) *No pooling*, and 3) *Partial pooling*. The distinction between complete, no, and partial pooling is made for each factor in a model. So, one factor may be estimated using partial pooling while another may be fit using no pooling. This makes understanding the distinction between these approaches very important.  

### Complete pooling {#c4-complete-pooling}

Last chapter we carried out a **complete pooling** analysis of our data. That is, we threw everything into one big pile and analyzed it as if it came from one undifferentiated group (as in figure \@ref(fig:F4-2)). There are several problems with this approach. First, the complete pooling approach does not allow us to make any statements about variation between listeners since it does not estimate values of $L_{[\;]}$. In fact, this model assumes that $sigma_L$ is equal to zero (or does not exist). If listeners do not differ from each other in any way, then $sigma_L$ will be equal to zero and the two models in figure \@ref(fig:F4-2) will converge on each other. However, if $sigma_L$ is not zero, then this approach will miss out on important information. 

There is perhaps a more serious problem with the complete pooling approach. The complete pooling model assumes that all deviations from the mean are due to random error. Recall that regression models assume that all random variation around predicted values (i.e. the residuals) are independent. Since independence is assumed, likelihoods are calculated the 'easy' way by multiplying individual marginal probabilities (see sections \@ref(c2-joint) and \@ref(c2-chars-of-likelihoods)). However, as we can see in figure \@ref(fig:F4-1), each listener had a slightly different average judged height. This means that the residuals associated with listener 14 are not independent, we actually expect almost all of them to be positive (i.e. larger than average) since nearly all of this listener's responses were above the average. The violation of this assumptions means that if we are using a no-pooling approach for our repeated-measures data we are calculating likelihoods the 'wrong' way. Basically, our data has 675 rows but not 675 totally-independent observations, but our model doesn't know this.

We've previously mentioned that our models are not exactly 'right' or 'true', so why does it matter all of the sudden that the complete pooling model is 'wrong'? Recall that in section \@ref(c2-chars-of-likelihoods) we mentioned that the likelihood function gets narrower as a function of the sample size $N$? This applies only to $N$ *independent* observations, but not necessarily to *dependent* observations. As a result, when we treat dependent observations as independent our likelihood may end up being narrower than is warranted. This gives us a false sense of security about how precise our parameter estimates are. So, it matters that this is wrong because it can end up being very wrong, and it can easily be improved. Basically, spherical models of billiards balls are wrong but useful, while cubic models of the balls are wrong and much less useful. Treating residuals both within and across listeners as independent is kind of like using a cubic model of a billiard ball.

### No pooling {#c4-no-pooling}

To account for systematic variation between listeners in the data we can include listener predictors ($L_{[\;]}$) in our model, as in as in \@ref(eq:44). When we estimate the levels of this factor with **no pooling**, we estimate every level of the factor totally independently. One way to model this mathematically is to think of the coefficients as coming from a uniform distribution that extends from negative to positive infinity. The uniform distribution is just a flat line, meaning any value in the interval is equally likely. So, a uniform distribution extending from positive to negative infinity places absolutely no restrictions on the possible values of a parameter and says absolutely any value is equally likely. This is equivalent to setting $\sigma_L=\infty$. As a result, we can see complete and no pooling as two extremes, in one case we assume there is no between-listener variation at all and in the other we assume that between-listener variation can potentially be infinitely large. 

In \@ref(eq:44) we see that in our no pooling model, the expected value ($\mu$) for each trial is the sum of our intercept and the listener predictor, and that each of the $L_{[\;]}$ terms is drawn from the same uniform distribution. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + L_{[L_{[i]}]}  \\
\\
\textrm{Priors:} \\
L_{[\bullet]} \sim \mathrm{uniform}(-\infty, \infty) \\\\
\mathrm{Intercept} \sim \mathrm{N}(176, 15) \\
\end{split}
(\#eq:44)
\end{equation}
$$

There is one main weaknesses to this approach: Assuming that $\sigma_L=\infty$ does not allow shrinkage or regularization to occur. We know that shrinkage occurs when an estimate is pulled towards its prior. Well, a perfectly uniform prior does not exert any influence on posterior distributions, so that these are completely reflective of the likelihood function. As a result, a factor estimated with no pooling is one in which shrinkage cannot occur. The absence of shrinkage is a *bad* thing, as this is considered to be a strength of Bayesian modeling. Shrinkage represents the pooling of information across the levels of your factor. For example, below we see some quantiles for the average apparent height reported for adult male speakers by the listeners in our experiment: 

```{r}
listener_means = tapply (men$height, men$L, mean)
quantile(listener_means)
```

The listeners in this experiment each carried it out independently and did not know each other. Despite this, we see below that half of listeners' average height judgments are between 171 and 176 cm, and that all listener averages fell between 168 and 181 cm. The consistency of these judgments suggest a common behavior between listeners, and an underlying similarity in listener characteristics and parameters. Given this, it might be surprising if a new listener carried out the experiment and provided a height judgment of 130 cm for an adult male. In the absence of strong evidence to suggest that this is a good representation of their true opinion, perhaps we should be skeptical of this value (e.g. perhaps they clicked somewhere by mistake). Unfortunately, our no pooling model has no mechanism by which this can occur. In the absence of shrinkage the characteristics of each listener are estimated as if only a single listener had taken part in our experiment. But we don't have one listener, we have 15, and they all performed the experiment in quite similar ways. Clearly, it may be useful if our model could consider what it knows about the values of $L_{[\;]}$ in general when it estimates the value of any given $L_{[\bullet]}$ in particular.

### (Adaptive) Partial pooling {#c4-partial-pooling}

Complete pooling and no pooling offer extremes, we either assume $sigma_L$ is zero or we assume it is infinite. Partial pooling offers a sort of middle ground between complete pooling and no pooling, a potential 'just right' compromise solution. **Partial pooling** estimates the values of $L_{[\bullet]}$ using non-zero values of $sigma_L$, meaning that the parameters representing the levels of a factor are neither completely independent nor totally merged. 

We know that our prior probability can help to pull parameters towards the prior (as discussed in section \@ref(c3-posterior)), resulting in *shrinkage*. In order for this to work effectively, the variance of the prior needs to be set at a reasonable value given the amount of variation in the data. Below, we see that the standard deviation of listener means is 3.6 cm. It would be nice to be able to use a value close to this, since we know this to be a good fit to our data. Unfortunately, if we did this it would not really be a *prior* probability anymore, since we would be analyzing the data, finding the most likely parameter values, and then using that to set what is supposed to be the *prior* probability. 

```{r}
sd(listener_means)
```

Instead of doing this, a partial pooling model allows us to estimate $\sigma_L$ using our data. This allows us to have a prior distribution for our parameters that is tailored to the data, without us having to actually specify it a priori. When standard deviation parameters like $\sigma_L$ are estimated from the data rather than a priori, this is called **adaptive partial pooling**. However, by convention the 'adaptive' part is usually dropped when partial pooling is discussed. So, when you read that a factor was estimated using 'partial pooling', most often that means that the standard deviation parameters of the model coefficients for that factor (e.g. $\sigma_L$ for $L_{[\;]}$) are estimated from the data.

The (adaptive) partial pooling version of our model is presented in \@ref(eq:45). There are two important differences with respect to the model in \@ref(eq:44). First, the $L_{[\bullet]}$ terms are drawn from a normal distribution with a mean of zero and a standard deviation equal to $\sigma_L$. The reason the mean of the distribution is equal to zero is that our listener coefficients only encode deviations relative to the intercept, and not the listener average. So, a listener that is perfectly average will have a coefficient equal to 0. Second, since the $\sigma_L$ term is estimated from the data a prior distribution for $\sigma_L$ is specified.

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + L_{[L_{[i]}]} \\ 
\\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\ 
\\
\mathrm{Intercept} \sim N(176,15) \\
\sigma \sim N(0,15) \\
\sigma_L \sim N(0,15)
\end{split}
(\#eq:45)
\end{equation}
$$

This approach is called partial pooling because the estimate of each level of $L_{[\;]}$ is potentially influenced by every other level of $L_{[\;]}$  by virtue of being drawn from the same distribution $N(0,\sigma_L)$. For example, we saw that half of listener mean height judgments were between 170 and 176 cm, and our model will now take that into account when estimating the individual coefficients that make up $L_{[\;]}$. So, unlike the complete pooling case we do not ignore variation between listeners, and unlike in the no pooling case we do not ignore the similarities between them. 

### Hyperpriors

You may have noticed that in \@ref(eq:45) we assign a prior distribution to $\sigma_L$ in the model above. However, $\sigma_L$ is a parameter in the prior distribution of our listener parameters $L_{[\bullet]}$. So actually, the prior for $\sigma_L$ is the prior of a prior. We might call this a 'grand prior' (like grandparent) but the actual term for it is **hyperprior**. One way to think of it is that in a multilevel model each level effectively acts as the prior for the level below it. This leads to a nested or hierarchical model structure (sometimes multilevel models are called **hierarchical models** for this reason). 

We're going to update the equation defining our posterior distributions (originally presented in \@ref(eq:312)) to take into account the structure of our partial pooling models. Our model will now reflect not only our prior beliefs but also the information encoded in our hyperpriors, and it will estimate the distribution of $\sigma_L$ from the data. We can begin with the posterior probability of observing a given value of $L_{[\bullet]}$ for a single listener in isolation. The equation below says that the posterior probability of some speaker effect ($L_{[\bullet]}$) given the data $y$ is equal to the product of the likelihood of $L_{[\bullet]}$ given the data ($P(y|L_{[\bullet]})$) and the prior of $L_{[\bullet]}$, divided by the marginal probability of the data ($P(y)$). 

$$
\begin{equation}
\begin{split}
P(L_{[\bullet]}|y) = \frac{P(y|L_{[\bullet]})*P(L_{[\bullet]})}{P(y)}
(\#eq:46)
\end{split}
\end{equation}
$$

If we want to consider the posterior probability of $\sigma_L$ given the data, we can add $\sigma_L$ to every term that previously included $L_{[\bullet]}$ in \@ref(eq:46). Basically, we are saying $\sigma_L$ *and* $L_{[\bullet]}$, instead of just $L_{[\bullet]}$. Equation \@ref(eq:47) can be read aloud as "the posterior probability of observing a certain listener effect and a certain amount of between-listener variation given some data is equal to the likelihood of the data given the listener effect and the amount of variation between listeners, times the joint prior probability of the listener effect and the amount of variation between listeners, divided by the marginal probability of the data". 

$$
\begin{equation}
\begin{split}
P(L_{[\bullet]},\sigma_L|y) = \frac{P(y|L_{[\bullet]},\sigma_L)*P(L_{[\bullet]},\sigma_L)}{P(y)}
(\#eq:47)
\end{split}
\end{equation}
$$

We know that we can represent the joint probability of a listener effect and the listener standard deviation, $P(L_{[\bullet]},\sigma_L)$, using normalized conditional probabilities as in $P(L_{[\bullet]}|\sigma_L)*P(\sigma_L)$. We can also remove the listener standard deviation $\sigma_L$ from the likelihood of effect given the data ($P(y|L_{[\bullet]})$) because we know that the probability of any given data point in a normal distribution depends only on the mean of the distribution and not the standard deviation of means *between* distributions. These two changes are reflected in \@ref(eq:48). The first term in the numerator on the right hand side is the likelihood, this was in our original equation in \@ref(eq:46). The second term is the prior distribution of listener effects. This also appeared in \@ref(eq:46), however, this distribution now allows for variation in the value of $\sigma_L$. Finally, the third term represents the hyperprior, the prior distribution of one of the parameters of another prior distribution. 

$$
\begin{equation}
\begin{split}
P(L_{[\bullet]},\sigma_L|y) = \frac{P(y|L_{[\bullet]})*P(L_{[\bullet]}|\sigma_L)*P(\sigma_L)}{P(y)}
(\#eq:48)
\end{split}
\end{equation}
$$

Our posterior estimates of $L_{[\bullet]}$ may differ from our maximum likelihood estimates of the listener effects. This is because in addition to the likelihood, the posterior probability of $L_{[\bullet]}$ in \@ref(eq:48) takes into account the distribution of values of $L_{[\bullet]}$ with respect to estimates of $\sigma_L$. When our posterior estimates of the listener (or any other) effects are more similar to the population average than the maximum-likelihood estimate, *regularization* has occurred. Importantly, the amount of pooling/shrinkage that will occur will be determined by $\sigma_L$, which is estimated from the data itself. 

## Estimating a multilevel model with `brms` {#c4-estimating1}

We're going to fit a model to the same data we investigated in Chapter 3, however, we're going to use a multilevel model that reflects the repeated-measures nature of the data. 

### Data and Research questions {#c4-data-and-qs-1}

Below we load and subset our full data to only include those trials involving adult male speakers, and only when the actual resonance size was presented. This is the same data we considered in chapter 3. 

```{r}
data (exp_data, package= "bmmb")
men = exp_data[exp_data$C_v=='m',]
```

Once again, we're going to try to address the following basic research questions: 

  Q1) How tall does the average adult male from the US sound?

  Q2) Can we set limits on credible average apparent heights based on the data we collected?

To analyze our data using `brms` in R we need it to be in a data frame with each row containing a different observation. We need one column containing our dependent variable (the variable we want to analyze) and one column that indicates the 'source' of the data. We can see that our data satisfies these conditions below, where the relevant columns (for this model) are `L` for listeners, and `height` for our dependent variable. 

```{r}
head (men)
```

Often, experiments (or data more generally) will feature a single source of information. However, in this experiment we have both speakers producing our data and listeners providing judgments about our data. This leads to non-independent observations based on both speaker and listener. For example, a speaker who sounded tall to one person may have sounded tall to other listeners, and listeners who identified all speakers as generally tall/short would have done so in a similar way across all speakers. We will begin by building a model that incorporates information about listeners before discussing a model that includes information regarding both speakers and listeners.

### Description of the model

We're going to keep working on developing an intuitive understanding for model notation, and using it to describe models. Just keep in mind that this is nothing more than an efficient way to denote the concepts being discussed in the text. To specify a multilevel model, you need to write a slightly more complicated model formula. This explanation assumes that you have a data frame where one column contains the variable you are interested in predicting (in this case `height`), and another column contains a vector containing unique labels for each speaker/listener/participant (in this case a unique listener label `L`). Before, the model formula looked like this:

`height ~ 1`

Which meant 'predict height using only an overall intercept'. To indicate that your model contains an 'upper' level where you have repeated-measures clusters of data coming from different sources, you have to put another model formula *inside* your main model formula. The model formula corresponding to the model shown in \@ref(eq:45) looks like this:

`height ~ 1 + ( 1 | L)`

Which means 'predict height using an overall intercept and separate intercepts for each level of the L (listener) factor'. When you place a predictor in parenthesis on the right-hand-side of a pipe `|`, like this `( | predictor )`, you tell `brm` that you would like to estimate separate parameters for each level of the factor. Whatever you put in the left-hand-side of the parentheses `( in here | predictor )` specifies the parameters you wish to estimate for each level of the factor. In addition, any parameters specified on the left-hand side of the pipes in brackets (`( in here | predictor )`) will be estimated using (adaptive) partial pooling. By convention, effects estimated using partial pooling are modeled as coming from a normal distribution with a mean of zero, so that aspect of the model does not need to be directly specified. 

So the model formula `height ~ 1 + ( 1 | L)` tells `brms` to build an intercept only model with a separate intercept for each listener, and that the listener intercept terms should be estimated with partial pooling. Effectively, this model formula specifies a subset of the information provided in our full model specification shown in \@ref(eq:49b) (originally \@ref(eq:45))

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + L_{[L_{[i]}]} \\ 
\\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\ 
\\
\mathrm{Intercept} \sim N(176,15) \\
\sigma \sim N(0,15) \\
\sigma_L \sim N(0,15)
\end{split}
(\#eq:49b)
\end{equation}
$$

In plain English, the model above says:

> We expect height judgments to be normally distributed around the expected value for any given trial, $\mu_{[i]}$, with some unknown standard deviation $\sigma$. The expected value for a trial is equal to a fixed overall average (Intercept) and some value associated with the individual listener who made a perceptual judgment on the trial ($L_{[L_{[i]}]}$). The listener coefficients ($L_{[\;]}$) were modeled as coming from a normal distribution with a mean of zero and a standard deviation ($\sigma_L$) that was estimated from the data. 

There is a very important difference in how the 'unilevel' (complete pooling) model from Chapter 3, and this model partition the variance in apparent talker heights. The initial complete pooling model broke down the total variation in the data ($\sigma_{total}$) like this:

$$
\begin{equation}
\sigma^2_{total} = \sigma^2
(\#eq:49)
\end{equation}
$$

In other words, all variation was error. We don't know why values vary from the mean, so all variation is just random noise. Our multilevel model views the variation in our data like this:

$$
\begin{equation}
\sigma^2_{total} = \sigma^2_{L} + \sigma^2
(\#eq:410)
\end{equation}
$$

The total variance is equal to the within listener variance ($\sigma^2$) plus the between-listener variance ($\sigma_L^2$). From the perspective of this model, the variation *within* a speaker's individual boxplot is (data-level) error. The differences from listener to listener represent random between-listener variation in apparent height judgments (i.e., listener-level error). To some extent, investigating the *effects* for the different listeners in our sample involves asking how much the listener averages tended to differ from the overall response. As noted earlier, if listeners do not differ from each other in any way, then $\sigma_L$ is equal to zero and the two models in figure \@ref(fig:F4-2) will converge on each other. So, a crucial part of investigating the overall effect of different listeners on our results is understanding the magnitude of $\sigma_L$. 

### Fitting the model {#c3-fitting-1}

We can use `brms` to fit a model that more closely resembles our model specification in \@ref(eq:45). To do this we need to use the formula given above, and we also need to specify priors for four parameters: $Intercept$, $\sigma_L$, and $\sigma$. We can confirm our expectations using the `get_prior` function as seen below. 

```{r}
brms::get_prior (height ~ 1 + (1|L), data = men)
```

We see a new `class` of prior, `sd`. Our classes of priors are now: 

-   `Intercept`: this is a unique class, only for intercepts.
-   `sd`: this is for our standard deviation parameters that relate to 'batches' of parameters, e.g. `sd(Intercept)` for `L` ($\sigma_{L}$).
-   `sigma`: the error term.

The information presented by `get_prior` does not just list the priors that must be specified, but also different ways that priors can be set, so there are some redundancies. This is not so necessary for our very simple models now but it will become very useful later so it is worth understanding now while it is still relatively simple. Top to bottom, here is the information presented in the output above. First, you can specify a prior for the `Intercept`, which is its own unique class. Second, you can specify a prior for *all* `sd` parameters in your model. Third, you can specify a prior for all the `sd` parameters, specifically for your `L` group. The fourth line indicates that you can also specify a prior *only* for the `Intercept` for your `L` group (i.e. $L_{[L]}$) and not to any other listener-related predictors in your model (there are none for now). The fifth line lets you specify the prior for the error in the model (i.e. $\sigma$). 

Below we fit a model and specify prior distributions for our intercept, for *all* variables in the `sd` class, and for the error term (basically the first, second, and fifth lines above). 

```{r,eval = FALSE}
# Fit the model yourself
set.seed (1)
model_multilevel =  
  brms::brm (height ~ 1 + (1|L), data = men, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2,
             prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
                       brms::set_prior("normal(0, 15)", class = "sd"),
                       brms::set_prior("normal(0, 15)", class = "sigma")))
```

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_multilevel = bmmb::get_model ('4_model_multilevel.RDS')
```
```{r, include = FALSE}
# saveRDS (model_multilevel, '../models/4_model_multilevel.RDS')
model_multilevel = readRDS ('../models/4_model_multilevel.RDS')
```

### Interpreting the model

We can inspect the model print statement:

```{r, collapse = TRUE, eval=TRUE}
# inspect model
bmmb::short_summary (model_multilevel)
```

This model contains one new chunk its print statement, the `Group-Level Effects`, which tells us about the standard deviation for the Intercept (`sd(Intercept)`) for the `~L` factor. The term `sd(Intercept)` in this section corresponds to $\sigma_L$, the standard deviation of listener effects ($L_{[\;]}$) in the sample. Below, we calculate the listener mean heights values, the standard deviation of these, and the amount of within-speaker variation in apparent height judgments. We can see that these resemble the value of the analogous parameters in our model.

```{r, collapse = TRUE}
# find mean height for each speaker
listener_means = aggregate (height ~ L, data = men, FUN = mean) 
# find the within speaker standard deviation This is the within-talker 'error'.
listener_sigmas = aggregate (height ~ L, data = men, FUN = sd) 

# the mean of the speaker means corresponds to our Intercept
mean (listener_means$height)

# the standard deviation of the speaker means corresponds to 'sd(Intercept)', 
# the estimate of the standard deviation of speaker intercepts
sd (listener_means$height)

# the average within-speaker standard deviation corresponds to 'sigma', 
# the estimated error
mean (listener_sigmas$height)
```

We're going to discuss the calculations we made above in terms of what they represent in the boxplots below, our model coefficients in the print statement above, and in our model definition in \@ref(eq:49b). The overall mean height in our data (173.8) corresponds quite well to our model estimate of 173.8, seen in the horizontal line in the figure below. This is the intercept in our model. The standard deviation of the listener means is 3.6 cm. This is very similar to our model estimate of 3.8 cm. The standard deviation of the listener effects reflects the average distance from each speaker's average to the overall average, and reflects $\sigma_L$ in our model. Finally, the average of the within-speaker standard deviations in our data (6.8 cm) corresponds closely to our model's error estimate (`sigma` = 7.0), represented by $\sigma$ in our model. This reflects the average spread of each speaker's data relative to their own mean, within their own little box in a listener-based boxplot. We can see that the information provided by `brms` is quite similar to what we can estimate directly using our data. However, *brms* does this all for us, in addition to giving us information regarding credible bounds for different parameters.

```{r F4-3, fig.height = 3, fig.width = 8, fig.cap = "Listener-specific boxplots for apparent height judgments made for adult male speakers. The horizontal line indicates the model intercept.", echo = FALSE}

################################################################################
### Figure 4.3
################################################################################

par (mfrow = c(1,1), mar = c(4,4,2,1))
boxplot (height ~ L, data = men, main = "Listener Boxplots",col=bmmb::cols) 
abline (h = mean(men$height), lwd=3,lty=3)
boxplot (height ~ L, data = men, main = "",col=bmmb::cols,add=TRUE) 
```

## 'Random' Effects {#c4-random-effects}

If you have some familiarity with statistical modeling or the analysis of experimental or repeated-measures data, you may have heard of **random-effects models** or **mixed-effects models**. People often distinguish the predictors in their models between those that are 'fixed' and those that are 'random'. So, a researcher might describe their model as including 'random effects' for so and so and 'fixed effects' for some other predictor. In fact, a very common way of describing our model above would be that we used a 'mixed-effect model with random intercepts by listener'. What makes a predictor fixed or random and what is the practical effect of this in our models? It turns out that although they are commonly used, the terms 'fixed' and 'random' effects do not have a single agreed-upon definition. Gelman (2005) highlights at least five different definitions of the distinction given in the literature:

(1) Fixed effects are constant across individuals/groups, while random effects can vary across them. 

(2) Effects are fixed when you are interested in them specifically, but random if you are actually interested in the population.

(3) “When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random” Green and Tukey (1960).

(4) “If an effect is assumed to be a realized value of a random variable, it is called a random effect” LaMotte (1983).

(5) Fixed effects are estimated using maximum likelihood (no pooling) estimates least squares, while random effects are estimated using partial pooling (and shrinkage).

You will note that some of these definitions (2 and 4 in particular) would have an effect change from random to fixed based on the researcher's conception of it. This is the **mind projection fallacy**, the mistaken assumption that the way we choose to represent the world in our model reflects the true nature of the thing itself (McIllwreath, p81). Rather than focus on our conception of effects as fixed or random in theory, we may ask: What is the difference between these sorts of effects in practice? What special treatment are we referring to when we say that a certain predictor in our model is a 'random effect'? 

The answer is that when researchers discuss the inclusion of **random effects** in their models, what they are usually (if not always) referring to is that the effects were estimated using (adaptive) partial-pooling. When researchers indicate that their effects were **fixed effects**, they are usually saying that they were fit with no pooling (or at least with weak priors). This applies to both Bayesian models, and to models fit using more 'traditional' approaches such as the popular `lmer` function in the `lme4` package in R (cite). As a result, although researchers may define 'random' effects using any of the definitions shown above, in practice terms labelled 'random' are usually those estimated using adaptive partial pooling. In light of this, Gelman (cite) recommends avoiding the terms 'fixed' and 'random', since these are vague and overly-broad. Although we agree with this position, the fact is that in many research areas it is still most common to refer to effects as being either fixed or random. For that reason we will follow this convention, although we will continuously highlight the relevant underlying distinction: That so-called 'random' effects are simply those that are estimated using (adaptive) partial pooling. 

In general, when you have many levels of a factor, it may be a good idea to include these as 'random' effects, regardless of how 'random' they might actually be (cite). There is not much downside to it: You get more information from the model (e.g., information about $\sigma_{predictor}$), there are several modeling advantages associated with estimating large batches of coefficients with partial pooling (cite), and you can always fit multiple models to see what, if any differences, result form the different approaches. Rather than considering the more 'philosophical' position outlined in 1-5 above, some useful things to consider when deciding between fixed and random effects are: Do you believe that treating a predictor as a 'random' effect offers a modeling advantage? Does it better reflect the process/relationships you are trying to understand? Does it provide you with information you find useful? Is it realistic to fit this kind of model given the amount of data you have, and the nature of the relationships in the data? Right now the last point is not something we have talked about very much, but it is something we will need to worry about more as our models become more complicated.

### Inspecting the random effects {#c4-inspecting-random-effects}

You may have noted that the estimates of our coefficients fit with partial pooling ($L_{[\;]}$) do not appear in the model print statement. Instead, only the standard deviation of the effects is presented ($\sigma_L$, or `sd(Intercept)`). The reason for this is that there may be a large number of random effects coefficients, which would result in models that are difficult to read if they were all shown. A second reason for this is that researchers are often not directly interested in the individual values of their random effects, but rather they are usually focused on the overall effects common to all listeners such as the model intercept. For example, to this point we have been talking about "what is the average apparent height of male speakers" and never "what did listener 04 think the average height of men is?". However, there are situations in which we may be interested in investigating the random effects, and the `brms` package has several functions to facilitate this process.

We will present two ways to consider the random listener effects using the `ranef` function, which gets information about your random effects from your `brms` model. In the first of these you set `summary=FALSE` in the call to `ranef` as shown below. This function returns a list of three-dimensional matrices, where each list element is named after the grouping factor it represents. Below, we collect the random effects and take the element named `L`, corresponding to a matrix representing our listener factor. The matrix representing each factor has three dimensions, with individual posterior samples varying across the first dimension (rows), factor levels varying along the second dimension (columns), and parameters varying across the third dimension. Effectively, `listener_effects` is a set of two-dimensional matrices stuck together, one for each random parameter. 

```{r}
# get random effects, without summarizing
random_effects = brms::ranef (model_multilevel, summary = FALSE)
# get only the listener random effects
listener_effects = random_effects$L
```

You can select the matrix corresponding to the intercept by putting the name of the parameter in the third dimension of the matrix, as in  `listener_effects[,,"Intercept"]`,reducing our three-dimensional matrix to a two-dimensional one representing the random listener intercepts in our model. 

```{r}
# along the third (coefficient) dimension, take only the intercept dimension
listener_intercepts = listener_effects[,,"Intercept"]
```

The above process results in a two-dimensional matrix representing the individual samples of our listener effects. We can see below that this matrix has 5000 rows, corresponding to the individual samples, and 15 columns, representing the individual listeners. We use the `head` function to see the first six samples of the listener effects.

```{r}
# Our matrix of posterior samples showing 5000 samples for each of 15 parameters
dim (listener_intercepts)
# we can see the first six samples for each of the 15 listener parameters
head ( round (listener_intercepts, 2))
```

If we were to find the average of the samples within each column, the result would be the posterior mean estimate for each of our individual speaker effects. 

```{r}
colMeans (listener_intercepts)
```

We can repeat the process above but without specifying `summary=FALSE` like we did above. As a result, rather than the individual samples, you get summaries of the posterior distribution. We will still get a list of three-dimensional matrices. However, this time factor levels vary across the first dimension (rows), information about each factor level varies across columns, and parameters vary across the third dimension. We again take the list element corresponding to our list element `L` and take only the third dimension corresponding to our Intercept. 

```{r, collapse = TRUE}
# get random effects, *with* summarizing (by default)
random_effects = brms::ranef (model_multilevel)
# get only the listener random effects
listener_effects = random_effects$L
# along the third (coefficient) dimension, take only the intercept dimension
listener_intercepts = listener_effects[,,"Intercept"]
```

This time, the process results in a two-dimensional matrix representing a summary of the posterior, as seen below. The leftmost column of the output below represents the posterior mean effect estimated for each listener, the second column represents the standard deviation of posterior samples, and the third and fourth columns provide he 2.5% and 97.5% credible intervals for each parameter, respectively. We can see that the information in the first column below exactly matches the posterior means calculated for each listener effect above. This is because they are the same thing. In the un-summarized representation above we were seeing the individual samples varying across columns. In the summarized representation we are seeing summarizes of those samples, varying across rows. 

```{r}
listener_intercepts
```

Notice that the listener averages vary around 0, and some are even negative. That is because the listener effects (and all random effects) are represented as deviations from the intercept, as noted above. We can verify this by calculating the average reported height for each listener and centering these values. We can then compare these centered means to our model random effects and see that they are very similar. 

```{r}
listener_means = tapply (men$height, men$L, mean)
listener_means_centered = listener_means - mean(listener_means)

round (rbind (listener_means_centered, listener_random_effects = listener_intercepts[,1]), 1)
```

We can use a simple plotting function included in the `bmmb` package (`brmplot`) to look at distributions of effects. The function takes in the summary matrices made by `brms` and plots a point for each parameter mean/median, and lines indicating the credible intervals calculated by `brm` (usually 95% credible intervals). These matrices all have a standard form where parameters vary across rows, the first column is the posterior mean, the second is the posterior standard deviation (the estimated error), and the third and fourth columns are the 2.5% and 97.5% credible intervals. If you have a matrix like this you can easily plot it with `brmplot` as shown below.

```{r eval=FALSE}
bmmb::brmplot(listener_intercepts, col = bmmb::cols)
```

We can compare the estimates of our by-speaker effects to the distribution of centered height responses arranged by subject. We can see that the arrangement of the means and raw data shows a close correspondence. 

```{r F4-4, fig.height = 3, fig.width = 8, fig.cap="Boxplots presenting apparent height judgments made by each listener, centered across all listeners. Colored points and lines indicate posterior mean estimates and 95% credible intervals for the listener-specific intercept effects.", echo=FALSE}

################################################################################
### Figure 4.4
################################################################################

par (mfrow = c(1,1), mar = c(4,4,1,1))

height_centered = (men$height-mean(men$height))
boxplot (height_centered ~ L, data=men, col = 0, ylim = c(-30,20), xlab="Listener",
         ylab="Centered Apparent Height (cm)")
abline (h=0)
bmmb::brmplot(listener_intercepts, col = bmmb::cols, ylim = c(-30,20), add = TRUE)
```

## Simulating data using our model parameters {#c4-simulating}

One way to think about what all the numbers in our model mean is to simulate data that has the same characteristics by building it from the individual component parts. First, we set the intercept to 174 cm. Then, we create a random sample of 15 effects representing simulated listeners. The listener population has a mean of 0 and a standard deviation equal to $\sigma_L$ (3.8 according to our model). These effects are stored in a vector called `L_`, and they represent the values of $L_{[\;]}$ for our different speakers. We want each of these simulated listeners to provide 45 responses just like in our data, so we need an `L` vector with 15 values that repeat 45 times each. We can use this to index the `L_` vector containing the effects (as in the example involving `listener_means[L]` above). We also draw our error (i.e. $\varepsilon \sim N(0,\sigma)$). It's important to note that the error is just 15x45 random draws from this population. There is no distinction between one listener and another when it comes to the magnitude of the error. This property is called **homoscedasticity**, and the error is called **homoscedastic** when it is constant across all conditions/groups. 

```{r, collapse = TRUE}
## don't run this line if you want a new simulated dataset. 
set.seed(1)
## this is the value of our intercept
Intercept = 174
## this is a vector of 15 listener effects
L_ = rnorm (15, 0, 3.8 )
## this is a vector indicating which speaker produced which utterance
L = rep (1:15, each = 45)
## this vector contains the error
error = rnorm (45 * 15, 0, 7)
```

After creating our components, we add the Intercept, listener effects and random error to make our fake 'replicated' data. Since this data has the same statistical properties as our real data, it should look a lot like it.

```{r, collapse = TRUE}
# the sum of an intercept, listener effects and random error makes our fake data
height_rep = Intercept + L_[L] + error
```

Figure \@ref(fig:F4-5) we compare the results of our simulation to our real data. The are reasonably similar, which tells us that our model is a good reflection of the data.

```{r F4-5, echo = FALSE, fig.width = 8, fig.height = 3, fig.cap = "Comparison of real and simulated f0 production data."}

################################################################################
### Figure 4.5
################################################################################

par (mfrow = c(1,2), mar = c(1,2,1,1), oma = c(0,2,0,0))
boxplot (height_rep ~ L, ylim = c(140,200), xaxt='n',xlab='',
         col=bmmb::cols)
abline (h = 173.8, lwd=3,lty=3)
boxplot (height ~ L, data = men, ylim = c(140,200), xaxt='n',xlab='',
         col=bmmb::cols)
abline (h = 173.8, lwd=3,lty=3)
mtext (text = "f0", side=2, outer=TRUE, line = 1)
```

Below we make two datasets that are 'incomplete' with respect to the variation in our data. The first contains the intercept and noise only, the second contains the intercept and speaker effects only.

```{r, collapse = TRUE}
# this fake data is missing between speaker variation
height_rep_1 = Intercept + error
# this fake data is missing within speaker variation
height_rep_2 = Intercept +  L_[L]
```

In figure \@ref(fig:F4-6) we compare these 'incomplete' datasets to the full simulated data. The top row contains only error ($\sigma$). As a result, height judgments vary around the intercept, but there is no listener to listener variation. This is what that data would look like if there is no random variation in means across listeners. However, notice that each little box is not centered at zero. Although the error distribution is centered at zero, the small number of errors added to the true speaker mean are **extremely** unlikely to add up to exactly zero. So, error contributes some unknown amount to each listener's sample mean estimate. This makes estimations of the *real* speaker effects impossible in practice. In the middle plot, the figure shows only between-listener variation ($\sigma_{L}$) but no within-speaker variation (i.e., no $\sigma$). Now listener vary from the intercept, but they do not vary within themselves. The final plot is the combination of the variation in the top two plots and show the final simulated data: The sum of the intercept, the within-listener variation and the between-listener variation.

```{r F4-6, fig.width = 8, fig.height = 6, fig.cap="(top) Simulated error variation around the intercept. (middle) Simulated between-speaker variation, but no production error. (bottom) Simulated data containing both within and between-speaker variation in f0.", echo = FALSE}

################################################################################
### Figure 4.6
################################################################################

par (mfrow = c(3,1), mar = c(1,3,1,1), oma = c(0,2,0,0))
boxplot (height_rep_1 ~ L, ylim = c(140,198),xaxt='n',
         col=bmmb::cols)
text (1, 145, label = "Intercept + error", cex = 1.5, pos=4)
abline (h=174.8,lty=2)

boxplot (height_rep_2 ~ L, ylim = c(140,198),xaxt='n',
         col=bmmb::cols)
abline (h=174.8,lty=2)
text (1, 145, label = "Intercept + L", cex = 1.5, pos=4)

boxplot (height_rep ~ L, ylim = c(140,198),xaxt='n',
         col=bmmb::cols)
abline (h=174.8,lty=2)
text (1, 145, label = "Intercept + L + error", cex = 1.5, pos=4)

mtext (side=2,text="Height (cm)", outer = TRUE, line=0)
```

Note that the top plot in figure \@ref(fig:F4-6) basically represents the complete pooling model, a model that includes $\sigma$ but not $\sigma_L$. As seen in the figure such a model is unlikely to generate the substantially-different listener boxplots we see in our data. This is because this data has no mechanism by which to make the listener-dependent boxplot *be* different, since there is no between-listener variation. We can try another simulation, identical in all respects except we set the between-listener variance to zero, and set the noise variance to 7.8 (as in our complete pooling model). Below we generate 10,000 simulated data sets and record the standard deviation of the listener means for each one. These values will be stored in a vector called `sigma_L_rep` since we are simulating values of $sigma_L$ for our complete pooling model.

```{r, cache = TRUE}
set.seed(1)
reps = 10000
sigma_L_rep = rep(0,reps)
for ( i in 1:reps){
  Intercept = 173.8
  L_L = rnorm (15, 0, 0)  # zero between-listener variance
  L = rep (1:15, each = 45)
  epsilon = rnorm (45 * 15, 0, 7.78)
  height_rep = Intercept + L_L[L] + epsilon
  sigma_L_rep[i] = sd (tapply(height_rep, L, mean))
}
```

Below, we see that even in 10,000 simulation we do not see a standard deviation greater than 2.1, and that most are smaller than 1.3. The standard deviation of the listener means in our data was 3.6 and our model estimate of $\sigma_L$ was 3.8. This reinforces the idea that between-listener variation ($\sigma_L$) is a 'real' aspect of our data and that our models benefit from taking it into account.

```{r}
quantile(sigma_L_rep)
```

## Adding a second random effect {#c4-second-random-effect}

Our model is looking better, but is still not 'right'. This is because in addition to repeatedly sampling from only 15 listeners, our model also features repeated sampling from 45 different speakers. In figure \@ref(fig:F4-7) we see that we can recreate figure \@ref(fig:F4-1), this time illustrating variation between speakers instead of listeners. So, we are interested in the average male from the US but have only sampled 45 of them, and we need to build this information into our model.

```{r F4-7, fig.height=3, fig.width=8, fig.cap = "(left) Boxplot of distribution of average height judgments made for each adult male speaker in the experiment. (middle) Individual boxplots of height judgments for each adult male speaker. (right) The marginal distribution of the height judgments shown in the middle plot.", echo = FALSE}

################################################################################
### Figure 4.7
################################################################################

par (mar = c(4,.1,.5,.1), mfrow = c(1,1), oma = c(0,4.2,0,0))
layout (m=matrix(1:3,1,3), widths = c(.15,.7,.15))
boxplot (tapply(men$height,men$S,mean), xlab = "", ylab="Height (cm)",
         col = bmmb::cols, ylim = c(140,195), width = 2,cex.axis=1.3)
abline (h=)
boxplot (height ~ S, data = men, xlab = "Speaker", ylab="",yaxt='n',
         col = bmmb::cols, ylim = c(140,195), cex.lab=1.3,cex.axis=1.3)
grid()
abline (h = mean(mens_height), lwd=2, lty=3)
boxplot (height ~ S, data = men, col = bmmb::cols,add=TRUE,yaxt='n',xaxt='n')
boxplot (men$height, ylab="",yaxt='n', col = bmmb::cols[8], ylim = c(140,195), width = 2)
mtext (side=2,outer=TRUE, text="Height (cm)", cex=1, line=2.9,adj=.55)
```

### Updating the model {#c4-updating-model}

In order to model the effects of the different speakers in our data, our model formula must be updated to include our `S` (speaker) predictor in parentheses, like this:

`height ~ 1 + ( 1 | L) + ( 1 | S)`

The new term in parenthesis indicates a second predictor whose levels we want to estimate using partial pooling. Again, we include only a one on the left hand side of the pipe indicating that we are only estimating speaker-specific intercepts. So, this formula says 'predict height using only an overall intercept, but also estimate a different intercept for each level of listener and speaker'. This regression model corresponding to the formula above now looks like this:

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + L_{[L_{[i]}]} + S_{[S_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\
\\
\mathrm{Intercept} \sim N(176,15) \\
\sigma \sim N(0,15) \\
\sigma_L \sim N(0,15) \\
\sigma_S \sim N(0,15)
\end{split}
(\#eq:26)
\end{equation}
$$

Just as with the listener coefficients ($L_{[\;]}$), the speaker coefficients ($S_{[\;]}$) represent deviations in values from the intercept and so the speaker average is set to zero. In plain English, the model description above says:

> We expect height judgments to be normally distributed around the expected value for any given trial, $\mu$, with some unknown standard deviation $\sigma$. The expected value for a trial is equal to the sum of a fixed overall average (Intercept), some value associated with the individual listener ($L_{[L_{[i]}]}$) who judged the trial, and some value associated with the individual speaker ($S_{[S_{[i]}]}$) who produced the trial. The listener and speaker coefficients ($L$ and $S$) were both modeled as coming from normal distributions whose standard deviations, $\sigma_L$ and $\sigma_S$, were estimated from the data. Prior distributions for $\sigma$, $\sigma_L$, $\sigma_S$, and the intercept were all specified a priori. 

The model we fit last chapter only included variation due to error. The first model we fit in this chapter divided variation into random between-listener variation and random within-listener error. Now, our model attempts to break up the total variance in our data into three components: Between-listener variation, between-speaker variation, and random error, as seen below. 

$$
\sigma^2_{total} = \sigma^2_{L} + \sigma^2_{S} + \sigma^2
(\#eq:210)
$$

If the error was the within-listener variation before, what does it represent now? Our error is now the difference between the value we expect, given the speaker *and* listener, and the value we observe. So, it is the variation *adjusting for* the listener and listener. In other words, if you know who the speaker is and who the listener is, and you adjust for this, how much random variability do you still have left over in your model? The answer is $\sigma$.   

### Fitting and interpreting the model

Below, we fit the new model using exactly the same code as for `model_multilevel`, save for the modification to the model formula. We don't need to specify a hyperprior for $\sigma_S$ because this falls under the `sd` category (see section \@ref({c3-fitting-1})). As long as we can use the same hyperprior for each factor-specific standard deviation, i.e. $\sigma_{F}$ for factor $F$, we don't need to specify a prior for each parameter. 

```{r,eval = FALSE}
# Fit the model yourself
set.seed (1)
model_multilevel_L_S =  
  brms::brm (height ~ 1 + (1|L) + (1|S), data = men, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2,
             prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
                       brms::set_prior("normal(0, 15)", class = "sd"),
                       brms::set_prior("normal(0, 15)", class = "sigma")))
```

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_multilevel_L_S = bmmb::get_model ('4_model_multilevel_L_S.RDS')
```
```{r, include = FALSE}
# saveRDS (model_multilevel_L_S, '../models/4_model_multilevel_L_S.RDS')
model_multilevel_L_S = readRDS ('../models/4_model_multilevel_L_S.RDS')
```

After fitting the model we inspect the output:

```{r, eval=FALSE}
bmmb::short_summary (model_multilevel_L_S)
```

The model print statement should look familiar except it contains one new chunk representing information about the speaker predictor in our model. This chunk reminds us that our `S` predictor had 45 levels, and provides us information about the estimate of the $\sigma_S$ parameter (`sd(Intercept)`), which is 2.84 according to our model.

## Investigating 'shrinkage' {#c4-investigating-shrinkage}

We've mentioned that multilevel models can result in *shrinkage*, that is, in coefficient estimates that are smaller in magnitude (i.e. closer to the mean) than their maximum likelihood counterparts. In the code below we extract the posterior mean estimates of our listener and speaker random effects (i.e., $L_{[\;]}$ and $S_{[\;]}$). We also calculate the maximum likelihood estimates for the listener and speaker averages, and center these so that they will resemble their corresponding effects. [@@ I think someone could argue that maximum likelihood estimates would be, e.g., what you get from fitting an `lmer` model, not simply from calculating means like this. Maybe better to call them "simple" estimates of these quantities? Or something like that, if "simple" doesn't feel right.]

```{r, cache = TRUE}
# these are the Bayesian 'shrinkage' estimates
L_hat = brms::ranef(model_multilevel_L_S)$L[,1,'Intercept']
S_hat = brms::ranef(model_multilevel_L_S)$S[,1,'Intercept']

# these are the maximum likelihood estimates
L_ml = tapply (men$height, men$L, mean)
S_ml = tapply (men$height, men$S, mean)
# and now they are centered
L_ml = L_ml - mean (L_ml)
S_ml = S_ml - mean (S_ml)
```

In the top row of figure \@ref(fig:F4-8) we see a comparison of the Bayesian estimates of the speaker and listener effects compared to their maximum likelihood estimates. We can see that the Bayesian listener effects are nearly identical to their maximum likelihood counterparts. In the bottom row we see the difference between our two estimates plotted against the maximum likelihood estimates. In this row we see that the degree to which an estimate is shrunk towards the mean depends on how much it deviates from the mean.   

```{r F4-8, fig.height = 5, fig.width = 8, fig.cap="(top row) Maximum likelihood (ml) estimates of listener and speaker effects plotted against the Bayesian estimates of the same effects (hat). (bottom row) The y-axis now represents the difference between the Bayesian and maximum likelihood estimates. Positive values indicate that the Bayesian estimate was greater than the maximum likelihood one, and vice versa.", echo=FALSE}

################################################################################
### Figure 4.8
################################################################################

par (mfrow = c(2,2), mar = c(.25,4,.25,1), oma=c(4,.1,.1,.1))
plot (L_ml, L_hat, xlim = c(-10,10), ylim = c(-10,10),
      pch=4, lwd=3, col=bmmb::cols,cex=2.25,xaxt="n",xlab="")
grid()
abline (0,1,col=2,lwd=2)
abline(h=0,v=0,lty=3,lwd=2)
points (L_ml, L_hat, pch=4,lwd=3, col=bmmb::cols,cex=2.25)

plot (S_ml, S_hat, xlim = c(-12,12), ylim = c(-12,12),
      pch=4, col=4,cex=2.25,xaxt="n",xlab="")
grid()
abline (0,1,col=2, lwd=2)
abline(h=0,v=0,lty=3,lwd=2)
points (S_ml, S_hat, pch=4,lwd=3, col=bmmb::cols,cex=2.25)

plot (L_ml, L_hat-L_ml, xlim = c(-10,10), ylim = c(-5,5),
      pch=3, lwd=3, col=bmmb::cols,cex=2.25)
grid()
abline(h=0,v=0,lty=3,lwd=2)
points (L_ml, L_hat-L_ml, pch=3,lwd=3, col=bmmb::cols,cex=2.25)

mtext (side = 1, text = "L_ml", line=2.5)

plot (S_ml, S_hat-S_ml, xlim = c(-12,12), ylim = c(-5,5),
      pch=3, col=4,cex=2.25)
grid()
abline(h=0,v=0,lty=3,lwd=2)
points (S_ml, S_hat-S_ml, pch=3,lwd=3, col=bmmb::cols,cex=2.25)

mtext (side = 1, text = "S_ml", line=2.5)
```

Figure \@ref(fig:F4-9) shows the effect of shrinkage for the listener and speaker effects: Individual effects are 'pulled' towards the overall mean (represented by zero). There are two reasons for the listener effects exhbiting less shrinkage than do the speaker effects. First, we have 45 observations for each listener but only 15 for each speaker. As a result the likelihood functions for our listener effects will be much narrower than those of our speaker effects. This has the effect of making our listener parameters more resistant to the effects of the prior, and therefore more resistant to shrinkage (see sections \@ref(c3-posterior) and \@ref(c2-chars-of-likelihoods) for more information). Second, a larger proportion of the speaker effects are close to the average, with only a relatively small number exhibiting large deviations from the average, whereas the listener effects are less concentrated near the average. The large "mass" of speaker effects near the average causes more shrinkage, particularly on the small number of more extreme values.

```{r F4-9, fig.height = 3, fig.width = 8, fig.cap="Empty points and dotted lines indicate the maximum likelihood estimates for listener and speaker effects, and the densities corresponding to the standard deviations of these effects. The filled points and solid lines indicate the Bayesian estimates and corresponding density estimates.", echo=FALSE}

################################################################################
### Figure 4.9
################################################################################

par (mfrow = c(1,2), mar = c(4,.51,.5,.51))
plot (L_ml,rep(0,15), ylim = c(0,5), cex=1.5,pch=1,col=bmmb::cols, xlim = c(-13,13), 
      yaxt = 'n',xlab = 'Listener Effects')
points (L_hat,rep(1,15), cex=1.5,pch=16,col=bmmb::cols,lwd=2)
arrows (L_ml,0.1,L_hat,0.9, length=0.1)
abline (v=0, lty=3,col='grey')
x = seq(-12,12,0.01)
den = dnorm (x, 0, 3.79)
den = den / max (den)
den2 = dnorm (x, 0, sd(S_ml))
den2 = den2 / max (den2)
text (0,2.5,label = expression(paste(sigma["L"]," = 3.79")), cex = 1.25)
lines (x, (den*2)+2, lwd = 3, col = 4)
lines (x, (den2*2)+2, lwd = 2, col = 4,lty=3)

plot (S_ml,rep(0,45), ylim = c(0,5), cex=1.5,pch=1,col=bmmb::cols, xlim = c(-13,13), 
      yaxt = 'n', ylab = '',xlab = 'Speaker Effects')
points (S_hat,rep(1,45), cex=1.5,pch=16,col=bmmb::cols,lwd=2)
arrows (S_ml,0.1,S_hat,0.9, length=0.1)
abline (v=0, lty=3,col='grey')
x = seq(-12,12,0.01)
den = dnorm (x, 0, 2.84)
den = den / max (den)
den2 = dnorm (x, 0, sd(L_ml))
den2 = den2 / max (den2)
text (0,2.5,label = expression(paste(sigma["S"]," = 2.84")), cex = 1.25)
lines (x, (den*2)+2, lwd = 3, col = 4)
lines (x, (den2*2)+2, lwd = 2, col = 4,lty=3)
```


## Answering our research questions {#c4-answering-question}

Let's return to the research questions we posed in section \@ref(c4-data-and-qs-1):

  Q1) How tall does the average adult male from the US sound?

  Q2) Can we set limits on credible average apparent heights based on the data we collected?

We can consider the answers provided to this question by the complete pooling model we fit at the end of last chapter:

```{r, include = FALSE, eval = FALSE}
model_priors = bmmb::get_model ('3_model_priors.RDS')
```
```{r, include = FALSE}
# load downloaded model from working directory
model_priors = readRDS ('../models/3_model_priors.RDS')
```

```{r, collapse = TRUE}
bmmb::short_summary (model_priors)
```

And compare this to the final model we considered above, which contained information about speakers and appropriate priors:

```{r, collapse = TRUE}
bmmb::short_summary (model_multilevel_L_S)
```

Our complete pooling model from the previous chapter, `model_priors`, and our partial pooling model from this chapter, `model_multilevel_L_S`, agree on the average apparent height as reflected by their respective intercept terms. However, they disagree on a credible interval for that parameter with the complete pooling model having a credible interval that is nearly four times narrower (1.2 cm vs 4.4 cm). This is because the complete pooling model did not specify information about repeated measures, which caused our model to think that it had more independent observations than it did. This led the initial model to return an overly-precise estimate. Another difference is that the final model has a smaller `sigma` parameter (7.8 vs 6.5), which indicates that the error is smaller in the final model than in the initial model. Keep in mind that 'error' is just what your model can't explain at the data level. Our final model explains more (at the listener and speaker levels) and so there is less error (at the data level). The reduced error is a direct consequence of the fact that the final model splits variation in the data into several components (i.e., $\sigma^2_{total}=\sigma^2_{L}+\sigma^2_{S}+\sigma^2$). Since $\sigma_{total}$ is a fixed value (given the data), obviously, the larger your between-speaker and between-listener variation is, the smaller your random error ($\sigma$) *must* be.

Usually, parameters should be reported with *at least* the mean/median and standard deviations of the posterior distribution, in addition to some useful credible interval (i.e., 50%, 95%) around that parameter. Based on the result of our final model, a thorough answer to our research questions might go something like: 

> "Based on our model the average apparent height of adult males is likely to be 174.8 cm (s.d. = 1.1, 95% CI = 171.6, 176.0). The estimated magnitude of the random error was 6.5 cm (s.d. = 0.2, 95% CI = 6.1, 6.8). Consistent between-listeners variation averages about 3.8 cm (s.d. = 0.9, 95% CI = 2.5, 5.8), while consistent between-listeners variation averages about 2.8 cm (s.d. = 0.4, 95% CI = 2.1, 3.8).

## Frequentist corner {#c4-frequentist}

In frequentist corner, we're going to compare the output of `brms` to some more 'traditional', often called *frequentist* approaches. We're not going to talk about the traditional models in much detail since there are hundreds of other sources for this, and this book is long enough as it is (see section X for suggestions). The focus of these sections is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in the different models. If you are already familiar with these approaches, these sections may be helpful. If not, feel free to skip these sections of the book, although they may still be helpful.  

### Bayesian multilevel models vs. lmer {#c4-vs-lmer}

Here we compare the output of `brms` to the output of the `lmer` ("linear mixed-effects regression") function, a very popular function for fitting multilevel models in the lme4 R package. The `lmer` function uses partial pooling to estimate its 'random effects' and no pooling to estimate its 'fixed effects'. We are not going to talk about `lmer` in very much detail. The focus of this section is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in both kinds of models. Below we use `lmer` to fit a model that is analogous to our `model_multilevel_L_S` model. Notice that the formulas are identical in both cases, except we do not specify priors for this model.

```{r, warning=FALSE, message = FALSE, collapse = TRUE, cache = TRUE}
set.seed (1)
lmer_model = lme4::lmer (height ~ 1 + (1|L) + (1|S), data = men)
```

We can consider the short print statement for our Bayesian model, which we have annotated with numbers. 

```{r, eval = FALSE}
bmmb::short_summary(model_multilevel_L_S)

# (1) Formula:  height ~ 1 + (1 | L) + (1 | S)
# 
# (2) Group-Level Effects:
# ~L (Number of levels: 15)
#               Estimate Est.Error l-95% CI u-95% CI
# (3) sd(Intercept)     3.79      0.86     2.51     5.83
# 
# ~S (Number of levels: 45)
#               Estimate Est.Error l-95% CI u-95% CI
# (4) sd(Intercept)     2.84      0.43      2.1     3.77
# 
# Population-Level Effects:
#           Estimate Est.Error l-95% CI u-95% CI
# (5) Intercept   173.83      1.12   171.64   176.07
# 
# Family Specific Parameters:
#       Estimate Est.Error l-95% CI u-95% CI
# (6) sigma     6.47      0.18     6.12     6.84
```

And compare this to the print statement of our `lmer` model, annotated with the same numbers. We see that our models provide very similar information, although it is organized (and named) in somewhat different ways. We also see that the two approaches provide us with very similar parameter estimates for our models. 

```{r, eval = FALSE}
summary (lmer_model)
#     Linear mixed model fit by REML ['lmerMod']
# (1) Formula: height ~ 1 + (1 | L) + (1 | S)
#        Data: men
# 
#     REML criterion at convergence: 4527.4
# 
#     Scaled residuals:
#         Min      1Q  Median      3Q     Max
#     -4.6205 -0.4868  0.0722  0.5700  2.7179
# 
# (2) Random effects:
#      Groups   Name        Variance Std.Dev.
# (3)  S        (Intercept)  7.593   2.756
# (4)  L        (Intercept) 11.990   3.463
# (6)  Residual             41.630   6.452
#     Number of obs: 675, groups:  S, 45; L, 15
# 
#     Fixed effects:
#                 Estimate Std. Error t value
# (5) (Intercept)  173.788      1.015   171.3
```

We can get the random effects out of our `brm` model and compare them to the random effects we get from `lmer`. As seen below, the random effects estimates we get from `brm` also include credible intervals. As a result, we have some idea regarding the uncertainty in these estimates.

```{r, collapse = TRUE, cache = TRUE, eval = TRUE}
brms_ranefs = brms::ranef (model_multilevel_L_S)$L[,,"Intercept"]
head (brms_ranefs)
```

In contrast, `lmer` gives you what are called **point estimates**. These are single estimates of parameter values with no intervals indicating uncertainty. Because of this, it is difficult to compare estimates for different speakers/participants in the data. 

```{r, collapse = TRUE, eval = TRUE}
lmer_ranefs = lme4::ranef (lmer_model)[["L"]]
head (lmer_ranefs)
```

Importantly, the values we get from both approaches are nearly identical, as seen below, the *largest* difference between the two sets of coefficients is 0.05 cm for the listener effects and 0.06 cm for the speaker effects, with the average absolute difference for both being around 0.03 cm. So, as a practical matter, analyzing this data using a Bayesian multilevel model provides some advantages (e.g. intervals around random effects), while still providing effectively the same 'answers' as a 'frequentist' approach to the data. 

```{r F4-10, fig.width=8, fig.height=3, fig.cap="(left) In green, the random speaker intercept estimates provided by brm. The red arrows indicate the estimates of the same provided by lmer. .", echo = FALSE, eval = FALSE}

################################################################################
### Figure 4.10
################################################################################

brms_ranefs_S = brms::ranef (model_multilevel_L_S)$S[,,"Intercept"]
lmer_ranefs_S = lme4::ranef (lmer_model)[["S"]]


par(mfrow = c(1,2), mar = c(4,4.5,1,1))

brmplot (brms_ranefs, xlab="Speaker", ylab="Speaker Effect (cm)", col=deepgreen, line = FALSE)
abline (h = 0)

points (lmer_ranefs[,1],pch=4,cex=1.5,lwd=2,col=coral, xaxt='n',xlab='',
      ylab="Speaker Effect (cm)")

brmplot (brms_ranefs_S, xlab="Speaker", ylab="Speaker Effect (cm)", col=deepgreen, line = FALSE)
abline (h = 0)

points (lmer_ranefs_S[,1],pch=4,cex=1.5,lwd=2,col=coral, xaxt='n',xlab='',
      ylab="Speaker Effect (cm)")

#plot (brms_ranefs[,1], lmer_ranefs[,1], lwd=2, cex=1.5, col=deeppurple,
#      xlab = 'brms Random Effects (cm)', ylab = 'lmer Random Effects (cm)', pch=16)
#abline (0,1,col=2)
#abline (h = 0, v = 0, lty=3)
```



