\newpage

# Variation in parameters, error variance, and model comparison 

Before powerful computers were easily available, statisticians had to come up with clever solutions to answer difficult questions. The reason for this is that things needed to be solved with pen and paper, or eventually calculators, and this ruled out extremely time-consuming solutions such as sampling from the posterior distribution of models as complicated as the ones discussed here. Approaches to answer commonly-encountered experimental designs (and corresponding data structures) often involved making simplifying assumptions that make arriving at a solution simpler. For example, when comparing two groups statisticians might say "we can solve this problem if we assume that the errors are normally distributed and that the error variance is the same across all conditions". These assumptions are rarely true in practice (and never *exactly* true in reality), but are usually close enough to the truth to make our models be useful. There is nothing particularly 'wrong' with making these assumptions. However, when we build our own Bayesian models we don't need to make them, and so it is worth considering if we might benefit from making other sort of assumptions. 

An analogy can be made to building your own furniture vs. shopping at a furniture store. The furniture at the store is perfectly fine and if you find something that suits you it will be fast and convenient. What happens if what you want is *not* at a store? Well if you don't know how to build your on furniture you are out of luck, you better just find the existing piece that is most similar to what you need, and try to make the most of it. Working with 'traditional' models based on maximum-likelihood estimation (discussed in X) is like going to the store. There are often a limited number of choices, and these choices make certain, often rigid, simplifying assumptions. Learning to use `brms` (or STAN directly) gives you the flexibility of trying many different assumptions, allowing you to build a model that might be very difficult to find 'at the store', but which is well-suited to your data.

In the last chapter we focused on models comparing two groups. Our approach to this was a bit old-fashioned in that it adhered to the limitations of more 'traditional' models. Some of the design choices made that are more 'traditional' are:

  (1) We did not consider the possibility of random variation in the effect of apparent speaker age across listeners. 

  (2) We assumed that the error variance was the same for all conditions, all speakers and all listeners. 
  
  (3) We assumed that the error distribution was normal. 
  
In this chapter we will revisit the models we discussed last chapter. However, this time we will revisit, and change, the assumptions we made above.

## Data and research questions

We're going to use the same data from the last chapter, we are going to exclude adult males and focus on the apparent height of adult females, girls and boys. We're going to build models that build on those of last chapter to ask the same questions:

  (Q1) How tall do speakers perceived as adult females sound? 
  
  (Q2) How tall do speakers perceived as children sound?
  
  (Q3) What is the difference in apparent height associated with the perception of adultness? 
 
Below we load the data and add a column (`A`) that indicates the apparent age of the speaker, `c` for child and `a` for adult.

```{r}
library (brms)
library (bmmb)
data (height_exp)
tmp = as.numeric(height_exp$L) * 0 
count=1
for (i in c(15,2,1,14,3,13,4,12,5,6,11,10,8,7,9)){
  tmp[as.numeric(height_exp$L)==i] = count 
  count = count + 1
}
height_exp$L = tmp

notmen = height_exp[height_exp$C_v!='m' & height_exp$C!='m' & height_exp$R=='a',]
notmen$A_v = c("c","a")[(notmen$C_v == "w")+1]
```

We recreate figure \ref(fig:51) as figure \ref(fig:61) below to show the distribution of apparent height by listener, and across apparent ages. We can use this figure that some of the design choices we used to build our model in the last chapter may not be totally valid for this data. First, we see that the effect of apparent age, the separation between the two boxes for each speaker, does appear to vary substantially between listeners. Second, we see that the error for this model, reflected by the width of the boxes, also seem to vary between listeners, and so many not be equal across all conditions. Third, our figure shows the presence of substantial numbers of outliers, the points individually plotted (beyond the *whiskers* for each box), suggesting that the error distribution may be more like a t-distribution than a normal one. In this chapter we are going to build models that allow for all of these details in the data to be estimated and represented by the model. However, before continuing to make our model more complicated we're going to address the question: Is any of this worth it?

```{r F61, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.1
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1)); layout (mat = t(c(1,2)), widths = c(.7,.3))
boxplot (height ~ A+L, data=notmen, col = rep(cols,each=2),ylim = c(100,200))

boxplot (height ~ A, data=notmen, col = c(beige,lightpink),ylim = c(100,200))
```

## Model Comparison

Bayesian models allow you to fit rather complicated 'bespoke' models for your data. A potential problem with this is at some point your model may be 'good enough' such that, although further tweaks could be made, they are no longer contributing any practical benefit to your model. In fact, you can build a model that is so good at predicting your own data that it can actually become bad at understanding the subject of investigation more generally. An analogy can be made between model fitting and bespoke clothing made at a tailor. You can go to a tailor and get a shirt made that exactly fits your body proportions. We can consider that this involves altering the length of the sleeves relative to torso length, adjusting the circumference of the sleeves and torso independently, and perhaps even making each sleeve a lightly different length. Let's imagine that tailoring the perfect shirt for a human body usually involved adjusting ten 'shirt parameters' to ensure a perfect fit for an individual. Despite this, when you go to a store to buy a shirt, there is usually one parameter, size, and it often comes in a small number of variants (e.g. small, medium, large). Since the shirts vary only in a small number of way, they are very unlikely to fit any individual perfectly. Why do stores intentionally sell clothing that doesn't fit people as well as they could? Because the store is interested in selling clothes that fit everyone *pretty well*, rather than selling clothes that fit everyone perfectly. If stores sold shirts that fit people perfectly, each shirt model would only be useful to a small number of people and would be useless for a large number of people. In contrast, by having a small number of shirts that fit the most people 'pretty well', the store can carry a small number of models and apply them successfully to customers in general.  

This general problem is sometimes referred to as the **bias-variance tradeoff**. In this case *bias* refers to the model's ability to represent the 'true' relationship the underlies the data, and variance refers to the degree to which the model is stable across data sets. Models that fit training data very well (i.e. the have a low variance) often change substantially when fit to new data sets (i.e. they have a high variance). This is analogous to the fact that the shape of a perfectly tailored shirt will potentially be very different across two people. On the other hand models that do not fit the data *too* well (i.e. they have a higher bias) often also have more consistent properties across new data sets (i.e. they have a low variance). This is analogous to the fact that two people are much more likely to pick the size size from a small, medium, or large choice of shirt, and these shirts are likely to fit each of these people pretty well. These issues can be understood with repsect to the extent to which our models can explain the behavior of in-sample dat and out-of-sample data. Your **in-sample** data is the data you used to fit the model, and the **out-of-sample** data is other data that you do not have access to. A statistical model is **overfit** when it corresponds too closely to your in-sample data, to the detriment of its ability to explain out-of-sample data. Recall that our models are usually used to carry our inductive reasoning: Going from a limited number of observations to the general case. In light of this, overfitting a model is exactly contrary to the goals of inductive inference. A model that does a poor job of generalizing to new observations is not useful to carry out induction. Further, a model that cannot explain out-of-sample data will not hold up to replication, which should be a concern for any researcher. This is because a researcher carrying out a replication will necessary be working with data that, from your perspective, is out-of-sample data. 

In this section we will offer a high-level conceptual understanding of Bayesian model comparison, along with an explanation of how to carry this out. For more information on the subject we recommend reading chapter X of the truly excellent *Statistical Rethinking* (cite). For an even more thorough treatment on the subject please see X and X. Much of this section is information is  provided in Gelman et al (2013), and with more mathematical detail.

### In-sample and out-of-sample prediction

In order to compare models in a quantitative way, we need some number that can calculated for different models. For Bayesian models, we begin with what is called the **log pointwise predictive density**, abbreviated `lpd` (or `lppd`). The `lpd` can be calculated in several ways, but equation \@ref(eq:61) presents an example of the general case. The `lpd` is the sum of the logarithm of the density for each of your data points. The symbol $\theta$ represents posterior estimates of all of the estimated parameters necessary to determine the probability density over each data point. This value is clearly related to the log posterior density, discussed in section \@ref(c3-log-posterior). Since we are adding logarithmic values, we know we are multiplying the underlying probabilities. If the data points are independent given our parameter values, the `lpd` represents the joint proability of observing the data given the model structure and the parameter estimates ($\theta$). Although prior probabilites are not included in this calculation they do factor into the estimation of $\theta$, and as a result do have an effect on values of `lpd`. 

$$
\begin{equation}
\mathrm{lpd} = \sum_{i=1}^{N} \mathrm{log} (p(y_i | \theta))
(\#eq:61)
\end{equation}
$$

Because $\mathrm{lpd}$ is a logarithmic value, we know large negative values are closer to zero. So, values of $\mathrm{lpd}$ closer to (or above) zero represent models that are more likely given the data. As a result, we might think that we should simply select the model with the highest $\mathrm{lpd}$ as our 'best' model. However, if we are not specifically interested in the in-sample prediction alone, then what we need to be concerned with is the possible characteristics of the out-of-sample prediction. We can think about this in terms of the **expected log pointwise predictive density**, abbreviated $\mathrm{elpd}$ (or $\mathrm{elppd}$), presented in \@ref(eq:62). Unlike the $\mathrm{lpd}$, the $\mathrm{elpd}$ is defined in terms of hypothetical out-of-sample data ($\tilde{y}$) instead of the in-sample data ($y$). The $\mathrm{E}()$ below represents the expected value function, which we have not really discussed to this point. The expected value of a variable is the ... [@@ SB - I don't know how to define this in the general case in an intuitive way]. So, the $\mathrm{elpd}$ basically represents the expected value of $\mathrm{lpd}$ for out-of-sample data. 

$$
\begin{equation}
\mathrm{elpd} = \sum_{i=1}^{N} \mathrm{E}(\mathrm{log} (p(\tilde{y}_i | \theta)))
(\#eq:62)
\end{equation}
$$

Of course, you can't actually know the value of $\mathrm{E}(\mathrm{log} (p(\tilde{y}_i | \theta))$ because you do not have access to the true properties of ($\tilde{y}$). As a result, you must settle for an estimate of $\mathrm{elpd}$, $\widehat{\mathrm{elpd}}$. The simplest, and worst, way to estimate out-of-sample prediction($\mathrm{elpd}$) is by simply looking at the in sample prediction ($\mathrm{lpd}$). One of the reasons that this does not generally work is because making our models more and more complicated will generally improve your in-sample prediction. However, if the predictors are not related to true characteristics of the underlying model, they will also tend to decrease the fit of the model to new data. Returning to the analogy of shirt fits discussed above. You can buy a shirt from the store and take it to a tailor to get it customized. The more the tailor changes this from the 'standard' shape, the better the shirt will fit you and the worse it will fit everyone else. Only alterations which conform to the 'true' average torso shape will increase the fit for people in general (and may in fact reduce the fit for you).

We can demonstrate this for some very simple models using simulations. In the code below, we generate three random variables consisting only of the values -1 and 1. We then use only the first variable (`x1`) to simulate some random data, and we add random Gaussian error to this fake variable. After this we generate two sets of data using the same data generating process, in_sample (`y`) and out-of-sample data (`y_tilde`). We then fit three models to both data sets. These models include increasingly more predictor variables, however, we know that only the first one is useful to understand the underlying process. The models are fit using a function that we have not discussed to this point. The `lm` (linear model) function uses maximum-likelihood estimation (see section X) to fit regression models with relatively simple structures. We use it here because it estimates regression parameters very quickly and therefore is reasonable to use when we want to fit 3000 models in no more than two seconds. We use the predicted values ($\mu$) and $\sigma$ estimates provided by our maximum-likelihood models to estimate the $\mathrm{lpd}$ and $\mathrm{elpd}$ for each model. In each case, we do so by calculating as in \@ref(eq:63)

$$
\begin{equation}
\mathrm{lpd} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(y_i | \mu, \sigma))
(\#eq:63)
\end{equation}
$$
and \@ref(eq:64), where the $\tilde{y}$ is the out-of-sample data being predicted using the in-sample estimates of $\mu$ and $\sigma$ associated parameter estimates.

$$
\begin{equation}
\mathrm{elpd} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(\tilde{y}_i | \mu, \sigma))
(\#eq:64)
\end{equation}
$$

In the formulas above, and in the code below, we replace $p(\tilde{y}_i | \theta)$ with $\mathrm{N}(y_i | \mu, \sigma)$ because the simulations below generate normally-distributed data. 


```{r, cache = TRUE}
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
lpd = matrix (0, iter, 3)
elpd = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)
  x3 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out-of-sample" data
  y_tilde = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y ~ 1+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y ~ 1+x1 + x2)
    if (j==3) mod = lm (y ~ 1+x1 + x2 + x3)
    
    # find the predicted value (mu) for each data point
    yhat = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    lpd[i,j] = sum (dnorm (y, yhat, sigma, log = TRUE))
    elpd[i,j] = sum (dnorm (y_tilde, yhat, sigma, log = TRUE))
  }
}
```

The above simulations result in three sets of $\mathrm{lpd}$ and three sets of $\mathrm{elpd}$, a pair of each for each of the models above. The average of each of these six sets of values is presented in figure X. Since logarithmic values closer to zero are closer to one, *more negative* log-likelihood values indicate a *less probable* outcome. We can see that the $\mathrm{lpd}$ increases as the model becomes more complicated, despite the fact that the $x_1$ and $x_2$ had absolutely no relationship to the data generating process or to the dependent variable. Why does this happen? One way to think about it is that the tallest person in California *and* Arizona will be *at least* as tall as the tallest person in Arizona. By increasing the number of ways it can explain your data, you more complex model will do *at least as well* as the model with fewer possible explanations. However, although the extra parameters increase the $\mathrm{lpd}$, they actually *decrease values* of $\mathrm{elpd}$. Why? The extra predictors included in our model in no way relate to our data, the 'answers' they provide can only be due to overfitting, the learning of chacracteristics that are specific to the in-sample data rather than properties of the out-of-sample data. 

```{r F62, fig.height = 3, fig.width = 8, fig.cap=" (left) Average value of lpd and elpd for each model in the simulations above. (right) The same values as on the left, however, now the elpd is estimated based on the number of parameters.", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.2
################################################################################

means = round (apply (cbind(lpd,elpd), 2, mean),1)

layout (mat=t(c(1:3)), widths = c(.4,.4,.2))
par (mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))

plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "lpd",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
arrows (1,means[1]-0.2, 1,means[4]+0.2,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[5]+0.2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[6]+0.2,code=3, length=0.1,lwd=2)
text ((1:3)+.1, -71, labels = round(c(means[1]-means[4],means[2]-means[5],
                                means[3]-means[6]),2))
text (0.8,c(-68.8,-73.6), labels = c("More Likely","Less likely"), pos=4)

mtext (side=2,text = "log density", line = 2.75)
#legend (3.4,-70.5, legend= c("In sample","out-of-sample"), 
#        col=c(1,2), lwd=2,bty="n")

plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))

plot (0, bty='n', xaxt='n',yaxt='n',xlab="",type="n")
legend (.6,.2,legend = c("lpd","elpd","elpd_hat"), lwd=4, 
        col = c(1,2,2),cex=1.4,lty = c(1,1,3))
```

So, we see that adding 'unneccessary' models can decrease our out-of-sample performance. Ok, so could we just not ever include any 'unnecessary' predictors in our models? Obviously this would be ideal, however, there are some complications. In the example in figure \@ref(fig:F63) we knew the true underlying model and generated out-of-sample data that exactly conformed to the characteristics of our sample. In real life, researchers do not usually have access to out-of-sample data, they do not know the characteristics of the 'true' model that underlies their data, nor can they confirm that any out-of-sample data shares the exact underlying process as their in sample data. As a result, we can never *really* know what the difference is between in sample and out-of-sample prediction for our models. However, you may have noticed that: 1) The slopes of the lines representing $\mathrm{elpd}$ and $\mathrm{lpd}$ in figure \@ref(fig:F63) seem to have predictable slopes, 2) The lines diverge form each other at predictable rates based on the complexity of the models. Mathematicians have noticed this too, and have used this to *adjust* $\mathrm{lpd}$ calculate on in-sample data in order to estimate values of $\mathrm{elpd}$ for out-of-sample data. 

### out-of-sample prediction: Adjusting predictive accuracy

The logic of adjusting $\mathrm{lpd}$ based on model complexity can be understood with reference to figure \@ref(fig:F63). Our goals is to select the model with the best out-of-sample prediction (i.e. the highest value on the red line), given only knowledge of the in-sample prediction (the values on the black line). In the left plot we can see that the difference between the black and red lines for each model increases by one for every parameter we add. Let us assume for the time being that this is a general property of all models. This means we can subtract one from the $\mathrm{lpd}$ of our least-complicated model to estimate the $\mathrm{elpd}$ for that model, and subtract three from the likelihood of the most-complicated model to estimate the $\mathrm{elpd}$ for that model. In general we can think of this as **penalizing** the value of $\mathrm{lpd}$ based on some penalty value $\mathrm{p}$. Different ways to estimate $\widehat{\mathrm{elpd}}$ differ in terms of how they estimate $\mathrm{lpd}$ and  $\mathrm{p}$. In the very simple example we are discussing here, we are setting $\mathrm{p}=k$, where $k$ is the number of parameters estimated by the model. If we set $\mathrm{p}=k$ and carry out the operation in \@ref(eq:65) usng the values in figure \@ref(fig:F63), we would arrived at values of $\mathrm{elpd}$ of -70.7 (-69.7-1) and -71.7 (-68.7-3) for the least and most complex models. If we chose models based on these estimates of $\widehat{\mathrm{elpd}}$ we would suggest the model with the worst $\mathrm{lpd}$, but the one that actually corresponded to the true data-degenrating process.

$$
\begin{equation}
\widehat{\mathrm{elpd}} = \mathrm{lpd} - \mathrm{p}
(\#eq:65)
\end{equation}
$$

Penalization will not always result in the highest values of $\widehat{\mathrm{elpd}}$ for the simplest model. In figure \@ref(fig:F64) we simulate new fake data, except now we include $x_2$ to the 'real' underlying data generating process (i.e., `y = 1 + x1 + rnorm (n, 0, 1)`). So, for this data we actually *do* need both $x_1$ and $x_2$ in the model. As seen below, penalization does not obscure the benefit of adding $x_2$ to our model when it is warranted. This is because the relatively small penalty associated with the increased model complexity does not overwhelm the large benefit due to the inclusion of a predictor that is actually related to our dependent variable. 

```{r F64, fig.height = 4, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.4
################################################################################
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
in_sample_ll = matrix (0, iter, 3)
out_of_sample_ll = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y_in = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out-of-sample" data
  y_out = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y_in ~ 0+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y_in ~ 1+x1)
    if (j==3) mod = lm (y_in ~ 1+x1 + x2)
    
    # find the predicted value (mu) for each data point
    yhat = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    in_sample_ll[i,j] = sum (dnorm (y_in, yhat, sigma, log = TRUE))
    out_of_sample_ll[i,j] = sum (dnorm (y_out, yhat, sigma, log = TRUE))
  }
}



means = round (apply (cbind(in_sample_ll,out_of_sample_ll), 2, mean),1)

par (mfrow = c(1,2), mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))
plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "Model log-likelihhod",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
mtext (side=2,text = "Log-likelihood", line = 2.5)
#legend (3.4,-70.5, legend= c("In sample","out-of-sample"), 
#        col=c(1,2), lwd=2,bty="n")

  plot (means[1:3], type = "b", lwd = 2, ylim =c(-73.5,-69),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))
```

Historically, the $k$ terms is related to the number of independent parameters estimated by the model. Estimated parameters are those that are not specified a priori but instead depend on the data and structure of the model. This is not so straightforward in our multilevel models since parameters estimated with partial pooling (and shrinkage) are not fully independent. So, for example, a set of 10 'random effects' may in effect act as fewer than 10 totally independent parameters. In addition, the parameters and penalty terms involved in estimates of elpd have traditionally been *point estimates*, single values often based on maximum-likelihood estimation. Since our models consist of posterior distributions of parameters, we instead have *distributions* of lpd and p. In this section we are only going to discuss the **widely available information criterion** (WAIC) as calculated using the brms package. For a more complete treatement of other ways, and historical approaches, please see Gelman et al. (2013). 

To calculate WAIC we first find the average probability of each data point given our model parameters across all posterior samples. Then we find the logarithm of this value and add this up across all of our data. This is presented in equation \@ref(eq:66)

$$
\begin{equation}
\mathrm{lpd_{waic}} = \sum_{i=1}^{N} \mathrm{log} (\frac{1}{S} \sum_{s=1}^{S} p(y_i | \theta^s))
(\#eq:66)
\end{equation}
$$

$$
\begin{equation}
\mathrm{p_{waic}} = \sum_{i=1}^{N} Var_{s=1}^{\,S}(\mathrm{log} (p(y_i | \theta^s)))
(\#eq:67)
\end{equation}
$$

Why does this work? This is one of those thing that you may need to get used to rather than understand, although explanations can be found in?.


$$
\begin{equation}
\widehat{\mathrm{elpd}}_{waic} = \mathrm{lpd_{waic}} - \mathrm{p_{waic}}
(\#eq:68)
\end{equation}
$$




Below we load the models we fit in the last chapter:

```{r, eval = FALSE}
# Or download it from the GitHub page:
model_sum_coding = bmmb::get_model ('5_model_sum_coding.RDS')
model_sum_coding_t = bmmb::get_model ('5_model_sum_coding.RDS_t')
```
```{r, include = FALSE}
model_sum_coding = readRDS ('../models/5_model_sum_coding.RDS')
model_sum_coding_t = readRDS ('../models/5_model_sum_coding_t.RDS')
```

And set our options to sum coding to match the coding we used when we fit the models:

```{r}
options (contrasts = c('contr.sum','contr.sum'))
```

We can use the `add_criterion` function in `brms`, and specify `criterion="waic"` to add the `waic` criterion to our model object. We do this for both our model with the normal and the t distributed error. 

```{r, cache = TRUE}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="waic")

model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="waic")
```

Adding the `waic` criterion to our model with Gaussian errors `model_sum_coding`) returns an error message. To understand why we get these errors we can investigate the model `waic` information, which we can see below:

```{r}
model_sum_coding$criteria$waic
```

The three rows represent the expected log pointwise predictive density (`elpd`), the penalty related to the flexibility of the model (`p_waic`) and the third column is the information criterion WAIC (`waic`) which is just -2 times the first row. The `p_waic' penalty term is sometimes referred to as the 'effective number of parameters', but Gelman (cite) cautions that this usage is somewhat figurative and should not be 'over interpreted'. The effective number of parameters relates to the complexity and flexibility of the model and its ability to adapt to new data. Our sum coding model has 114 estimated parameters (15 L + 1 lsd, 94 s + 1 ssd, 2 fe, 1 sigma, 114 = 16 + 95 + 2 +1) but only about 81 effective parameters. The statistics provided above are calculated individually for each data point. We can get the pointwise information as seen below: 

```{r}
model_waic_info = model_sum_coding$criteria$waic$pointwise
model_t_waic_info = model_sum_coding_t$criteria$waic$pointwise
```

Below we have a look at the first six values: 

```{r}
# first six data points
head (model_waic_info)
```

There are as many rows as data points were used to fit the model. The Estimates above simply correspond to the sum of each column. The standard error (SE) corresponds to the variance of the column times the square root of N (since the statistics are sums and not means). We recreate the summary values seen above:

```{r}
# the sum of each column
colSums (model_waic_info)
# the standard deviation of each column
apply (model_waic_info,2,sd) * sqrt(1386)
```

The error message said that "28 (2.0%) p_waic estimates greater than 0.4". Large `p_waic` suggest a poor fit between data points and the model. To investigate this we can get the posterior mean residuals from each of the models we are considering:

```{r, cache = TRUE}
resids = scale(residuals(model_sum_coding)[,1])
resids_t = scale(residuals(model_sum_coding_t)[,1])
```

We scale our residuals so that they represent standard deviations from the mean, and plot these against the `p_waic` value calculated for each data point by each model. Clearly, large residuals relate to large `p_waic` values. In other words, large `p_waic` values are relate to data points that are a very poor fit for our model. In our model with Gaussian errors this relationship is very predictable and quickly leads to large values in `p_waic`. For our model with t-distributed errors (`model_sum_coding_t`) residuals beyond about two standard deviations do not strongly affect `p_waic`. These differences are due to the different behaviors of the normal and t-distributions in their tails, as discussed in section X. 

```{r}
par (mfrow = c(1,2), mar = c(4,4,1,1))
plot(model_sum_coding$criteria$waic$pointwise[,2], resids, 
     ylim = c(-4.5,4.5), xlim = c(0,1.1), col=cols[6],pch=16)
grid()
abline (v = 0.4)
#abline (h=c(-2.5,2.5))

plot(model_sum_coding_t$criteria$waic$pointwise[,2], resids_t, 
     ylim = c(-4.5,4.5), xlim = c(0,1.1), col=cols[6],pch=16)
grid()
abline (v = 0.4)
```

The warning message suggest that we use `loo` (discussed in the next section) rather than `waic` for model comparison. The reason for this is that, as seen above, some very large residuals in our model with Gaussian error result in large `p_waic` values. As to why large `p_waic` values are bad, when the assumptions underlying `waic` are met, there should not be large `p_waic` values. As a result, large `p_waic` suggest that the assumptions used by this method are being violated. When this occurs, as the error message suggests, you shouldn't use `waic`, even if it seems reliable and looks 'normal'. Think of it this way, if a bridge say it has a weight limit of three tonnes and you're driving a truck that weighs four tonnes, should you drive across the bridge? Maybe you get across safely and save some time. Maybe you crash of the bridge into the river. When you use something like this despite being warned not to you run the same risk: Maybe what you report is a true and reliable analysis, and maybe its not and you are reporting nonsense (the academic equivalent of crashing into the river). 

### Out-of-sample prediction: Cross validation

The last way to evaluate models that we'll be discussing is called **cross validation**. Cross validation consists of dividing your data into two groups. You use one group to fit a model (i.e. in sample data), and then use this model to predict the other group of data. To cross validate you need to pick a group to use for in group data. When one carries out a cross validation,  model predictive accuracy can be measured in any number of application-specific ways. For example, we might be interested in which model can accurately predict height judgments. We can measure predictive error using mean square error or absolute mean or median deviation among others. For each model, the measure (or measures of interest) are collected. This provides a set of observations that can be considered a sample. Researchers can then use this sample to infer the average value and an interval around that value. 

You can leave out just one group (holdout) or split the data into groups and use each group as the out-of-sample data in turn. This is called k fold. A higher k is better because its less prone to bias. The logical endpoint of this is leave one out cross validation, where you leave a single data point out and predicting that. Leave one out cross validation is whats called exhaustive, it divides the data in all possible ways. If you do ten fold you are not using all 10 sized groups. This means that there is more bias since you randomly chose those groups. 

Leave one out cross validation would be very time consuming and impossible for our models. Luckily, there is a package and function do do this in an efficient way. This loo package/function more or less combines elements of cross validation and penalization. It uses an estimation of leave one out cross validation to penalize predictive likelihoods.  

- and it penalizes the model likelihood based on this
- or actually penalizes unnormalized posteriors??


We will compare three models, the model we fit in the last chapter without listener dependent Age effects (random age affects by listener), the model with these age effects, and the model with age effects and also heteroskedastic errors. We will do so using both loo and waic. 

- add them, compare them.
- get one estimate for each set of samples so distribution per model. 
- not so much about "how much is big" as how different are they. 



```{r}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="loo")
model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="loo")
```


```{r}
brms::loo_compare (model_sum_coding, model_sum_coding_t, criterion = "loo")
```


```{r}
bmmb::short_summary (model_sum_coding)

bmmb::short_summary (model_re_t)

```


classic adjustments work in a classic way, undersome basic assumptions thy tend to work





```{r}
brms::loo_compare (model_sum_coding,model_sum_coding_t, criterion ="waic")
```

```{r}
sum(model_waic_info[,1]-model_t_waic_info[,1])
sum(model_waic_info[,1]-model_t_waic_info[,1]) * sqrt(1386)
```





- explain elppd using example from model 1
- it uses this and penalizes in a way based on parameters
- out-of-sample prediction is always worse than in sample
- talk about EXCELLENT explanation in statistical rethinking



- using in sample as out-of-sample - bad
- using penalized in sample - better theoretical
- cross validation - better empirical

- in sample vs out-of-sample
- cross validation
- dont know what you dont know


## Variation in parameters across sources of data

The models we fit in chapter 5 included a predictor representing the apparent age of the speaker. However, this was only a `population-level parameter` (i.e. a 'fixed' effect) meaning that it had the same value for all listeners. An inspection of figure X last chapter clearly suggests that apparent age did not, in fact, have the same effect for the judgments of all listeners. 

Another way to think about this is that our model modeled only the *marginal* effect for apparent age. This is the effect of apparent age on average across all listeners, *independent* of listener. This sort of effect is often referred to as a main effect. Someone might ask "what is the average apparent height difference between children and adult females?" and you might say "about 16 cm". Which listener exactly does this statement apply to? To all of them, this is the average *overall effect*. In contrast, we may want to think about the effect of apparent age *conditional* on listener. Recall from chapter 2 that if the conditional distribution of a variable changes as a function of another variable then these are not independent. In the same way, if the conditional effects of a predictor change as a function of another predictor, then these effects are not independent. For example, if the effect of apparent age varies substantially across listeners then if someone asks "what is the average apparent height difference between children and adult females?" you may have to answer "well it depends on the listener".      

```{r}
listener_age_differences = tapply (notmen$height, notmen[c("A","L")], mean)
listener_age_effects = (listener_age_differences[1,]-listener_age_differences[2,])/2
```


```{r F622, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.2
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_age_differences[1,], 
     type = 'b', col = lavender, pch=16, ylim = c(132,172),ylab = "Height (cm)",lwd=2)
lines(listener_age_differences[2,], 
      type = 'b', col = deepgreen, pch=16,lwd=2)
abline (h = c(157, 165, 149), lty = c(1,3,3))

plot(listener_age_effects, lwd=2,lty=1,
     type = 'b', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
abline (h = mean (listener_age_effects))

plot(listener_age_effects, lwd=1,lty=3,
     type = 'n', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
arrows (1:15, rep(mean(listener_age_effects),15), 1:15, listener_age_effects,
        length=.1, lwd=2, col=skyblue)
abline (h = mean(listener_age_effects))

```

When two variables do not have independent effects on your outcome, they are said to **interact** and have an **interaction**. The parameters in your model that help you capture these conditional effects are referred to as interactions or **interaction effects**. So, the 'fixed' effect for `A1` really represents the marginal (overall) effect for the predictor. To investigate the values of this predictor *conditional* on listener, we need to include the listener by `A1` *interaction* in our model.

### Description of our model

Before including interactions in our model, let's take  step back and consider the model formula we used last chapter. Below, we explicitly include the leading one (indicating the intercept) to make the following discussion simpler. 

`height ~ 1 + A + (1|L) + (1|S)`

This formula told our model to draw the A parameter from a fixed prior distribution and to draw the `L` and `S` terms from distributions whose standard deviations were estimated from the data (i.e. using adaptive partial pooling). We can pretend that we want to estimate our `L` terms as 'fixed' effects (i.e. without partial pooling) by pulling it out of the parentheses, as seen below.    

`height ~ 1 + A + L + (1|S)`

We are doing this to highlight the two ways that we can include interactions in our model. Interactions between combinations of fixed effects can be denoted using `:`. For example the formula below says "include the main effect for A and L, and the interaction between A and L". The `:` symbol can be read as "given", which helps to highlight that these help represent conditional effects. So `A:L` can be read out loud as "A given L" or "the effect of A given the value of L". 

`y ~ 1 + A + L + A:L`

As a shortcut, we can use the symbol `*` which means "include the main effects for these predictors and all interactions between them."

`y ~ 1 + A * L`

So, if we wanted to include the marginal effects of age and listener, and the interaction between them we could include them in either of the two ways below, provided we want to estimated these without partial pooling. 

`height ~ 1 + A + L + A:L + (1|S)`

`height ~ 1 + A * L + (1|S)`

However, we know that is is advisable to estimate factors with large numbers of levels using partial pooling, regardless of how 'random' the effect may be (see section X). This applies to the marginal effects of predictors such as listener, but also to the interactions between these 'random' effects and our 'fixed' effects. This leads to the second way of including interactions in our model. Whenever any fixed effects interacts with our random effects, it is most common to include these inside the parentheses belonging to the random effect they interact with, like so:

`height ~ 1 + A + (1 + A|L) + (1|S)`

When we do this, we tell our model to fit the age by listener interaction using partial pooling just like we did for the listener intercept effects. Actually, our 'listener intercept effects' are actually an intercept by listener interaction. In other words, out listener effects are simply listener-specific deviations from the intercept conditional on listener, i.e. the listener by intercept interaction. When our formula has a non intercept predictor, we can omit rthe `1` and make our formula as follows: 

`height ~ A + (A|L) + (1|S)`

The formula above tells `brms`: "height varies as a function of an intercept and an age effect, a listener-specific intercept and age effect, and a speaker specific intercept. It also tells `brms` to estimate the speaker intercepts and age effects, and the speaker intercepts as random effects, i.e. using adaptive partial pooling. Our model formula might specify a model that looks like below:

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\
\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12)
\end{split}
(\#eq:61)
\end{equation}
$$

Note that all we have done is added a new term to our prediction equation ($A1 \colon L_{[A_{[i]}]}$), and we have set up the prior for these terms ($A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L})$), and the prior for the term-specific standard deviation parameter ($\sigma_{A1 \colon L} \sim N(0,12)$). This model is perfectly fine, sort of. One potential problem is that we are now estimating two parameters for each listener, an intercept and an effect for `A`, and our model above draws each of these variables from independent distributions as if these were totally unrelated. The problem with treating multiple listener-specific effects as independent is that they are often not. As we have seen repeatedly to this point, treating dependent variables as if they were independent can cause problems for inference. For this reason, when there is more than one group=specific predictor in our model, these are usually treated as draws from a *multivariate normal* distributions, as will be discussed in the following section. 

### Random effects and the multivariate normal distribution

The **multivariate normal** distribution is a straightforward and very useful generalization of the normal distribution. A multivariate normal variable can be thought of as a set of variables whose marginal distributions are themselves normal. Each of these variables represents a **dimension** of the multivariate normal. The dimensions of the multivariate normal can have arbitrarily different means and standard deviations, but their marginal distributions will *always* be a (univariate) normal. Below we 'make' five samples of a three-dimensional normal distribution by independently sampling three univariate normals and sticking these together as columns. Below, each row represents a different sample of the variable and each column represents a different dimension of the variable for that observation. 

```{r}
multivariate_normal = cbind(rnorm (5,0,1),rnorm (5,5,37),rnorm (5,-100,0.001))
multivariate_normal
```

If this were all there is to multivariate normal distributions, then they would not be worth the trouble in many situations. However, treating a set of variables as a multivariate normal distribution allows for the **linear correlation** (often referred to as just the **correlation**) between dimensions to be modeled. The linear correlation between two variables is a value between -1 and 1 which measures the extent two which the two variables tend to vary along straight lines. For example, below we calculate mean height judgments. 

```{r}
listener_means = tapply (notmen$height, notmen[,"L"], mean)
```

In the figure below we plot these, along with the listener age effects on apparent height, calculated (and presented) above. In the rightmost panel we see that these two variables seem to have a negative relationship such that smaller values of average heights tend to be associated with larger age effects. This suggests that these variables are negatively correlated. This is important because, if this is true, it suggests that we are unlikely to observe listener who reported tall speakers on average *and* had large age effects on height. 

```{r F65, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.5
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_means[c(15,1,2,14,3,13,4,12,5,6,11,10,8,7,9)],ylim=c(142,167),cex=1.5, 
     type = 'b', col = lavender, pch=16, ylab = "Height Intercept (cm)",lwd=2,
     xlab = "Listener")
abline (h = mean(listener_means), lty=3)

plot(listener_age_effects[c(15,1,2,14,3,13,4,12,5,11,6,10,8,7,9)], lwd=2,lty=1,cex=1.5,
     type = 'b', col = darkorange, pch=16, ylab = "Age Effect (cm)",ylim=c(1,18),
     xlab = "Listener")
abline (h = mean(listener_age_effects), lty=3)

plot(listener_means,listener_age_effects, lwd=1,lty=3,xlim=c(142,167),ylim=c(1,18),
     type = 'p', col = cols[4], pch=16, ylab = "Age Effect (cm)",cex=1.5,
     xlab = "Height Intercept (cm)")
abline (v = mean(listener_means), h = mean(listener_age_effects), lty=3)

```

We can calculate **Pearson's correlation coefficient** to measure the degree of correlation using the `cor` function in R, which tells us that the correlation between these variables is -0.85. The negative correlation means that a scatter plot of the two variables will have a negative slope left to right. A value of -1 would indicate a *perfect* negative correlation, a value of 0 indicates no correlation, and a value of one indicates a perfect positive correlation. So, the correlation of -0.85 represents a strong negative correlation between these two variables. 

```{r}
cor (listener_means, listener_age_effects)
```

Consider the two vectors below. The element of the second vector is exactly equal to twice every element of the first vector: They are perfectly predictable one from the other. As a result, these two vectors have a correlation of 1. 

```{r}
x1 = c(-1, -1, 1, 1)
y1 = c(-2, -2, 2, 2)
cor (x1, y1)
```

Below we see the opposite situation. The second vector is still twice the first vector, but now every sign differs across the two vectors. These are still perfectly predictable, just backwards. For example, if a gambler were wrong 100% of the time, anyone who did the opposite could perfectly predict the outcome of every game by doing the opposite. Below, we see that these vectors have a correlation of -1.

```{r}
x1 = c(-1, -1,  1,  1)
y1 = c( 2,  2, -2, -2)
cor (x1, y1)
```

In the next example we see that the signs of each element of the second vector are totally *unpredictable* from the corresponding element in the first vector. In half the cases the signs match and in half the cases they do not. This results in a correlation of 0 between the vectors. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2,  2, -2, 2)
cor (x1, y1)
```

Finally, we see a situation where the vectors *almost* match. In the example below three of the four elements match in sign, resulting in a positive correlation between 0 and 1. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2, -2, -2, 2)
cor (x1, y1)
```


The variance of a variables is the expected value (i.e. mean) of squared deviations from the mean. The **covariance** of two variables is the expected value of the product of deviations from their respective means.

$$
\begin{equation}
r = \frac {\sigma_{xy}^2} 
          {\sigma_x \sigma_y}
(\#eq:62)
\end{equation}
$$

The above can be estimated from a sample using the formula below. 

$$
\begin{equation}
r = \frac {\Sigma(x_i-\bar{x})(y_i-\bar{y})} 
          {(N-1) \times \hat{\sigma}_x \hat{\sigma}_y}
(\#eq:63)
\end{equation}
$$

When we estimate random effects for a group as multivariate normal distributions we also estimate the correlation between all pairs of dimensions. In many cases these correlation can influence the likelihood of different outcomes in our model. The easiest way to see the reason for this is by drawing a bivariate (2-dimensional) normal variables with different correlations and plotting the. Below we do this for distributions corresponding to the mean and standard deviations of listener means and age effects. The difference in each row is in the difference in correlation between the two dimensions/variables. 

In the left column below we compare three bivariate normal variables along the two dimensions. In the absence of any correlation between variables, a plot of this distribution will be *spherical* (or circular in 2 dimensions). When there is a correlation between the two dimensions, the distribution starts looking more and more like a straight line. When there is a negative correlation, the line just points down rather than up. 

Note the the marginal (independent) distributions of the variables (the left and right histograms) don't change as the correlation changes. The correlation is a reflection of the *joint* variation in the two variables and will not necessarily be evident in the marginal distributions of each variable.  

```{r F66, fig.height = 6.5, fig.width = 7, fig.cap="10,000 bivatiate normal draws of simulated intercept and slope coefficients from distributions with a mean of 0 and a standard deviation of 1. The correlation of the variables is 0 (top), 0.5 (middle) and 0.9 (bottom). The left column presents both variables together, the middle column presents intercepts and the right column presents slopes. ", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.6
################################################################################

par (mfrow = c(3,3), mar = c(4,4,3,1))

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,0,0,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3)
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,5,5,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,-14.8,-14.8,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()
```

When our dimensions are uncorrelated they are independent. The value of one does not help you understand the other. However, when the dimensions *are* correlated we can use this to make better predictions using our data. For example, an intercept of 2 in the bottom row in the figure above is very likely to be paired with a slope of 2, but *extremely* unlikely to be seen with a slope of -2. In contrast, in the top row a slope of 2 and -2 seem about equally likely given an intercept of 2. So, when we use multiple random predictors per grouping factor, we are really drawing from a multivariate normal distributions that acknowledges the relationships between random predictors in our data, within-cluster (e.g. subject/participant/speaker).  

For example, consider the experiment regarding coffee and speaking rate. Perhaps people who speak fast normally get an even larger boost to their speaking rate from coffee. On the other hand, maybe since they already speak fast, the effect for coffee is diminished in these speakers. In other case, the relationship between the intercept for these speakers (baseline rate) and their coffee effect would not be independent. 

### Specifying priors for a multivariate normal distribution

The shape of the multivariate normal distribution (i.e. how much it looks like a circle vs an ellipse) is determined by a covariance matrix called sigma ($\Sigma$). This matrix is a square $n$ x $n$ matrix for a variable with $n$ dimensions. When we dealt with unidimensional normal distributions for our previous random effects, we specified priors for the (unidimensional) standard deviations using t distributions. The specification of priors for our covariance matrix is only slightly more complicated. 

In our models, we won't actually include priors for $\Sigma$ directly. This is because `brms` (and STAN) build up $\Sigma$ for us from the components we *do* specify. This is more information that you *really* need, but it helps to understand why the priors are specified the way they are for our random effects.

Consider two random effects, a random by subject intercept $\alpha_{[subj]}$, and a random by-subject slope called \alpha_{[subj]}. The covariance matrix for our random effects is created by multiplying the standard deviations of our individual dimensions by a correlation matrix ($R$) specifying the correlations between each dimension. The operation is like this:

$$
\begin{equation}
\begin{split}
\Sigma = \begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} 
\times R \times
\begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} \\
\end{split}
(\#eq:64)
\end{equation}
$$

The values in the outside matrices are the the standard deviations of the random intercepts ($\sigma_{\alpha_{[subj]}}$) and slopes ($\sigma_{\beta_{[subj]}}$) individually. The correlation matrix $R$ contains information about the correlation between the dimensions of the variable (e.g., $\rho_{\alpha_{[subj]} \beta _{[subj]}}$).
  
So, when we have multiple random effects we have a multidimensional variable, and we need to specify priors for each dimension and for the correlation between all dimensions (but not for $\Sigma$ directly). 

We provide priors for the standard deviations of the individual dimensions in the same way as we do for 'unidimensional' random effects (like $\alpha_{[speaker]}$). 

The correlation matrix $R$ will look something like below (for two dimensions). It will contain only values of 1 on the main diagonal and have mirrored values between -1 and 1 off of the diagonal (since the correlation of a and b equals the correlation of b and a).

$$
\begin{equation}
\begin{split}
R = \begin{bmatrix} x & y \\ y & z \\ \end{bmatrix} \\ \\
\end{split}
(\#eq:65)
\end{equation}
$$

We specify priors for variables of this type using the $LKJCorr$ distribution in `brms`. This distribution has a single parameter that determines how peaked the distribution is around 0. Basically, higher numbers make it harder to find larger correlations (and therefore yield more conservative estimates). [See here for an example](https://eager-roentgen-523c83.netlify.app/2014/12/27/d-lkj-priors/).   

$$
\begin{equation}
\begin{split}
R \sim \mathrm{LKJCorr} (2)
\end{split}
(\#eq:66)
\end{equation}
$$

The above was a full explanation of what information the model needs and why it needs it. You don't need to *understand* any of the above to use random effects correctly. The important take away is that whenever you are estimating any random effects above and beyond a random intercept, you need to: 

  1) Specify priors for the standard deviation of each dimension.
  
  2) Specify a prior for the correlation matrix for the multivariate normal used for the random parameters.
  
and `brm` (and STAN) will do the rest.


### Fitting the model

We can fit a model with age coefficients that vary across listeners. Notice that the only change in the formula is the inclusion of the `A` predictor on the left-hand-side of the pipe in `(A|L)`. We now include a prior for a new class of parameter `cor` which applies to the correlation matrices for our multivariate normal variables. In addition, we specify the priors *outside* the `brm` function call, and pass this to the function. We will do this when the number of priors is large enough that including them directly in the function call makes the whole thing look too *busy* (in our opinion). 

```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t_t =  
  brms::brm (height ~ A + (A|L) + (1|S), data = notmen, chains = 4, cores = 4,
       warmup = 1000, iter = 5000, thin = 4, prior = priors, family = "student")
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t_t = bmmb::get_model ('6_model_re_t.RDS')
```
```{r, include = FALSE}
model_re_t_t = readRDS ('../models/6_model_re_t.RDS')
```


### Interpreting the model

We can check out the summary of our model:

```{r}
bmmb::short_summary (model_re_t_t)
```

and see that the only new information is in the `L` group-level effects:

```{r}
## ~L (Number of levels: 15)
##                   Estimate Est.Error l-95% CI u-95% CI
## sd(Intercept)         4.09      0.82     2.83     6.08
## sd(A1)                4.17      0.84     2.88     6.11
## cor(Intercept,A1)    -0.81      0.12    -0.96    -0.50
```

which now includes an estimate of the standard deviation of the by-listener effect for age (i.e. the standard deviation of the listener by age interaction, $\sigma_{A1 \colon L}$ in \@ref(eq:61)), and the correlation between our listener-dependent intercept and age-effect terms. We can see that the estimate of this correlation (-0.81) is very similar to the correlation estimated for these terms in our example above (-0.86).

You can get information about the variance, correlation, and covariance parameters using the `VarCorr` function in the `brms` package. As with the `fixef` and `ranef` function, this function also has the option of setting `summary=FALSE` to get the individual samples for these parameters, while by default it returns summaries of these samples. We are not going to talk much about the structure of this output, but we encourage you to investigate it using the `str` function. 

```{r, echo = FALSE}
varcorr_information = brms::VarCorr (model_re_t_t)
str (varcorr_information)
```

```{r}
varcorr_information$L$cor[,,1]
```

We can load the sum-coded model we fit in the last chapter, which contained an identical structure save for the inclusion of listener-dependent age affects (and associated parameters).

```{r}
model_sum_coding = readRDS ('../models/5_model_sum_coding.RDS')
```

Despite the similarities between the models, there are some changes to the credible intervals around the parameters that are shared across both models (presented in figure \@ref(fig:F65)). It's always risky to make up explanations for things after the fact (post hoc), but we can try to think about why these changes may have occurred. Overall, our interpretation of the changed hinges on the fact that we can account for more variation in our data, and now explicitly allow for random variation to exist in the effect of `A` at the different levels of listener. 

```{r F67, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.7
################################################################################
comparison = rbind (brms::fixef(model_sum_coding),
                    brms::VarCorr (model_sum_coding)$L$sd,
                    brms::VarCorr (model_sum_coding)$residual__$sd,
                    brms::fixef(model_re_t),
                    varcorr_information$L$sd[1,],
                    varcorr_information$residual__$sd)

par (mfrow = c(1,4), mar = c(4,4,3,1))
bmmb::brmplot (comparison[c(1,5),], ylim = c(152,159),main="Intercept", 
               xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(2,6),], ylim = c(6,11),main="A1", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(3,7),], ylim = c(2,8),main="Lsd", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(4,8),], ylim = c(7,9.5),main="sigma", 
              xlim = c(.75,2.25), col = c(4,2))
```

The change in the interval around the intercept is very small, and likely reflects our better overall understanding of the data. In contrast, the interval around `A1` has grown substantially. This is likely because we are know acknowledging the uncertainty between different listeners in the `A1` parameter, which we previously treated as fixed between listeners. Our model predicts a standard deviation of 4.2 cm in this effects between listeners, and our data includes judgments from only 15 unique listeners. This limits how precise our `A1` estimate can be. The standard deviation of our listener intercepts has decreased. This is to be expected as what previously seemed like variation in listener averages may have actually been variation in listener age effects. The separation of this into two separate variables may lead to the magnitude of variation in one being diminished. Finally, the decrease in the error standard deviation is due to the inclusion of the listener random effects for `A1`, and the fact that included listener-dependent values for `A1` helped explain our variable. 


## Varying variances: Heteroskedasticity and distributional (or mixture) models

Most 'typical' regression models assume that the error variance is the same for all observations, all conditions, all listeners, etc. The property of having a homogeneous variance is called **homoscedasticity**, and models that make this assumption can be said to be **homoscedastic**. Historically, homogeneity of variance has been assumed in models because it makes models simpler to understand, and not because it is 'true' for all (or any) data. With Bayesian models it is straightforward to relax and test this assumption, and to build models that exhibit **heteroscedasticity**, differences in the error variance across different conditions. Consider the residuals for the random effects model we just fit to our data, which we can get from the model using the `residuals` function:

```{r, cache = TRUE}
residuals_re = residuals (model_re_t_t)
```

If we make a boxplot of the residuals we see that the distribution of errors is not exactly equal for all listeners. For example, the interquartile range for listener 10 is almost as wide as the entire distribution of residuals for listener 11. The model we just fit (`model_re_t_t`) allowed us to estimate between-listener differences in the apparent age effect on apparent height. In this section we will fit a model with listener-specific error variances ($\sigma$) in addition to listener-specific parameters related to specific effects. By allowing the random error to vary as a function of listener, our model may be able to provide more-reliable information regarding our data, in addition to letting us ask questions like "is listener 11's error variance actually smaller than listener 10's, or does it just seem that way?" in a more formal manner. 

```{r F68, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.8
################################################################################

boxplot(residuals_re[,1] ~ model_re_t$data$L, col = cols)
```

Models that allow for heteroskedastic error are sometimes referred to as **mixture models** or **distributional models**. Mixture models refers to the fact that these models can be thought of resulting from a mixture of several different error distributions. One distribution provides the error in some cases and another in other cases, resulting in heteroskedastic error. Distributional models are those which seeks to model systematic variation in the error, just as more traditional models focus on systematic variation in means. In this section we will fit a distributional model to our data, we will seek to understand systematic variation in both the mean and standard deviation parameters in our model. 

Before continuing, we need to talk about whether our (or your) data can realistically support the estimation of such a model. To this point we haven't discussed this much, but as our models get more and more complicated it is something we need to think about. Our data has between 84 and 94 observations for each listener, and seeks to estimate a listener-specific mean, apparent-age parameter, and error variance. So, we want to estimate three listener-dependent parameters using about 90 observations, which is reasonable. However, if we had only 10 observations per listener perhaps it may not be such a good idea to fit a heteroskedastic model to our data, or to add many more listener-specific parameters. In general, before fitting a model that you *can* fit, it is a good idea to think whether you *should* fit it given the nature of your data and the number of observations you have overall, and in different conditions. 

### Description of our model

Our heteroskedastic model is at the same time more complicated, but also more of the same. The `brms` package makes it exceptionally easy to fit models of this kind by letting you write formulas for the error just as you do for the mean. For example, the model formula for our previous model was:

`height ~ A + (A|L) + (1|S)`

This told `brms` "model apparent height as a function of an intercept, an effect for apparent age, listener-specific adjustments to the intercept and age effect, and speaker-specific intercepts". Implicitly, we know that all of this models variation in the $\nu$ parameter of a normal distribution. We want our model to fit listener-specific error terms, and we want to estimate these with partial pooling since it involves estimating 15 different parameters. We can do this by including a separate formula for our error, called `sigma` by `brms` using the following formula:

`sigma ~ 1 + (A|L)`

This formula tells `brms` "model the standard deviation of the error as varying around an overall intercept and listener-specific deviations (estimated with partial pooling)". We can 'stick' our two model formulas together using the `bf` (Bayes formula) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))
```

Rather than estimate the `sigma` parameter directly, `brms` effectively models `log-sigma`, the logarithm of the error standard deviation (although hit still calls it sigma). The problem with trying to model the standard deviation parameter directly is that it is bounded by zero. As a result, effects need to get smaller and smaller as standard deviations approach zero, otherwise they may push standard deviations into negative territory (which is impossible). One way to deal with this is to log transform standard deviation (or variance) parameters and model those instead. The log transform helps model standard deviations because, just like standard deviations, logarithms are bounded by zero. Logarithms map all numbers from 0 to 1 to values between negative infinity and 0. So, when you model the logarithm of `sigma`, an effect can *always* results in a smaller sigma, a value towards, but never quite at, negative infinity. 

Since we are modeling log-transformed sigmas, we now specify the priors for sigma in a log space. Previously, our prior for sigma was a t-distribution with a standard deviation of 15. This meant that the majority of the mass of the distribution was going to be between 0 and 30 cm. Now, we set the ditribution to 1.5, however, these are logarithmic units. This means we expect the mass of the distribution to be between $exp(-3)=0.05$ and $exp(3)=20.1$. We can use a prior with the same ranges for the standard deviation of our listener-dependent error adjustments ($\sigma_{L_\sigma}$). The full model specification is provided below: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\

\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12) \\ \\ 
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:67)
\end{equation}
$$

This model is quite large, but it only contains five new (or modified) lines compared to model presented in \@ref(eq:61). These lines are presented in \@ref(eq:68). First, our model now uses a trial-specific error term for trial $i$ ($\sigma_{[i]}$), whereas it always used a constant standard deviation previously. Second, we are now modeling this trial-specific $\sigma$ parameter using an intercept specific to our sigma term ($Intercept_{\sigma}$), and a listener-dependent deviation from this ($L_{\sigma[L_{[i]}]}$). Third, we specify the prior for our listener-dependent sigma effects. Finally, in the fourth and fifth lines and specify the prior for our sigma intercept, and for the standard deviation of the listener-specific sigma terms. In a sense, the model expressed in \@ref(eq:68) is effectively the model in \@ref(eq:61), with the additional structure in \@ref(eq:68) added to it. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\ \\
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:68)
\end{equation}
$$

### Fitting the model

For the first time below, we specify both the model formula (using the `bf` function) and the priors (using the `set_prior` function) outside of the call to `brm`. We do this to preserve the legibility of the code in general. Our prior specification includes two new lines to specify the priors for our sigma intercept ($\mathrm{Intercept_{\sigma}}$) and the standard deviations of our sigma-related terms ($\sigma_{L_\sigma}$). These priors are specified with the lines:

```{r}
brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma")
brms::set_prior("student_t(3, 0, 1.5)", class = "sd", dpar = "sigma")
```

Notice that we set these using the classes we have already discussed, `Intercept` and `sd` respectively. However, now we set `dpar` (distributional parameter) to `sigma` to tell `brm` that these priors are specifically for parameters related to the model error term, and not to variation in predicted values (i.e. $\nu$).

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))

priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "sd", dpar = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t_het = 
  brms::brm (model_formula, data = notmen, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t_het = bmmb::get_model ('6_model_re_t_het.RDS')
```
```{r, include = FALSE}
model_re_t_het = readRDS ('../models/6_model_re_t_het.RDS')
```

### Interpreting the model

If we look at the model output:

```{r}
bmmb::short_summary (model_re_t_het)
```

We see a two new lines reflecting our new model parameters. The first new line is in the `Group-level effects` for `L`. This line indicates the standard deviation of the listener-specific sigma terms (`sd(sigma_Intercept)`), which we see is 0.34 (remember this is in a log scale). The second new line is in the `Population-level effects`, and it indicates the overall intercept for our sigma term (also on a log scale). If we exponentiate the value of `sigma_Intercept` ($exp(1.75)$), we get 5.X, the value of the sigma term in bla bla. 

Our listener-specific adjustments to the sigma intercept were fit with partial pooling, i.e. they are 'random' effects. As such, we can get these in the same two ways indicated for other random effects terms in section X. First, we can get these with the `ranef` function, which returns a matrix representing our listener random effects terms.

```{r}
str (ranef(model_re_t_het)$L)
```

After this, we can get just the sigma intercepts, the first five of which are presented below. 

```{r}
ranef(model_re_t_het)$L[1:5,,"sigma_Intercept"]
```

Alternatively we can use the `hypothesis` function to get the listener random effects themselves, or the sum of these with the sigma intercept. 

```{r}
sigmas_centered = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

We compare these in the plot below,  exponenting the recreated sigma values to get the listener-specific standard deviation terms. 

```{r F69, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.9
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
brmplot (sigmas_centered, col = cols)
brmplot (exp(sigmas), col = cols, ylim = c(1,13))
#abline (h = 7.71)
```








## But what does it all mean? 








- here at the end I need a part about how does this invalidate the last model, that reasoning is a problem, bla bla
- model comparison?
- compare previous chapter and this model.
- 95% of published work fatures regular models which we see werent really apprpriate. does that make those fake and results invald. 
 - theres problems with this reasoning
 - also our model is also wrong


So are they different yes or no? Statistics aside, a fair assessment of our data suggests that neither binary conclusion is fully supported: they are distinguishable but overlap substantially. If you want to use this model to highlight the differences between girls and women, I think it is valid. I also think it would be valid to use this data to talk about between and within-speaker variation, highlighting the overlap that exists between speakers. Both are true! To a large extent, the meaning is as much in our heads as it is in the model, and we are free to interpret the results as we see fit, as long as reviewers (and readers in general) will believe us. The model is simply a reflection of the relationships in our data, and the interpretation is up to us. 

Keep in mind that the "as long as reviewers (and readers in general) will believe us" component is crucial. The results of the model will need to be interpreted in the larger context of the work it is presented in, and in terms of scientific and general knowledge that readers have. The results of any model will need to 'make sense' given this, and a statistical result on its own will not be enough to make people (including us) believe outlandish, or even weakly supported claims. 

The model is not reality and should not be confused with reality. This is a very important point! A statistical finding does not *prove* that something is *true*. This kind of thinking has [caused many problems for researchers in the social sciences recently](https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html). In general, we can imagine that 10 people might approach any given research question in 10 different ways, a concept known as [researcher degrees of freedom](https://en.wikipedia.org/wiki/Researcher_degrees_of_freedom). This would cause slight differences in their results, resulting in a sort of 'distribution' for any given result. How can a fixed underlying reality result in a distribution or results? When they are all slightly wrong! 

For example, we know for a fact that f0 varies, weakly but systematically, across vowel categories, a concept known as [intrinsic f0](https://www.sciencedirect.com/science/article/abs/pii/S0095447095801650). A model that included vowel category as a within-speaker predictor would reduce the apparent error in our model (making $\sigma_{error}$ smaller), and might affect the precision of our other estimates. Would this new model invalidate our current model? Answering yes to this question is generally problematic because there is *always* a better model out there, and so every model would automatically be invalid.  

The solution is to think of your model not as a mathematical implementation of *reality* but instead as a mathematical implementation of your research questions. Your model should include the information and structure that you think are necessary to represent and investigate your questions. Using a different model can result in different results given the same data, but asking a different question can also lead to different results given then same data! One of my favorite phrases to use is "given our data and model structure". This phrase is helpful because it highlights the fact that your results are contingent on:

  1) the data you collected. Given other data you may have come to other conclusions.
  
  2) the model you chose. Given another model you may have come to other conclusions.
  










