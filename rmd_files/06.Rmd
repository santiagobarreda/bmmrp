\newpage

# Variation in parameters, error variance, and model comparison 

Before powerful computers were easily available, statisticians had to come up with clever solutions to answer difficult questions. The reason for this is that things needed to be solved with pen and paper, or eventually calculators, and this ruled out extremely time-consuming solutions such as sampling from the posterior distribution of models as complicated as the ones discussed here. Approaches to answer commonly-encountered experimental designs (and corresponding data structures) often involved making simplifying assumptions that make arriving at a solution simpler. For example, when comparing two groups statisticians might say "we can solve this problem if we assume that the errors are normally distributed and that the error variance is the same across all conditions". These assumptions are rarely true in practice (and never *exactly* true in reality), but are usually close enough to the truth to make our models be useful. There is nothing particularly 'wrong' with making these assumptions. However, when we build our own Bayesian models we don't need to make them, and so it is worth considering if we might benefit from making other sort of assumptions. 

An analogy can be made to building your own furniture vs. shopping at a furniture store. The furniture at the store is perfectly fine and if you find something that suits you it will be fast and convenient. What happens if what you want is *not* at a store? Well if you don't know how to build your on furniture you are out of luck, you better just find the existing piece that is most similar to what you need, and try to make the most of it. Working with 'traditional' models based on maximum-likelihood estimation (discussed in X) is like going to the store. There are often a limited number of choices, and these choices make certain, often rigid, simplifying assumptions. Learning to use `brms` (or STAN directly) gives you the flexibility of trying many different assumptions, allowing you to build a model that might be very difficult to find 'at the store', but which is well-suited to your data.

In the last chapter we focused on models comparing two groups. Our approach to this was a bit old-fashioned in that it adhered to the limitations of more 'traditional' models. Some of the design choices made that are more 'traditional' are:

  (1) We did not consider the possibility of random variation in the effect of apparent speaker age across listeners. 

  (2) We assumed that the error variance was the same for all conditions, all speakers and all listeners. 
  
  (3) We assumed that the error distribution was normal. 
  
In this chapter we will revisit the models we discussed last chapter. However, this time we will revisit, and change, the assumptions we made above.

## Data and research questions

We're going to use the same data from the last chapter, we are going to exclude adult males and focus on the apparent height of adult females, girls and boys. We're going to build models that build on those of last chapter to ask the same questions:

  (Q1) How tall do speakers perceived as adult females sound? 
  
  (Q2) How tall do speakers perceived as children sound?
  
  (Q3) What is the difference in apparent height associated with the perception of adultness? 
 
Below we load the data and add a column (`A`) that indicates the apparent age of the speaker, `c` for child and `a` for adult.

```{r}
library (bmmb)
data (height_exp)
tmp = as.numeric(height_exp$L) * 0 
count=1
for (i in c(15,2,1,14,3,13,4,12,5,6,11,10,8,7,9)){
  tmp[as.numeric(height_exp$L)==i] = count 
  count = count + 1
}
height_exp$L = tmp

notmen = height_exp[height_exp$C_v!='m' & height_exp$C!='m' & height_exp$R=='a',]
notmen$A_v = c("c","a")[(notmen$C_v == "w")+1]
```

We recreate figure \ref(fig:51) as figure \ref(fig:61) below to show the distribution of apparent height by listener, and across apparent ages. We can use this figure that some of the design choices we used to build our model in the last chapter may not be totally valid for this data. First, we see that the effect of apparent age, the separation between the two boxes for each speaker, does appear to vary substantially between listeners. Second, we see that the error for this model, reflected by the width of the boxes, also seem to vary between listeners, and so many not be equal across all conditions. Third, our figure shows the presence of substantial numbers of outliers, the points individually plotted (beyond the *whiskers* for each box), suggesting that the error distribution may be more like a t-distribution than a normal one. In this chapter we are going to build models that allow for all of these details in the data to be estimated and represented by the model. However, before continuing to make our model more complicated we're going to address the question: Is any of this worth it?

```{r F61, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.1
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1)); layout (mat = t(c(1,2)), widths = c(.7,.3))
boxplot (height ~ A+L, data=notmen, col = rep(cols,each=2),ylim = c(100,200))

boxplot (height ~ A, data=notmen, col = c(beige,lightpink),ylim = c(100,200))
```

## Model Comparison

Bayesian models allow you to fit rather complicated 'bespoke' models for your data. A potential problem with this is at some point your model may be 'good enough' such that, although further tweaks could be made, they are no longer contributing any practical benefit to your model. In fact, you can build a model that is so good at predicting your own data that it can actually become bad at understanding the subject of investigation more generally. In this section we will offer a high-level conceptual understanding of Bayesian model comparison, along with an explanation of how to carry it out. For more information on the subject we recommend reading chapter X of the truly excellent *Statistical Rethinking* (cite). For an even more thorough treatment on the subject please see X and X. 

An analogy can be made between model fitting and bespoke clothing made at a tailor. You can go to a tailor and get a shirt made that exactly fits your body proportions. We can consider that this involves altering the length of the sleeves relative to torso length, adjusting the circumference of the sleeves and torso independently, and perhaps even making each sleeve a lightly different length. Let's imagine that tailoring the perfect shirt for a human body usually involved adjusting ten 'shirt parameters' to ensure a perfect fit for an individual. Despite this, when you go to a store to buy a shirt, there is usually one parameter, size, and it often comes in a small number of variants (e.g. small, medium, large). Since the shirts vary only in a small number of way, they are very unlikely to fit any individual perfectly. Why do stores intentionally sell clothing that doesn't fit people as well as they could? Because the store is interested in selling clothes that fit everyone *pretty well*, rather than selling clothes that fit everyone perfectly. To sell clothes that fit everyone perfectly, the store would need to carry dozens or hundreds of models for each shirt. Each model would only be useful to a small number of people and would be useless for a large number of people. In contrast, by having a small number of models that fit the most people 'pretty well', the store can carry a small number of models and apply them successfully to customers in general.  

This general problem is sometimes referred to as the **bias-variance tradeoff**. These are specialized uses of the words *bias* and *variance*. In this case *bias* refers to the model's ability to represent the 'true' relationship the underlies the data, and variance refers to the degree to which the model can offer consistent predictions to new data sets. Suppose we own a shirt store. When we first design we may use a model to design the fit of the shirt, and *fit* to shirt to this person's body. We can tailor it exactly to their body, which would result in a low bias. However, this would result in a high variance since it would not be as likely to fit others well. On the other hand if we specifically do not fit the shirt exactly to the model, we chose a fit with a higher bias. Although the fit may not be as perfect for the model, such a shirt design may fit other people better in general, and so have a lower variance. If we were to choose the low bias shirt for sale to the general public, this would be an example of *overfitting*. The problem with the overfit shirt is that while it is perfect for our model, it is not suitable for its purpose, which is to fit everyone at least *pretty well*. 

In terms of statistical models, a model is **overfit** when it correspond to closely to the data that was used to fit it (the **in-sample data**), to the detriment of its ability to explain new datasets (the **out-of-sample data**). A model that is able to predict the in-sample data with high accuracy is generally desirable, but not if it cannot predict the characteristics of out of sample data. Recall that our models are usually employed to carry our inductive reasoning: Going from a limited number of observations to the general case. In light of this, overfitting a model is exactly contrary to the goals of inductive inference. A model that does a poor job of generalizing to new observations is not useful to carry out induction. Furthermore, a model that cannot explain out-of-sample data will not hold up to replication, which should be a concern for any researcher. 



```{r}
set.seed(1)
x = -4:4
y = x*0.1 + rnorm (9,0,2)
tmp_dat = data.frame(y,x)
n = length(x)

par (mfrow = c(1,3), mar =c(4,4,1,1))

plot (tmp_dat$x,tmp_dat$y,xlim = c(-5,5), ylim = c(-4,4),pch=16,col=4,cex=2)
for (i in 1:n){
  X = lm(y~x,tmp_dat[-i,])$coefficients
  x = seq(-5,5,0.01)
  y_hat = X[1] + X[2]*x
  lines (x,y_hat)
}

plot (tmp_dat$x,tmp_dat$y,xlim = c(-5,5), ylim = c(-4,4),pch=16,col=4,cex=2)
for (i in 1:n){
  X = lm(y~x+I(x^2),tmp_dat[-i,])$coefficients
  x = seq(-5,5,0.01)
  y_hat = X[1] + X[2]*x + X[3]*x^2
  lines (x,y_hat)
}

plot (tmp_dat$x,tmp_dat$y,xlim = c(-5,5), ylim = c(-4,4),pch=16,col=4,cex=2)
for (i in 1:n){
  X = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7),tmp_dat[-i,])$coefficients
  x = seq(-5,5,0.01)
  y_hat = X[1]+X[1]*x+X[2]*x^2+X[3]*x^3+X[4]*x^4+X[5]*x^5+X[6]*x^6+X[7]*x^7+X[8]*x^8
  lines (x,y_hat)
}

```



In this section we are going to discuss three ways that we can test our models to make sure they generalize well, and then we are going to put these methods into practice to investigate whether the increased complexity we have discussed to this point is justified for our model. We will focus on a conceptual understanding of model comparison, and on developing practical skills. For a more thorough (and excellent) presentation of these topics please see stat reth. For an even more thorough treatment of these topics please see paper1 and paper2. 

### In-sample prediction

The simplest, and worst, way to consider out of sample prediction is by simply looking at the in sample prediction. Generally speaking, adding more and more predictors and making more and more complicated models will improve the fit of your model to the data you have, i.e. your in sample prediction. However, if the predictors are not related to true characteristics of the underlying model, they will also tend to decrease the fit of the model to new data. The 'common sense' analogy of the tailored clothing actually works perfectly for statistical models: A model that is perfectly suited to one particular set of data is, by its very nature, unlikely to generalize well to other models. 

We can demonstrate this for some very simple models using simulations. In the code below, we generate three random variables consisting only of the values -1 and 1. We then use only the second one to 'simulate' some random data, and we add random Gaussian error to this fake variable. After this we generate two sets of data, an in sample and an out of sample set. We then fit three models to both data sets. These models include increasingly more predictor variables, however, we know that only the first one is useful to understand the underlying process. The models are fit with recognizable model formulas, though they use a function that we have not discussed to this point. The `lm` (linear model) function uses maximum-likelihood estimation (see section X) to fit regression models with relatively simple structures. We are going to use it here because it estimates regression parameters very quickly and therefore is reasonable to use when we want to fit 3000 models in no more than five seconds. We use the predicted values and $\sigma$ estimates provided by our maximum-likelihood models to estimate the likelihood of the entire model by adding together the log density over every individual observation (see X and X). 

```{r, cache = TRUE}
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
in_sample_ll = matrix (0, iter, 3)
out_of_sample_ll = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)
  x3 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y_in = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out of sample" data
  y_out = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y_in ~ 1+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y_in ~ 1+x1 + x2)
    if (j==3) mod = lm (y_in ~ 1+x1 + x2 + x3)
    
    # find the predicted value (mu) for each data point
    yhat = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    in_sample_ll[i,j] = sum (dnorm (y_in, yhat, sigma, log = TRUE))
    out_of_sample_ll[i,j] = sum (dnorm (y_out, yhat, sigma, log = TRUE))
  }
}
```

The above simulations result in six sets of likelihoods: In sample and out of sample likelihoods for the true model and for two other overly-complicated models. The average of each of these six likelihoods is presented in figure X. Since logarithmic values closer to zero are closer to one, *more negative* log-likelihood values indicate a *less probable* outcome. We can see that the in sample likelihood increases as the model becomes more complicated, despite the fact that the $x_1$ and $x_2$ had absolutely no relationship to the underlying process or to the dependent variable. Why does this happen? One way to think about it is that the tallest person in California *and* Arizona will be *at least* as tall as the tallest person in Arizona. By increasing the number of ways it can explain your data, you more complex model will do at least as well as the model with fewer possible explanations. Although the extra parameters increase the in sample prediction, they actually make the out of sample prediction worse. Why? Because you (and your models) can always make up a story to explain some limited number of observations. You watch your friend rub her lucky hat before sinking free throws in a basketball game and infer that rubbing your lucky hat increases the probability of making a basket. If you were to apply this model to new data, you would realize that it does a poor job of explaining variation in free throw percentage between and within players. [@@ SB - the example is not good but its hard to explain without getting into causal stuff, the 'real' process model, etc. maybe this need to be explained already , or even in this chapter??]. 

```{r F66, fig.height = 4, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.6
################################################################################

means = round (apply (cbind(in_sample_ll,out_of_sample_ll), 2, mean),1)

par (mfrow = c(1,2), mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))
plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "Model log-likelihhod",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
arrows (1,means[1]-0.2, 1,means[4]+0.2,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[5]+0.2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[6]+0.2,code=3, length=0.1,lwd=2)
text ((1:3)+.1, -71, labels = round(c(means[1]-means[4],means[2]-means[5],
                                means[3]-means[6]),2))
text (0.8,c(-69.25,-73.2), labels = c("More Likely","Less likely"), pos=4)

mtext (side=2,text = "Log-likelihood", line = 2.5)
#legend (3.4,-70.5, legend= c("In sample","Out of sample"), 
#        col=c(1,2), lwd=2,bty="n")

plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))


```

In many situations, we care more, or perhaps exclusively, about out of sample prediction. When someone carries out a perceptual experiment they want to make statements about listeners in general and not only those that carried out the experiment. This means that when we evaluate our models, we may need to consider the out of sample prediction more than the in sample prediction. 

### Out of sample prediction: Penalization

In the example in figure \@ref(fig:F66) we knew the true underlying model and generated out of sample data that exactly conformed to the characteristics of our sample. In real life, researchers do not usually have access to out of sample data, they do not know the characteristics of the 'true' model that underlies their data, nor can they confirm that any out of sample data shares the exact underlying process as their in sample data. As a result, we can never *really* know what the difference is between in sample and out of sample prediction for our models. However, you may have noticed that: 1) The slopes of the lines in figure \@ref(fig:F66) seemed to have predictable slopes, 2) The lines seem to be a mirror image of each other. Mathematicians have noticed this too, and have used this to *penalize* in sample likelihoods to estimate out of sample likelihoods. 

The logic of penalization can be understood with reference to figure \@ref(fig:F66). We would like to select the model with the highest value on the red line, given only the values on the black line. We can see that the difference between the black and red lines for each model increases by one for every parameter we add. If we assume that this is a general property of all models, we can subtract three from the likelihood of our most-complicated model to estimate the out of sample prediction, and subtract one from the likelihood of the least complicated model to estimate its out of sample prediction. Subtracting/adding a given value to a model likelihood based on model complexity is called *penalization*. If we did this we would arrived at penalized likelihoods of -71.2 (-70.2-1) and -72.2 (-69.2-3) for the least and most complex models. When we select the model with the highest *penalized* rather than in sample likelihood, we will not necessarily select the more complicated model. 

- penalization will not just pick the least complicated model. 
- the queston is: the the increase lilihood more than the increase we might expect by pure coincidence?
- historically aic and dic. lots of great sources about these but we wont talk about them much.
- historically penalized by numbr of parameter,s effective parameters for our models because params are semi independent due to partial pooling
- now waic, we will show how to use this and compare
- if errors come up this motivates loo!.



```{r F67, fig.height = 4, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.7
################################################################################
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
in_sample_ll = matrix (0, iter, 3)
out_of_sample_ll = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y_in = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out of sample" data
  y_out = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y_in ~ 0+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y_in ~ 1+x1)
    if (j==3) mod = lm (y_in ~ 1+x1 + x2)
    
    # find the predicted value (mu) for each data point
    yhat = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    in_sample_ll[i,j] = sum (dnorm (y_in, yhat, sigma, log = TRUE))
    out_of_sample_ll[i,j] = sum (dnorm (y_out, yhat, sigma, log = TRUE))
  }
}



means = round (apply (cbind(in_sample_ll,out_of_sample_ll), 2, mean),1)

par (mfrow = c(1,2), mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))
plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "Model log-likelihhod",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
mtext (side=2,text = "Log-likelihood", line = 2.5)
#legend (3.4,-70.5, legend= c("In sample","Out of sample"), 
#        col=c(1,2), lwd=2,bty="n")

  plot (means[1:3], type = "b", lwd = 2, ylim =c(-73.5,-69),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))
```


```{r, cache = TRUE}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="waic")
model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="waic")
model_re_t = brms::add_criterion (model_re_t, criterion="waic")
```

```{r}
model_sum_coding$criteria$waic
```

```{r}
head (model_sum_coding$criteria$waic$pointwise)
```


```{r}
model_waic_info = model_sum_coding$criteria$waic$pointwise
model_t_waic_info = model_sum_coding_t$criteria$waic$pointwise
```

```{r}
colSums (model_waic_info)

apply (model_waic_info,2,sd) * sqrt(1386)
```


```{r}
brms::loo_compare (model_sum_coding,model_sum_coding_t, criterion ="waic")
```

```{r}
sum(model_waic_info[,1]-model_t_waic_info[,1])
sum(model_waic_info[,1]-model_t_waic_info[,1]) * sqrt(1386)
```




```{r, cache = TRUE}
resids = residuals(model_sum_coding)[,1]
resids_t = residuals(model_sum_coding_t)[,1]
```

```{r, cache = TRUE}
par (mfrow = c(1,2), mar = c(4,4,1,1))
plot(model_sum_coding$criteria$waic$pointwise[,2], scale(resids), 
     ylim = c(-5.5,5.5), xlim = c(0,1.1))
grid()
abline (v = 0.4)
#abline (h=c(-2.5,2.5))

plot(model_sum_coding_t$criteria$waic$pointwise[,2], scale(resids_t), 
     ylim = c(-5.5,5.5), xlim = c(0,1.1))
grid()
abline (v = 0.4)
#abline (h=c(-2.5,2.5))

```


### Out-of-sample prediction: Cross validation

The last way to evaluate models that we'll be discussing is called **cross validation**. Cross validation consists of dividing your data into two groups. You use one group to fit a model (i.e. in sample data), and then use this model to predict the other group of data. To cross validate you need to pick a group to use for in group data. When one carries out a cross validation,  model predictive accuracy can be measured in any number of application-specific ways. For example, we might be interested in which model can accurately predict height judgments. We can measure predictive error using mean square error or absolute mean or median deviation among others. For each model, the measure (or measures of interest) are collected. This provides a set of observations that can be considered a sample. Researchers can then use this sample to infer the average value and an interval around that value. 

You can leave out just one group (holdout) or split the data into groups and use each group as the out of sample data in turn. This is called k fold. A higher k is better because its less prone to bias. The logical endpoint of this is leave one out cross validation, where you leave a single data point out and predicting that. Leave one out cross validation is whats called exhaustive, it divides the data in all possible ways. If you do ten fold you are not using all 10 sized groups. This means that there is more bias since you randomly chose those groups. 

Leave one out cross validation would be very time consuming and impossible for our models. Luckily, there is a package and function do do this in an efficient way. This loo package/function more or less combines elements of cross validation and penalization. It uses an estimation of leave one out cross validation to penalize predictive likelihoods.  

- and it penalizes the model likelihood based on this
- or actually penalizes unnormalized posteriors??

### Comparing our models

We will compare three models, the model we fit in the last chapter without listener dependent Age effects (random age affects by listener), the model with these age effects, and the model with age effects and also heteroskedastic errors. We will do so using both loo and waic. 

- add them, compare them.
- get one estimate for each set of samples so distribution per model. 
- not so much about "how much is big" as how different are they. 



```{r}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="loo")
model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="loo")
model_re = brms::add_criterion (model_re, criterion="loo")
model_t_het = brms::add_criterion (model_t_het, criterion="loo")
```


```{r}
loo_compare (model_sum_coding, model_sum_coding_t, model_re, model_t_het,criterion = "loo")
```

```{r}
loo_compare (model_sum_coding, model_sum_coding_t, model_re, model_t_het,criterion = "waic")
```

```{r}
bmmb::short_summary (model_sum_coding)

bmmb::short_summary (model_re)

bmmb::short_summary (model_re_het)

bmmb::short_summary (model_re_het_t)

```


classic adjustments work in a classic way, undersome basic assumptions thy tend to work



```{r}
model_re = add_criterion(model_re, "loo")
model_t = add_criterion(model_t, "loo")
model_t_het = add_criterion(model_t_het, "loo")

qq = loo_compare(model_re, model_t, model_t_het, criterion="loo")
```



- explain elppd using example from model 1
- it uses this and penalizes in a way based on parameters
- out of sample prediction is always worse than in sample
- talk about EXCELLENT explanation in statistical rethinking



- using in sample as out of sample - bad
- using penalized in sample - better theoretical
- cross validation - better empirical

- in sample vs out of sample
- cross validation
- dont know what you dont know



## Variation in parameters across sources of data

The models we have considered so far include only listener and speaker-specific intercepts, but nothing else. For example, our models in chapter 5 included a predictor representing the apparent age of the speaker. However, this was only a `population-level parameter` (i.e. a 'fixed' effect) meaning that it had the same value for all listeners. An inspection of figure X last chapter clearly suggests that apparent age did not, in fact, have the same effect for the judgments of all listeners. 

Another way to think about this is that our model modeled only the *marginal* effect for apparent age. This is the effect of apparent age on average across all listeners, *independent* of listener. This sort of effect is often referred to as a main effect. Someone might ask "what is the average apparent height difference between children and adult females?" and you might say "about 16 cm". Which listener exactly does this statement apply to? To all of them, this is the average *overall effect*. In contrast, we may want to think about the effect of apparent age *conditional* on listener. Recall from chapter 2 that if the conditional distribution of a variable changes as a function of another variable then these are not independent. In the same way, if the conditional effects of a predictor change as a function of another predictor, then these effects are not independent. For example, if the effect of apparent age varies substantially across listeners then if someone asks "what is the average apparent height difference between children and adult females?" you may have to answer "well it depends on the listener".      

```{r}
listener_age_differences = tapply (notmen$height, notmen[c("A","L")], mean)
listener_age_effects = (listener_age_differences[1,]-listener_age_differences[2,])/2
```


```{r F62, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.2
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_age_differences[1,], 
     type = 'b', col = lavender, pch=16, ylim = c(132,172),ylab = "Height (cm)",lwd=2)
lines(listener_age_differences[2,], 
      type = 'b', col = deepgreen, pch=16,lwd=2)
abline (h = c(157, 165, 149), lty = c(1,3,3))

plot(listener_age_effects, lwd=2,lty=1,
     type = 'b', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
abline (h = mean (listener_age_effects))

plot(listener_age_effects, lwd=1,lty=3,
     type = 'n', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
arrows (1:15, rep(mean(listener_age_effects),15), 1:15, listener_age_effects,
        length=.1, lwd=2, col=skyblue)
abline (h = mean(listener_age_effects))

```

When two variables do not have independent effects on your outcome, they are said to **interact** and have an **interaction**. The parameters in your model that help you capture these conditional effects are referred to as interactions or **interaction effects**. So, the 'fixed' effect for `A1` really represents the marginal (overall) effect for the predictor. To investigate the values of this predictor *conditional* on listener, we need to include the listener by `A1` *interaction* in our model.

### Description of our model

Before including interactions in our model, let's take  step back and consider the model formula we used last chapter. Below, we explicitly include the leading one (indicating the intercept) to make the following discussion simpler. 

`height ~ 1 + A + (1|L) + (1|S)`

This formula told our model to draw the A parameter from a fixed prior distribution and to draw the `L` and `S` terms from distributions whose standard deviations were estimated from the data (i.e. using adaptive partial pooling). We can pretend that we want to estimate our `L` terms as 'fixed' effects (i.e. without partial pooling) by pulling it out of the parentheses, as seen below.    

`height ~ 1 + A + L + (1|S)`

We are doing this to highlight the two ways that we can include interactions in our model. Interactions between combinations of fixed effects can be denoted using `:`. For example the formula below says "include the main effect for A and L, and the interaction between A and L". The `:` symbol can be read as "given", which helps to highlight that these help represent conditional effects. So `A:L` can be read out loud as "A given L" or "the effect of A given the value of L". 

`y ~ 1 + A + L + A:L`

As a shortcut, we can use the symbol `*` which means "include the main effects for these predictors and all interactions between them."

`y ~ 1 + A * L`

So, if we wanted to include the marginal effects of age and listener, and the interaction between them we could include them in either of the two ways below, provided we want to estimated these without partial pooling. 

`height ~ 1 + A + L + A:L + (1|S)`

`height ~ 1 + A * L + (1|S)`

However, we know that is is advisable to estimate factors with large numbers of levels using partial pooling, regardless of how 'random' the effect may be (see section X). This applies to the marginal effects of predictors such as listener, but also to the interactions between these 'random' effects and our 'fixed' effects. This leads to the second way of including interactions in our model. Whenever any fixed effects interacts with our random effects, it is most common to include these inside the parentheses belonging to the random effect they interact with, like so:

`height ~ 1 + A + (1 + A|L) + (1|S)`

When we do this, we tell our model to fit the age by listener interaction using partial pooling just like we did for the listener intercept effects. Actually, our 'listener intercept effects' are actually an intercept by listener interaction. In other words, out listener effects are simply listener-specific deviations from the intercept conditional on listener, i.e. the listener by intercept interaction. When our formula has a non intercept predictor, we can omit rthe `1` and make our formula as follows: 

`height ~ A + (A|L) + (1|S)`

The formula above tells `brms`: "height varies as a function of an intercept and an age effect, a listener-specific intercept and age effect, and a speaker specific intercept. It also tells `brms` to estimate the speaker intercepts and age effects, and the speaker intercepts as random effects, i.e. using adaptive partial pooling. Our model formula might specify a model that looks like below:

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\
\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12)
\end{split}
(\#eq:61)
\end{equation}
$$

Note that all we have done is added a new term to our prediction equation ($A1 \colon L_{[A_{[i]}]}$), and we have set up the prior for these terms ($A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L})$), and the prior for the term-specific standard deviation parameter ($\sigma_{A1 \colon L} \sim N(0,12)$). This model is perfectly fine, sort of. One potential problem is that we are now estimating two parameters for each listener, an intercept and an effect for `A`, and our model above draws each of these variables from independent distributions as if these were totally unrelated. The problem with treating multiple listener-specific effects as independent is that they are often not. As we have seen repeatedly to this point, treating dependent variables as if they were independent can cause problems for inference. For this reason, when there is more than one group=specific predictor in our model, these are usually treated as draws from a *multivariate normal* distributions, as will be discussed in the following section. 

### Random effects and the multivariate normal distribution

The **multivariate normal** distribution is a straightforward and very useful generalization of the normal distribution. A multivariate normal variable can be thought of as a set of variables whose marginal distributions are themselves normal. Each of these variables represents a **dimension** of the multivariate normal. The dimensions of the multivariate normal can have arbitrarily different means and standard deviations, but their marginal distributions will *always* be a (univariate) normal. Below we 'make' five samples of a three-dimensional normal distribution by independently sampling three univariate normals and sticking these together as columns. Below, each row represents a different sample of the variable and each column represents a different dimension of the variable for that observation. 

```{r}
multivariate_normal = cbind(rnorm (5,0,1),rnorm (5,5,37),rnorm (5,-100,0.001))
multivariate_normal
```

If this were all there is to multivariate normal distributions, then they would not be worth the trouble in many situations. However, treating a set of variables as a multivariate normal distribution allows for the **linear correlation** (often referred to as just the **correlation**) between dimensions to be modeled. The linear correlation between two variables is a value between -1 and 1 which measures the extent two which the two variables tend to vary along straight lines. For example, below we calculate mean height judgments. 

```{r}
listener_means = tapply (notmen$height, notmen[,"L"], mean)
```

In the figure below we plot these, along with the listener age effects on apparent height, calculated (and presented) above. In the rightmost panel we see that these two variables seem to have a negative relationship such that smaller values of average heights tend to be associated with larger age effects. This suggests that these variables are negatively correlated. This is important because, if this is true, it suggests that we are unlikely to observe listener who reported tall speakers on average *and* had large age effects on height. 

```{r F63, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.3
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_means[c(15,1,2,14,3,13,4,12,5,6,11,10,8,7,9)],ylim=c(142,167),cex=1.5, 
     type = 'b', col = lavender, pch=16, ylab = "Height Intercept (cm)",lwd=2,
     xlab = "Listener")
abline (h = mean(listener_means), lty=3)

plot(listener_age_effects[c(15,1,2,14,3,13,4,12,5,11,6,10,8,7,9)], lwd=2,lty=1,cex=1.5,
     type = 'b', col = darkorange, pch=16, ylab = "Age Effect (cm)",ylim=c(1,18),
     xlab = "Listener")
abline (h = mean(listener_age_effects), lty=3)

plot(listener_means,listener_age_effects, lwd=1,lty=3,xlim=c(142,167),ylim=c(1,18),
     type = 'p', col = cols[4], pch=16, ylab = "Age Effect (cm)",cex=1.5,
     xlab = "Height Intercept (cm)")
abline (v = mean(listener_means), h = mean(listener_age_effects), lty=3)

```

We can calculate **Pearson's correlation coefficient** to measure the degree of correlation using the `cor` function in R, which tells us that the correlation between these variables is -0.85. The negative correlation means that a scatter plot of the two variables will have a negative slope left to right. A value of -1 would indicate a *perfect* negative correlation, a value of 0 indicates no correlation, and a value of one indicates a perfect positive correlation. So, the correlation of -0.85 represents a strong negative correlation between these two variables. 

```{r}
cor (listener_means, listener_age_effects)
```

Consider the two vectors below. The element of the second vector is exactly equal to twice every element of the first vector: They are perfectly predictable one from the other. As a result, these two vectors have a correlation of 1. 

```{r}
x1 = c(-1, -1, 1, 1)
y1 = c(-2, -2, 2, 2)
cor (x1, y1)
```

Below we see the opposite situation. The second vector is still twice the first vector, but now every sign differs across the two vectors. These are still perfectly predictable, just backwards. For example, if a gambler were wrong 100% of the time, anyone who did the opposite could perfectly predict the outcome of every game by doing the opposite. Below, we see that these vectors have a correlation of -1.

```{r}
x1 = c(-1, -1,  1,  1)
y1 = c( 2,  2, -2, -2)
cor (x1, y1)
```

In the next example we see that the signs of each element of the second vector are totally *unpredictable* from the corresponding element in the first vector. In half the cases the signs match and in half the cases they do not. This results in a correlation of 0 between the vectors. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2,  2, -2, 2)
cor (x1, y1)
```

Finally, we see a situation where the vectors *almost* match. In the example below three of the four elements match in sign, resulting in a positive correlation between 0 and 1. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2, -2, -2, 2)
cor (x1, y1)
```


The variance of a variables is the expected value (i.e. mean) of squared deviations from the mean. The **covariance** of two variables is the expected value of the product of deviations from their respective means.

$$
\begin{equation}
r = \frac {\sigma_{xy}^2} 
          {\sigma_x \sigma_y}
(\#eq:62)
\end{equation}
$$

The above can be estimated from a sample using the formula below. 

$$
\begin{equation}
r = \frac {\Sigma(x_i-\bar{x})(y_i-\bar{y})} 
          {(N-1) \times \hat{\sigma}_x \hat{\sigma}_y}
(\#eq:63)
\end{equation}
$$

When we estimate random effects for a group as multivariate normal distributions we also estimate the correlation between all pairs of dimensions. In many cases these correlation can influence the likelihood of different outcomes in our model. The easiest way to see the reason for this is by drawing a bivariate (2-dimensional) normal variables with different correlations and plotting the. Below we do this for distributions corresponding to the mean and standard deviations of listener means and age effects. The difference in each row is in the difference in correlation between the two dimensions/variables. 

In the left column below we compare three bivariate normal variables along the two dimensions. In the absence of any correlation between variables, a plot of this distribution will be *spherical* (or circular in 2 dimensions). When there is a correlation between the two dimensions, the distribution starts looking more and more like a straight line. When there is a negative correlation, the line just points down rather than up. 

Note the the marginal (independent) distributions of the variables (the left and right histograms) don't change as the correlation changes. The correlation is a reflection of the *joint* variation in the two variables and will not necessarily be evident in the marginal distributions of each variable.  

```{r F64, fig.height = 6.5, fig.width = 7, fig.cap="10,000 bivatiate normal draws of simulated intercept and slope coefficients from distributions with a mean of 0 and a standard deviation of 1. The correlation of the variables is 0 (top), 0.5 (middle) and 0.9 (bottom). The left column presents both variables together, the middle column presents intercepts and the right column presents slopes. ", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.4
################################################################################

par (mfrow = c(3,3), mar = c(4,4,3,1))

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,0,0,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3)
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,5,5,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,-14.8,-14.8,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()
```

When our dimensions are uncorrelated they are independent. The value of one does not help you understand the other. However, when the dimensions *are* correlated we can use this to make better predictions using our data. For example, an intercept of 2 in the bottom row in the figure above is very likely to be paired with a slope of 2, but *extremely* unlikely to be seen with a slope of -2. In contrast, in the top row a slope of 2 and -2 seem about equally likely given an intercept of 2. So, when we use multiple random predictors per grouping factor, we are really drawing from a multivariate normal distributions that acknowledges the relationships between random predictors in our data, within-cluster (e.g. subject/participant/speaker).  

For example, consider the experiment regarding coffee and speaking rate. Perhaps people who speak fast normally get an even larger boost to their speaking rate from coffee. On the other hand, maybe since they already speak fast, the effect for coffee is diminished in these speakers. In other case, the relationship between the intercept for these speakers (baseline rate) and their coffee effect would not be independent. 

### Specifying priors for a multivariate normal distribution

The shape of the multivariate normal distribution (i.e. how much it looks like a circle vs an ellipse) is determined by a covariance matrix called sigma ($\Sigma$). This matrix is a square $n$ x $n$ matrix for a variable with $n$ dimensions. When we dealt with unidimensional normal distributions for our previous random effects, we specified priors for the (unidimensional) standard deviations using t distributions. The specification of priors for our covariance matrix is only slightly more complicated. 

In our models, we won't actually include priors for $\Sigma$ directly. This is because `brms` (and STAN) build up $\Sigma$ for us from the components we *do* specify. This is more information that you *really* need, but it helps to understand why the priors are specified the way they are for our random effects.

Consider two random effects, a random by subject intercept $\alpha_{[subj]}$, and a random by-subject slope called \alpha_{[subj]}. The covariance matrix for our random effects is created by multiplying the standard deviations of our individual dimensions by a correlation matrix ($R$) specifying the correlations between each dimension. The operation is like this:

$$
\begin{equation}
\begin{split}
\Sigma = \begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} 
\times R \times
\begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} \\
\end{split}
(\#eq:64)
\end{equation}
$$

The values in the outside matrices are the the standard deviations of the random intercepts ($\sigma_{\alpha_{[subj]}}$) and slopes ($\sigma_{\beta_{[subj]}}$) individually. The correlation matrix $R$ contains information about the correlation between the dimensions of the variable (e.g., $\rho_{\alpha_{[subj]} \beta _{[subj]}}$).
  
So, when we have multiple random effects we have a multidimensional variable, and we need to specify priors for each dimension and for the correlation between all dimensions (but not for $\Sigma$ directly). 

We provide priors for the standard deviations of the individual dimensions in the same way as we do for 'unidimensional' random effects (like $\alpha_{[speaker]}$). 

The correlation matrix $R$ will look something like below (for two dimensions). It will contain only values of 1 on the main diagonal and have mirrored values between -1 and 1 off of the diagonal (since the correlation of a and b equals the correlation of b and a).

$$
\begin{equation}
\begin{split}
R = \begin{bmatrix} x & y \\ y & z \\ \end{bmatrix} \\ \\
\end{split}
(\#eq:65)
\end{equation}
$$

We specify priors for variables of this type using the $LKJCorr$ distribution in `brms`. This distribution has a single parameter that determines how peaked the distribution is around 0. Basically, higher numbers make it harder to find larger correlations (and therefore yield more conservative estimates). [See here for an example](https://eager-roentgen-523c83.netlify.app/2014/12/27/d-lkj-priors/).   

$$
\begin{equation}
\begin{split}
R \sim \mathrm{LKJCorr} (2)
\end{split}
(\#eq:66)
\end{equation}
$$

The above was a full explanation of what information the model needs and why it needs it. You don't need to *understand* any of the above to use random effects correctly. The important take away is that whenever you are estimating any random effects above and beyond a random intercept, you need to: 

  1) Specify priors for the standard deviation of each dimension.
  
  2) Specify a prior for the correlation matrix for the multivariate normal used for the random parameters.
  
and `brm` (and STAN) will do the rest.


### Fitting the model

We can fit a model with age coefficients that vary across listeners. Notice that the only change in the formula is the inclusion of the `A` predictor on the left-hand-side of the pipe in `(A|L)`. We now include a prior for a new class of parameter `cor` which applies to the correlation matrices for our multivariate normal variables. In addition, we specify the priors *outside* the `brm` function call, and pass this to the function. We will do this when the number of priors is large enough that including them directly in the function call makes the whole thing look too *busy* (in our opinion). 

```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t =  
  brms::brm (height ~ A + (A|L) + (1|S), data = notmen, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2, prior = priors, family = "student")
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t = bmmb::get_model ('6_model_re_t.RDS')
```
```{r, include = FALSE}
model_re_t = readRDS ('../models/6_model_re_t.RDS')
```


### Interpreting the model

We can check out the summary of our model:

```{r}
bmmb::short_summary (model_re)
```

and see that the only new information is in the `L` group-level effects:

```{r}
## ~L (Number of levels: 15)
##                   Estimate Est.Error l-95% CI u-95% CI
## sd(Intercept)         4.09      0.82     2.83     6.08
## sd(A1)                4.17      0.84     2.88     6.11
## cor(Intercept,A1)    -0.81      0.12    -0.96    -0.50
```

which now includes an estimate of the standard deviation of the by-listener effect for age (i.e. the standard deviation of the listener by age interaction, $\sigma_{A1 \colon L}$ in \@ref(eq:61)), and the correlation between our listener-dependent intercept and age-effect terms. We can see that the estimate of this correlation (-0.81) is very similar to the correlation estimated for these terms in our example above (-0.86).

You can get information about the variance, correlation, and covariance parameters using the `VarCorr` function in the `brms` package. As with the `fixef` and `ranef` function, this function also has the option of setting `summary=FALSE` to get the individual samples for these parameters, while by default it returns summaries of these samples. We are not going to talk much about the structure of this output, but we encourage you to investigate it using the `str` function. 

```{r, echo = FALSE}
varcorr_information = brms::VarCorr (model_re)
str (varcorr_information)
```

```{r}
varcorr_information$L$cor[,,1]
```

We can load the sum-coded model we fit in the last chapter, which contained an identical structure save for the inclusion of listener-dependent age affects (and associated parameters).

```{r}
model_sum_coding = readRDS ('../models/5_model_sum_coding.RDS')
```

Despite the similarities between the models, there are some changes to the credible intervals around the parameters that are shared across both models (presented in figure \@ref(fig:F65)). It's always risky to make up explanations for things after the fact (post hoc), but we can try to think about why these changes may have occurred. Overall, our interpretation of the changed hinges on the fact that we can account for more variation in our data, and now explicitly allow for random variation to exist in the effect of `A` at the different levels of listener. 

```{r F65, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.5
################################################################################
comparison = rbind (brms::fixef(model_sum_coding),
                    brms::VarCorr (model_sum_coding)$L$sd,
                    brms::VarCorr (model_sum_coding)$residual__$sd,
                    brms::fixef(model_re),
                    varcorr_information$L$sd[1,],
                    varcorr_information$residual__$sd)

par (mfrow = c(1,4), mar = c(4,4,3,1))
bmmb::brmplot (comparison[c(1,5),], ylim = c(152,159),main="Intercept", 
               xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(2,6),], ylim = c(6,11),main="A1", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(3,7),], ylim = c(2,8),main="Lsd", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(4,8),], ylim = c(7,9.5),main="sigma", 
              xlim = c(.75,2.25), col = c(4,2))
```

The change in the interval around the intercept is very small, and likely reflects our better overall understanding of the data. In contrast, the interval around `A1` has grown substantially. This is likely because we are know acknowledging the uncertainty between different listeners in the `A1` parameter, which we previously treated as fixed between listeners. Our model predicts a standard deviation of 4.2 cm in this effects between listeners, and our data includes judgments from only 15 unique listeners. This limits how precise our `A1` estimate can be. The standard deviation of our listener intercepts has decreased. This is to be expected as what previously seemed like variation in listener averages may have actually been variation in listener age effects. The separation of this into two separate variables may lead to the magnitude of variation in one being diminished. Finally, the decrease in the error standard deviation is due to the inclusion of the listener random effects for `A1`, and the fact that included listener-dependent values for `A1` helped explain our variable. 


## Varying variances: Heteroskedasticity and distributional (or mixture) models

Most 'typical' regression models assume that the error variance is the same for all observations, all conditions, all listeners, etc. The property of having a homogeneous variance is called **homoscedasticity**, and models that make this assumption can be said to be **homoscedastic**. Historically, homogeneity of variance has been assumed in models because it makes models simpler to understand, and not because it is 'true' for all (or any) data. With Bayesian models it is straightforward to relax and test this assumption, and to build models that exhibit **heteroscedasticity**, differences in the error variance across different conditions. Consider the residuals for the random effects model we just fit to our data, which we can get from the model using the `residuals` function:

```{r, cache = TRUE}
residuals_re = residuals (model_re)
```

If we make a boxplot of the residuals we see that the distribution of errors is not exactly equal for all listeners. For example, the interquartile range for listener 10 is almost as wide as the entire distribution of residuals for listener 11. The model we just fit (`model_re`) allowed us to estimate between-listener differences in the apparent age effect on apparent height. In this section we will fit a model with listener-specific error variances ($\sigma$) in addition to listener-specific parameters related to specific effects. By allowing the random error to vary as a function of listener, our model may be able to provide more-reliable information regarding our data, in addition to letting us ask questions like "is listener 11's error variance actually smaller than listener 10's, or does it just seem that way?" in a more formal manner. 

```{r F65, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
boxplot(residuals_re[,1] ~ model_re$data$L, col = cols)
```

Models that allow for heteroskedastic error are sometimes referred to as **mixture models** or **distributional models**. Mixture models refers to the fact that these models can be thought of resulting from a mixture of several different error distributions. One distribution provides the error in some cases and another in other cases, resulting in heteroskedastic error. Distributional models are those which seeks to model systematic variation in the error, just as more traditional models focus on systematic variation in means. In this section we will fit a distributional model to our data, we will seek to understand systematic variation in both the mean and standard deviation parameters in our model. 

Before continuing, we need to talk about whether our (or your) data can realistically support the estimation of such a model. To this point we haven't discussed this much, but as our models get more and more complicated it is something we need to think about. Our data has between 84 and 94 observations for each listener, and seeks to estimate a listener-specific mean, apparent-age parameter, and error variance. So, we want to estimate three listener-dependent parameters using about 90 observations, which is reasonable. However, if we had only 10 observations per listener perhaps it may not be such a good idea to fit a heteroskedastic model to our data, or to add many more listener-specific parameters. In general, before fitting a model that you *can* fit, it is a good idea to think whether you *should* fit it given the nature of your data and the number of observations you have overall, and in different conditions. 

### Description of our model

Our heteroskedastic model is at the same time more complicated, but also more of the same. The `brms` package makes it exceptionally easy to fit models of this kind by letting you write formulas for the error just as you do for the mean. For example, the model formula for our previous model was:

`height ~ A + (A|L) + (1|S)`

This told `brms` "model apparent height as a function of an intercept, an effect for apparent age, listener-specific adjustments to the intercept and age effect, and speaker-specific intercepts". Implicitly, we know that all of this models variation in the $\nu$ parameter of a normal distribution. We want our model to fit listener-specific error terms, and we want to estimate these with partial pooling since it involves estimating 15 different parameters. We can do this by including a separate formula for our error, called `sigma` by `brms` using the following formula:

`sigma ~ 1 + (A|L)`

This formula tells `brms` "model the standard deviation of the error as varying around an overall intercept and listener-specific deviations (estimated with partial pooling)". We can 'stick' our two model formulas together using the `bf` (Bayes formula) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))
```

Rather than estimate the `sigma` parameter directly, `brms` effectively models `log-sigma`, the logarithm of the error standard deviation (although hit still calls it sigma). The problem with trying to model the standard deviation parameter directly is that it is bounded by zero. As a result, effects need to get smaller and smaller as standard deviations approach zero, otherwise they may push standard deviations into negative territory (which is impossible). One way to deal with this is to log transform standard deviation (or variance) parameters and model those instead. The log transform helps model standard deviations because, just like standard deviations, logarithms are bounded by zero. Logarithms map all numbers from 0 to 1 to values between negative infinity and 0. So, when you model the logarithm of `sigma`, an effect can *always* results in a smaller sigma, a value towards, but never quite at, negative infinity. 

Since we are modeling log-transformed sigmas, we now specify the priors for sigma in a log space. Previously, our prior for sigma was a t-distribution with a standard deviation of 15. This meant that the majority of the mass of the distribution was going to be between 0 and 30 cm. Now, we set the ditribution to 1.5, however, these are logarithmic units. This means we expect the mass of the distribution to be between $exp(-3)=0.05$ and $exp(3)=20.1$. We can use a prior with the same ranges for the standard deviation of our listener-dependent error adjustments ($\sigma_{L_\sigma}$). The full model specification is provided below: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\

\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12) \\ \\ 
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:67)
\end{equation}
$$

This model is quite large, but it only contains five new (or modified) lines compared to model presented in \@ref(eq:61). These lines are presented in \@ref(eq:68). First, our model now uses a trial-specific error term for trial $i$ ($\sigma_{[i]}$), whereas it always used a constant standard deviation previously. Second, we are now modeling this trial-specific $\sigma$ parameter using an intercept specific to our sigma term ($Intercept_{\sigma}$), and a listener-dependent deviation from this ($L_{\sigma[L_{[i]}]}$). Third, we specify the prior for our listener-dependent sigma effects. Finally, in the fourth and fifth lines and specify the prior for our sigma intercept, and for the standard deviation of the listener-specific sigma terms. In a sense, the model expressed in \@ref(eq:68) is effectively the model in \@ref(eq:61), with the additional structure in \@ref(eq:68) added to it. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\ \\
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:68)
\end{equation}
$$

### Fitting the model

For the first time below, we specify both the model formula (using the `bf` function) and the priors (using the `set_prior` function) outside of the call to `brm`. We do this to preserve the legibility of the code in general. Our prior specification includes two new lines to specify the priors for our sigma intercept ($\mathrm{Intercept_{\sigma}}$) and the standard deviations of our sigma-related terms ($\sigma_{L_\sigma}$). These priors are specified with the lines:

```{r}
brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma")
brms::set_prior("student_t(3, 0, 1.5)", class = "sd", dpar = "sigma")
```

Notice that we set these using the classes we have already discussed, `Intercept` and `sd` respectively. However, now we set `dpar` (distributional parameter) to `sigma` to tell `brm` that these priors are specifically for parameters related to the model error term, and not to variation in predicted values (i.e. $\nu$).

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))

priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "sd", dpar = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_het_t = 
  brms::brm (model_formula, data = notmen, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_het_t = bmmb::get_model ('6_model_re_het_t.RDS')
```
```{r, include = FALSE}
model_re_het_t = readRDS ('../models/6_model_re_het_t.RDS')
```


saveRDS (model_re_het_t,'../models/6_model_re_het_t.RDS')


### Interpreting the model

If we look at the model output:

```{r}
bmmb::short_summary (model_t_het)
```

We see a two new lines reflecting our new model parameters. The first new line is in the `Group-level effects` for `L`. This line indicates the standard deviation of the listener-specific sigma terms (`sd(sigma_Intercept)`), which we see is 0.34 (remember this is in a log scale). The second new line is in the `Population-level effects`, and it indicates the overall intercept for our sigma term (also on a log scale). If we exponentiate the value of `sigma_Intercept` ($exp(1.75)$), we get 5.X, the value of the sigma term in bla bla. 

Our listener-specific adjustments to the sigma intercept were fit with partial pooling, i.e. they are 'random' effects. As such, we can get these in the same two ways indicated for other random effects terms in section X. First, we can get these with the `ranef` function, which returns a matrix representing our listener random effects terms.

```{r}
str (ranef(model_t_het)$L)
```

After this, we can get just the sigma intercepts, the first five of which are presented below. 

```{r}
ranef(model_t_het)$L[1:5,,"sigma_Intercept"]
```

Alternatively we can use the `hypothesis` function to get the listener random effects themselves, or the sum of these with the sigma intercept. 

```{r}
sigmas_centered = short_hypothesis(model_t_het, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(model_t_het, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

We compare these in the plot below,  exponenting the recreated sigma values to get the listener-specific standard deviation terms. 

```{r F66, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.6
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
brmplot (sigmas_centered, col = cols)
brmplot (exp(sigmas), col = cols, ylim = c(1,13))
#abline (h = 7.71)
```








## But what does it all mean? 








- here at the end I need a part about how does this invalidate the last model, that reasoning is a problem, bla bla
- model comparison?
- compare previous chapter and this model.
- 95% of published work fatures regular models which we see werent really apprpriate. does that make those fake and results invald. 
 - theres problems with this reasoning
 - also our model is also wrong


So are they different yes or no? Statistics aside, a fair assessment of our data suggests that neither binary conclusion is fully supported: they are distinguishable but overlap substantially. If you want to use this model to highlight the differences between girls and women, I think it is valid. I also think it would be valid to use this data to talk about between and within-speaker variation, highlighting the overlap that exists between speakers. Both are true! To a large extent, the meaning is as much in our heads as it is in the model, and we are free to interpret the results as we see fit, as long as reviewers (and readers in general) will believe us. The model is simply a reflection of the relationships in our data, and the interpretation is up to us. 

Keep in mind that the "as long as reviewers (and readers in general) will believe us" component is crucial. The results of the model will need to be interpreted in the larger context of the work it is presented in, and in terms of scientific and general knowledge that readers have. The results of any model will need to 'make sense' given this, and a statistical result on its own will not be enough to make people (including us) believe outlandish, or even weakly supported claims. 

The model is not reality and should not be confused with reality. This is a very important point! A statistical finding does not *prove* that something is *true*. This kind of thinking has [caused many problems for researchers in the social sciences recently](https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html). In general, we can imagine that 10 people might approach any given research question in 10 different ways, a concept known as [researcher degrees of freedom](https://en.wikipedia.org/wiki/Researcher_degrees_of_freedom). This would cause slight differences in their results, resulting in a sort of 'distribution' for any given result. How can a fixed underlying reality result in a distribution or results? When they are all slightly wrong! 

For example, we know for a fact that f0 varies, weakly but systematically, across vowel categories, a concept known as [intrinsic f0](https://www.sciencedirect.com/science/article/abs/pii/S0095447095801650). A model that included vowel category as a within-speaker predictor would reduce the apparent error in our model (making $\sigma_{error}$ smaller), and might affect the precision of our other estimates. Would this new model invalidate our current model? Answering yes to this question is generally problematic because there is *always* a better model out there, and so every model would automatically be invalid.  

The solution is to think of your model not as a mathematical implementation of *reality* but instead as a mathematical implementation of your research questions. Your model should include the information and structure that you think are necessary to represent and investigate your questions. Using a different model can result in different results given the same data, but asking a different question can also lead to different results given then same data! One of my favorite phrases to use is "given our data and model structure". This phrase is helpful because it highlights the fact that your results are contingent on:

  1) the data you collected. Given other data you may have come to other conclusions.
  
  2) the model you chose. Given another model you may have come to other conclusions.
  





## waic simulation explanation

```{r, cache = TRUE}
set.seed(1)
n = 50
iter = 1000
lppds = matrix (0, iter, 8)
for (i in 1:iter){
  # create 4 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)
  x3 = sample (c(-1,1), n, replace=TRUE)
  x4 = sample (c(-1,1), n, replace=TRUE)
  
  # generate the observed data with an underlying process
  # that only uses the x1 predictor
  y = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some unobserved data
  y_o = 1 + x1 + rnorm (n, 0, 1)
 
  mod = lm (y ~ x1)
  yhat = mod$fitted.values
  sigma = summary(mod)$sigma

  lppds[i,1] = sum (dnorm (y, yhat, sigma, log = TRUE))
  lppds[i,5] = sum (dnorm (y_o, yhat, sigma, log = TRUE))

  mod = lm (y ~ x1+x2)
  yhat = mod$fitted.values
  sigma = sqrt (sum((mod$residuals)^2)/(n-3))
  lppds[i,2] = sum (dnorm (y, yhat, sigma, log = TRUE))
  lppds[i,6] = sum (dnorm (y_o, yhat, sigma, log = TRUE))
  
  mod = lm (y ~ x1+x2+x3)
  yhat = mod$fitted.values
  sigma = summary(mod)$sigma
  lppds[i,3] = sum (dnorm (y, yhat, sigma, log = TRUE))  
  lppds[i,7] = sum (dnorm (y_o, yhat, sigma, log = TRUE))
  
  mod = lm (y ~ x1+x2+x3+x4)
  yhat = mod$fitted.values
  sigma = summary(mod)$sigma
  lppds[i,4] = sum (dnorm (y, yhat, sigma, log = TRUE))  
  lppds[i,8] = sum (dnorm (y_o, yhat, sigma, log = TRUE))
}
```




```{r}
means = round (apply (lppds, 2, mean),1)

plot (means[1:4], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),
      pch=16, xlim = c(.8,4.2), ylab = "lppd")
lines (means[5:8], col =2, lwd=2, type = "b",pch=16)
arrows (1,means[1]-0.2, 1,means[5]+0.2,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[6]+0.2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[7]+0.2,code=3, length=0.1,lwd=2)
arrows (4,means[4]-0.2, 4,means[8]+0.2,code=3, length=0.1,lwd=2)
text ((1:4)+.1, -71, labels = round(c(means[1]-means[5],means[2]-means[6],
                                means[3]-means[7],means[4]-means[8]),2))
```





```{r}
model_formula = brms::bf(height ~ A + (A|L))

priors = c(brms::set_prior("student_t(3, 156, 12)", class = "Intercept"),
          brms::set_prior("student_t(3, 0, 12)", class = "b"),
          brms::set_prior("student_t(3, 0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "sd", dpar = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
tmpp_modd = 
  brms::brm (model_formula, data = notmen, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2,  prior = priors)
```


```{r}
sigmas_centered = short_hypothesis(tmpp_modd, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(tmpp_modd, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

We compare these in the plot below,  exponenting the recreated sigma values to get the listener-specific standard deviation terms. 

```{r, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
par (mfrow = c(1,2), mar = c(4,4,1,1))
brmplot (sigmas_centered, col = cols)
brmplot (exp(sigmas), col = cols, ylim = c(2.5,12.3))
abline (h = 6.55)
```

