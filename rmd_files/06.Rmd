\newpage

# Variation in parameters ('random effects') and model comparison 

 - chapter 4 introduced random effects, random intercepts.
 - here we will consider the addition of other group-level variation in parameters.

Before powerful computers were easily available, statisticians had to come up with clever solutions to answer difficult questions. The reason for this is that things needed to be solved with pen and paper, or eventually calculators, and this ruled out extremely time-consuming solutions such as sampling from the posterior distribution of models as complicated as the ones discussed here. Approaches to answer commonly-encountered experimental designs (and corresponding data structures) often involved making simplifying assumptions that make arriving at a solution simpler. For example, when comparing two groups statisticians might say "we can solve this problem if we assume that the errors are normally distributed and that the error variance is the same across all conditions". These assumptions are rarely true in practice (and never *exactly* true in reality), but are usually close enough to the truth to make our models be useful. There is nothing particularly 'wrong' with making these assumptions. However, when we build our own Bayesian models we don't need to make them, and so it is worth considering if we might benefit from making other sort of assumptions. 

An analogy can be made to building your own furniture vs. shopping at a furniture store. The furniture at the store is perfectly fine and if you find something that suits you it will be fast and convenient. What happens if what you want is *not* at a store? Well if you don't know how to build your on furniture you are out of luck, you better just find the existing piece that is most similar to what you need, and try to make the most of it. Working with 'traditional' models based on maximum-likelihood estimation (discussed in X) is like going to the store. There are often a limited number of choices, and these choices make certain, often rigid, simplifying assumptions. Learning to use `brms` (or STAN directly) gives you the flexibility of trying many different assumptions, allowing you to build a model that might be very difficult to find 'at the store', but which is well-suited to your data. In the last chapter we focused on models comparing two groups. Our approach to this was a bit old-fashioned in that it adhered to the limitations of more 'traditional' models. In this chapter we will revisit the models we discussed last chapter. However, this time we will consider the possibility of random variation in the effect of apparent speaker age across listeners. 

## Data and research questions

We're going to use the same data from the last chapter, we are going to exclude adult males and focus on the apparent height of adult females, girls and boys. We're going to build models that build on those of last chapter to ask the same questions:

  (Q1) How tall do speakers perceived as adult females sound? 
  
  (Q2) How tall do speakers perceived as children sound?
  
  (Q3) What is the difference in apparent height associated with the perception of adultness? 
 
Below we load the data and add a column (`A`) that indicates the apparent age of the speaker, `c` for child and `a` for adult.

```{r}
library (brms)
library (bmmb)
data (height_exp)
tmp = as.numeric(height_exp$L) * 0 
count=1
for (i in c(15,2,1,14,3,13,4,12,5,6,11,10,8,7,9)){
  tmp[as.numeric(height_exp$L)==i] = count 
  count = count + 1
}
height_exp$L = tmp

notmen = height_exp[height_exp$C_v!='m' & height_exp$C!='m' & height_exp$R=='a',]
notmen$A_v = c("c","a")[(notmen$C_v == "w")+1]
```

We recreate figure \ref(fig:51) as figure \ref(fig:61) below to show the distribution of apparent height by listener, and across apparent ages. We can use this figure that some of the design choices we used to build our model in the last chapter may not be totally valid for this data. First, we see that the effect of apparent age, the separation between the two boxes for each speaker, does appear to vary substantially between listeners. Second, our figure shows the presence of substantial numbers of outliers, the points individually plotted (beyond the *whiskers* for each box), suggesting that the error distribution may be more like a t-distribution than a normal one. In this chapter we are going to build models that allow for these details in the data to be estimated and represented by the model. After this, we're going to address the question: Are the extra complications included in our models worth it?

```{r F61, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.1
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1)); layout (mat = t(c(1,2)), widths = c(.7,.3))
boxplot (height ~ A+L, data=notmen, col = rep(cols,each=2),ylim = c(100,200))

boxplot (height ~ A, data=notmen, col = c(beige,lightpink),ylim = c(100,200))
```

## Variation in parameters across sources of data

The models we fit in chapter 5 included a predictor representing the apparent age of the speaker. However, this was only a `population-level parameter` (i.e. a 'fixed' effect) meaning that it had the same value for all listeners. An inspection of figure X last chapter clearly suggests that apparent age did not, in fact, have the same effect for the judgments of all listeners. Another way to think about this is that our model modeled only the *marginal* effect for apparent age. This is the effect of apparent age on average across all listeners, *independent* of listener. This sort of effect is often referred to as a main effect. Someone might ask "what is the average apparent height difference between children and adult females?" and you might say "about 16 cm". Which listener exactly does this statement apply to? To all of them, this is the average *overall effect*. In contrast, we may want to think about the effect of apparent age *conditional* on listener. Recall from chapter 2 that if the conditional distribution of a variable changes as a function of another variable then these are not independent. In the same way, if the conditional effects of a predictor change as a function of another predictor, then these effects are not independent. For example, if the effect of apparent age varies substantially across listeners then if someone asks "what is the average apparent height difference between children and adult females?" you may have to answer "well it depends on the listener".      

```{r}
listener_age_differences = tapply (notmen$height, notmen[c("A","L")], mean)
listener_age_effects = (listener_age_differences[1,]-listener_age_differences[2,])/2
```


```{r F62, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.2
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_age_differences[1,], 
     type = 'b', col = lavender, pch=16, ylim = c(132,172),ylab = "Height (cm)",lwd=2)
lines(listener_age_differences[2,], 
      type = 'b', col = deepgreen, pch=16,lwd=2)
abline (h = c(157, 165, 149), lty = c(1,3,3))

plot(listener_age_effects, lwd=2,lty=1,
     type = 'b', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
abline (h = mean (listener_age_effects))

plot(listener_age_effects, lwd=1,lty=3,
     type = 'n', col = darkorange, pch=16, ylim = c(0,18),ylab = "Height difference (cm)")
arrows (1:15, rep(mean(listener_age_effects),15), 1:15, listener_age_effects,
        length=.1, lwd=2, col=skyblue)
abline (h = mean(listener_age_effects))

```

When two variables do not have independent effects on your outcome, they are said to **interact** and have an **interaction**. The parameters in your model that help you capture these conditional effects are referred to as interactions or **interaction effects**. So, the 'fixed' effect for `A1` really represents the marginal (overall) effect for the predictor. To investigate the values of this predictor *conditional* on listener, we need to include the listener by `A1` *interaction* in our model.

### Description of our model

Before including interactions in our model, let's take  step back and consider the model formula we used last chapter. Below, we explicitly include the leading one (indicating the intercept) to make the following discussion simpler. 

`height ~ 1 + A + (1|L) + (1|S)`

This formula told our model to draw the A parameter from a fixed prior distribution and to draw the `L` and `S` terms from distributions whose standard deviations were estimated from the data (i.e. using adaptive partial pooling). We can pretend that we want to estimate our `L` terms as 'fixed' effects (i.e. without partial pooling) by pulling it out of the parentheses, as seen below.    

`height ~ 1 + A + L + (1|S)`

We are doing this to highlight the two ways that we can include interactions in our model. Interactions between combinations of fixed effects can be denoted using `:`. For example the formula below says "include the main effect for A and L, and the interaction between A and L". The `:` symbol can be read as "given", which helps to highlight that these help represent conditional effects. So `A:L` can be read out loud as "A given L" or "the effect of A given the value of L". 

`y ~ 1 + A + L + A:L`

As a shortcut, we can use the symbol `*` which means "include the main effects for these predictors and all interactions between them."

`y ~ 1 + A * L`

So, if we wanted to include the marginal effects of age and listener, and the interaction between them we could include them in either of the two ways below, provided we want to estimated these without partial pooling. 

`height ~ 1 + A + L + A:L + (1|S)`

`height ~ 1 + A * L + (1|S)`

However, we know that is is advisable to estimate factors with large numbers of levels using partial pooling, regardless of how 'random' the effect may be (see section X). This applies to the marginal effects of predictors such as listener, but also to the interactions between these 'random' effects and our 'fixed' effects. This leads to the second way of including interactions in our model. Whenever any fixed effects interacts with our random effects, it is most common to include these inside the parentheses belonging to the random effect they interact with, like so:

`height ~ 1 + A + (1 + A|L) + (1|S)`

When we do this, we tell our model to fit the age by listener interaction using partial pooling just like we did for the listener intercept effects. Actually, our 'listener intercept effects' are actually an intercept by listener interaction. In other words, out listener effects are simply listener-specific deviations from the intercept conditional on listener, i.e. the listener by intercept interaction. When our formula has a non intercept predictor, we can omit rthe `1` and make our formula as follows: 

`height ~ A + (A|L) + (1|S)`

The formula above tells `brms`: "height varies as a function of an intercept and an age effect, a listener-specific intercept and age effect, and a speaker specific intercept. It also tells `brms` to estimate the speaker intercepts and age effects, and the speaker intercepts as random effects, i.e. using adaptive partial pooling. Our model formula might specify a model that looks like below:

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\
\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12)
\end{split}
(\#eq:61)
\end{equation}
$$

Note that all we have done is added a new term to our prediction equation ($A1 \colon L_{[A_{[i]}]}$), and we have set up the prior for these terms ($A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L})$), and the prior for the term-specific standard deviation parameter ($\sigma_{A1 \colon L} \sim N(0,12)$). This model is perfectly fine, sort of. One potential problem is that we are now estimating two parameters for each listener, an intercept and an effect for `A`, and our model above draws each of these variables from independent distributions as if these were totally unrelated. The problem with treating multiple listener-specific effects as independent is that they are often not. As we have seen repeatedly to this point, treating dependent variables as if they were independent can cause problems for inference. For this reason, when there is more than one group=specific predictor in our model, these are usually treated as draws from a *multivariate normal* distributions, as will be discussed in the following section. 

### Random effects and the multivariate normal distribution

The **multivariate normal** distribution is a straightforward and very useful generalization of the normal distribution. A multivariate normal variable can be thought of as a set of variables whose marginal distributions are themselves normal. Each of these variables represents a **dimension** of the multivariate normal. The dimensions of the multivariate normal can have arbitrarily different means and standard deviations, but their marginal distributions will *always* be a (univariate) normal. Below we 'make' five samples of a three-dimensional normal distribution by independently sampling three univariate normals and sticking these together as columns. Below, each row represents a different sample of the variable and each column represents a different dimension of the variable for that observation. 

```{r}
multivariate_normal = cbind(rnorm (5,0,1),rnorm (5,5,37),rnorm (5,-100,0.001))
multivariate_normal
```

If this were all there is to multivariate normal distributions, then they would not be worth the trouble in many situations. However, treating a set of variables as a multivariate normal distribution allows for the **linear correlation** (often referred to as just the **correlation**) between dimensions to be modeled. The linear correlation between two variables is a value between -1 and 1 which measures the extent two which the two variables tend to vary along straight lines. For example, below we calculate mean height judgments. 

```{r}
listener_means = tapply (notmen$height, notmen[,"L"], mean)
```

In the figure below we plot these, along with the listener age effects on apparent height, calculated (and presented) above. In the rightmost panel we see that these two variables seem to have a negative relationship such that smaller values of average heights tend to be associated with larger age effects. This suggests that these variables are negatively correlated. This is important because, if this is true, it suggests that we are unlikely to observe listener who reported tall speakers on average *and* had large age effects on height. 

```{r F63, fig.height = 3, fig.width=8, fig.cap = "--.", echo = FALSE}

################################################################################
### Figure 6.3
################################################################################
par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(listener_means[c(15,1,2,14,3,13,4,12,5,6,11,10,8,7,9)],ylim=c(142,167),cex=1.5, 
     type = 'b', col = lavender, pch=16, ylab = "Height Intercept (cm)",lwd=2,
     xlab = "Listener")
abline (h = mean(listener_means), lty=3)

plot(listener_age_effects[c(15,1,2,14,3,13,4,12,5,11,6,10,8,7,9)], lwd=2,lty=1,cex=1.5,
     type = 'b', col = darkorange, pch=16, ylab = "Age Effect (cm)",ylim=c(1,18),
     xlab = "Listener")
abline (h = mean(listener_age_effects), lty=3)

plot(listener_means,listener_age_effects, lwd=1,lty=3,xlim=c(142,167),ylim=c(1,18),
     type = 'p', col = cols[4], pch=16, ylab = "Age Effect (cm)",cex=1.5,
     xlab = "Height Intercept (cm)")
abline (v = mean(listener_means), h = mean(listener_age_effects), lty=3)

```

We can calculate **Pearson's correlation coefficient** to measure the degree of correlation using the `cor` function in R, which tells us that the correlation between these variables is -0.85. The negative correlation means that a scatter plot of the two variables will have a negative slope left to right. A value of -1 would indicate a *perfect* negative correlation, a value of 0 indicates no correlation, and a value of one indicates a perfect positive correlation. So, the correlation of -0.85 represents a strong negative correlation between these two variables. 

```{r}
cor (listener_means, listener_age_effects)
```

Consider the two vectors below. The element of the second vector is exactly equal to twice every element of the first vector: They are perfectly predictable one from the other. As a result, these two vectors have a correlation of 1. 

```{r}
x1 = c(-1, -1, 1, 1)
y1 = c(-2, -2, 2, 2)
cor (x1, y1)
```

Below we see the opposite situation. The second vector is still twice the first vector, but now every sign differs across the two vectors. These are still perfectly predictable, just backwards. For example, if a gambler were wrong 100% of the time, anyone who did the opposite could perfectly predict the outcome of every game by doing the opposite. Below, we see that these vectors have a correlation of -1.

```{r}
x1 = c(-1, -1,  1,  1)
y1 = c( 2,  2, -2, -2)
cor (x1, y1)
```

In the next example we see that the signs of each element of the second vector are totally *unpredictable* from the corresponding element in the first vector. In half the cases the signs match and in half the cases they do not. This results in a correlation of 0 between the vectors. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2,  2, -2, 2)
cor (x1, y1)
```

Finally, we see a situation where the vectors *almost* match. In the example below three of the four elements match in sign, resulting in a positive correlation between 0 and 1. 

```{r}
x1 = c(-1, -1,  1, 1)
y1 = c(-2, -2, -2, 2)
cor (x1, y1)
```


The variance of a variables is the expected value (i.e. mean) of squared deviations from the mean. The **covariance** of two variables is the expected value of the product of deviations from their respective means.

$$
\begin{equation}
r = \frac {\sigma_{xy}^2} 
          {\sigma_x \sigma_y}
(\#eq:62)
\end{equation}
$$

The above can be estimated from a sample using the formula below. 

$$
\begin{equation}
r = \frac {\Sigma(x_i-\bar{x})(y_i-\bar{y})} 
          {(N-1) \times \hat{\sigma}_x \hat{\sigma}_y}
(\#eq:63)
\end{equation}
$$

When we estimate random effects for a group as multivariate normal distributions we also estimate the correlation between all pairs of dimensions. In many cases these correlation can influence the likelihood of different outcomes in our model. The easiest way to see the reason for this is by drawing a bivariate (2-dimensional) normal variables with different correlations and plotting the. Below we do this for distributions corresponding to the mean and standard deviations of listener means and age effects. The difference in each row is in the difference in correlation between the two dimensions/variables. 

In the left column below we compare three bivariate normal variables along the two dimensions. In the absence of any correlation between variables, a plot of this distribution will be *spherical* (or circular in 2 dimensions). When there is a correlation between the two dimensions, the distribution starts looking more and more like a straight line. When there is a negative correlation, the line just points down rather than up. 

Note the the marginal (independent) distributions of the variables (the left and right histograms) don't change as the correlation changes. The correlation is a reflection of the *joint* variation in the two variables and will not necessarily be evident in the marginal distributions of each variable.  

```{r F64, fig.height = 6.5, fig.width = 7, fig.cap="10,000 bivatiate normal draws of simulated intercept and slope coefficients from distributions with a mean of 0 and a standard deviation of 1. The correlation of the variables is 0 (top), 0.5 (middle) and 0.9 (bottom). The left column presents both variables together, the middle column presents intercepts and the right column presents slopes. ", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.4
################################################################################

par (mfrow = c(3,3), mar = c(4,4,3,1))

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,0,0,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3)
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,5,5,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()

ranefs = phonTools::rmvtnorm (10000, means = c(159,8), sigma = matrix (c(4.7^2,-14.8,-14.8,3.7^2),2,2))
hist (ranefs[,1],main='',col=4,xlab='Intercept',freq = FALSE,breaks=40, xlim = c(145,175))
hist (ranefs[,2],main='',col=4,xlab='A1',freq = FALSE,breaks=40, xlim = c(-5,20))
plot (ranefs, pch=16,col=4,xlim=c(140,180),ylim=c(-5,22),xlab='Intercept',
      ylab='A1')
phonTools::sdellipse (ranefs,add = TRUE, lwd=2,col=2,stdev =3, xlim = c(145,175))
grid()
```

When our dimensions are uncorrelated they are independent. The value of one does not help you understand the other. However, when the dimensions *are* correlated we can use this to make better predictions using our data. For example, an intercept of 2 in the bottom row in the figure above is very likely to be paired with a slope of 2, but *extremely* unlikely to be seen with a slope of -2. In contrast, in the top row a slope of 2 and -2 seem about equally likely given an intercept of 2. So, when we use multiple random predictors per grouping factor, we are really drawing from a multivariate normal distributions that acknowledges the relationships between random predictors in our data, within-cluster (e.g. subject/participant/speaker).  

For example, consider the experiment regarding coffee and speaking rate. Perhaps people who speak fast normally get an even larger boost to their speaking rate from coffee. On the other hand, maybe since they already speak fast, the effect for coffee is diminished in these speakers. In other case, the relationship between the intercept for these speakers (baseline rate) and their coffee effect would not be independent. 

### Specifying priors for a multivariate normal distribution

The shape of the multivariate normal distribution (i.e. how much it looks like a circle vs an ellipse) is determined by a covariance matrix called sigma ($\Sigma$). This matrix is a square $n$ x $n$ matrix for a variable with $n$ dimensions. When we dealt with unidimensional normal distributions for our previous random effects, we specified priors for the (unidimensional) standard deviations using t distributions. The specification of priors for our covariance matrix is only slightly more complicated. 

In our models, we won't actually include priors for $\Sigma$ directly. This is because `brms` (and STAN) build up $\Sigma$ for us from the components we *do* specify. This is more information that you *really* need, but it helps to understand why the priors are specified the way they are for our random effects.

Consider two random effects, a random by subject intercept $\alpha_{[subj]}$, and a random by-subject slope called \alpha_{[subj]}. The covariance matrix for our random effects is created by multiplying the standard deviations of our individual dimensions by a correlation matrix ($R$) specifying the correlations between each dimension. The operation is like this:

$$
\begin{equation}
\begin{split}
\Sigma = \begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} 
\times R \times
\begin{bmatrix} \sigma_{L} & 0 \\ 0 & \sigma_{A1 \colon L} \\ \end{bmatrix} \\
\end{split}
(\#eq:64)
\end{equation}
$$

The values in the outside matrices are the the standard deviations of the random intercepts ($\sigma_{\alpha_{[subj]}}$) and slopes ($\sigma_{\beta_{[subj]}}$) individually. The correlation matrix $R$ contains information about the correlation between the dimensions of the variable (e.g., $\rho_{\alpha_{[subj]} \beta _{[subj]}}$).
  
So, when we have multiple random effects we have a multidimensional variable, and we need to specify priors for each dimension and for the correlation between all dimensions (but not for $\Sigma$ directly). 

We provide priors for the standard deviations of the individual dimensions in the same way as we do for 'unidimensional' random effects (like $\alpha_{[speaker]}$). 

The correlation matrix $R$ will look something like below (for two dimensions). It will contain only values of 1 on the main diagonal and have mirrored values between -1 and 1 off of the diagonal (since the correlation of a and b equals the correlation of b and a).

$$
\begin{equation}
\begin{split}
R = \begin{bmatrix} x & y \\ y & z \\ \end{bmatrix} \\ \\
\end{split}
(\#eq:65)
\end{equation}
$$

We specify priors for variables of this type using the $LKJCorr$ distribution in `brms`. This distribution has a single parameter that determines how peaked the distribution is around 0. Basically, higher numbers make it harder to find larger correlations (and therefore yield more conservative estimates). [See here for an example](https://eager-roentgen-523c83.netlify.app/2014/12/27/d-lkj-priors/).   

$$
\begin{equation}
\begin{split}
R \sim \mathrm{LKJCorr} (2)
\end{split}
(\#eq:66)
\end{equation}
$$

The above was a full explanation of what information the model needs and why it needs it. You don't need to *understand* any of the above to use random effects correctly. The important take away is that whenever you are estimating any random effects above and beyond a random intercept, you need to: 

  1) Specify priors for the standard deviation of each dimension.
  
  2) Specify a prior for the correlation matrix for the multivariate normal used for the random parameters.
  
and `brm` (and STAN) will do the rest.


### Fitting the model

We can fit a model with age coefficients that vary across listeners. Notice that the only change in the formula is the inclusion of the `A` predictor on the left-hand-side of the pipe in `(A|L)`. We now include a prior for a new class of parameter `cor` which applies to the correlation matrices for our multivariate normal variables. In addition, we specify the priors *outside* the `brm` function call, and pass this to the function. We will do this when the number of priors is large enough that including them directly in the function call makes the whole thing look too *busy* (in our opinion). 

```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t_t =  
  brms::brm (height ~ A + (A|L) + (1|S), data = notmen, chains = 4, cores = 4,
       warmup = 1000, iter = 5000, thin = 4, prior = priors, family = "student")
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t = bmmb::get_model ('6_model_re_t.RDS')
```
```{r, include = FALSE}
model_re_t = readRDS ('../models/6_model_re_t.RDS')
```


### Interpreting the model

We can check out the summary of our model:

```{r}
bmmb::short_summary (model_re_t)
```

and see that the only new information is in the `L` group-level effects:

```{r}
## ~L (Number of levels: 15)
##                   Estimate Est.Error l-95% CI u-95% CI
## sd(Intercept)         4.09      0.82     2.83     6.08
## sd(A1)                4.17      0.84     2.88     6.11
## cor(Intercept,A1)    -0.81      0.12    -0.96    -0.50
```

which now includes an estimate of the standard deviation of the by-listener effect for age (i.e. the standard deviation of the listener by age interaction, $\sigma_{A1 \colon L}$ in \@ref(eq:61)), and the correlation between our listener-dependent intercept and age-effect terms. We can see that the estimate of this correlation (-0.81) is very similar to the correlation estimated for these terms in our example above (-0.86). You can get information about the variance, correlation, and covariance parameters using the `VarCorr` function in the `brms` package. As with the `fixef` and `ranef` function, this function also has the option of setting `summary=FALSE` to get the individual samples for these parameters, while by default it returns summaries of these samples. We are not going to talk much about the structure of this output, but we encourage you to investigate it using the `str` function. 

```{r, echo = FALSE}
varcorr_information = brms::VarCorr (model_re_t)
str (varcorr_information)
```

```{r}
varcorr_information$L$cor[,,1]
```

We can load the sum-coded model we fit in the last chapter, which contained an identical structure save for the inclusion of listener-dependent age affects (and associated parameters).

```{r}
model_sum_coding = readRDS ('../models/5_model_sum_coding.RDS')
```

Despite the similarities between the models, there are some changes to the credible intervals around the parameters that are shared across both models (presented in figure \@ref(fig:F65)). It's always risky to make up explanations for things after the fact (post hoc), but we can try to think about why these changes may have occurred. Overall, our interpretation of the changed hinges on the fact that we can account for more variation in our data, and now explicitly allow for random variation to exist in the effect of `A` at the different levels of listener. 

```{r F65, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.5
################################################################################
comparison = rbind (brms::fixef(model_sum_coding),
                    brms::VarCorr (model_sum_coding)$L$sd,
                    brms::VarCorr (model_sum_coding)$residual__$sd,
                    brms::fixef(model_re_t),
                    varcorr_information$L$sd[1,],
                    varcorr_information$residual__$sd)

par (mfrow = c(1,4), mar = c(4,4,3,1))
bmmb::brmplot (comparison[c(1,5),], ylim = c(152,159),main="Intercept", 
               xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(2,6),], ylim = c(6,11),main="A1", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(3,7),], ylim = c(2,8),main="Lsd", 
              xlim = c(.75,2.25), col = c(4,2))
bmmb::brmplot(comparison[c(4,8),], ylim = c(4,9.5),main="sigma", 
              xlim = c(.75,2.25), col = c(4,2))
```

The change in the interval around the intercept is very small, and likely reflects our better overall understanding of the data. In contrast, the interval around `A1` has grown substantially. This is likely because we are know acknowledging the uncertainty between different listeners in the `A1` parameter, which we previously treated as fixed between listeners. Our model predicts a standard deviation of 4.2 cm in this effects between listeners, and our data includes judgments from only 15 unique listeners. This limits how precise our `A1` estimate can be. The standard deviation of our listener intercepts has decreased. This is to be expected as what previously seemed like variation in listener averages may have actually been variation in listener age effects. The separation of this into two separate variables may lead to the magnitude of variation in one being diminished. Finally, the decrease in the error standard deviation is due to the inclusion of the listener random effects for `A1`, and the fact that included listener-dependent values for `A1` helped explain our variable. 

## Model Comparison

Bayesian models allow you to fit rather complicated 'bespoke' models for your data. A potential problem with this is at some point your model may be 'good enough' such that, although further tweaks could be made, they are no longer contributing any practical benefit to your model. In fact, you can build a model that is so good at predicting your own data that it can actually become bad at understanding the subject of investigation more generally. An analogy can be made between model fitting and bespoke clothing made at a tailor. You can go to a tailor and get a shirt made that exactly fits your body proportions. We can consider that this involves altering the length of the sleeves relative to torso length, adjusting the circumference of the sleeves and torso independently, and perhaps even making each sleeve a lightly different length. Let's imagine that tailoring the perfect shirt for a human body usually involved adjusting ten 'shirt parameters' to ensure a perfect fit for an individual. Despite this, when you go to a store to buy a shirt, there is usually one parameter, size, and it often comes in a small number of variants (e.g. small, medium, large). Since the shirts vary only in a small number of way, they are very unlikely to fit any individual perfectly. Why do stores intentionally sell clothing that doesn't fit people as well as they could? Because the store is interested in selling clothes that fit everyone *pretty well*, rather than selling clothes that fit everyone perfectly. If stores sold shirts that fit people perfectly, each shirt model would only be useful to a small number of people and would be useless for a large number of people. In contrast, by having a small number of shirts that fit the most people 'pretty well', the store can carry a small number of models and apply them successfully to customers in general.  

This general problem is sometimes referred to as the **bias-variance tradeoff**. In this case *bias* refers to the model's ability to represent the 'true' relationship the underlies the data, and variance refers to the degree to which the model is stable across data sets. Models that fit training data very well (i.e. the have a low variance) often change substantially when fit to new data sets (i.e. they have a high variance). This is analogous to the fact that the shape of a perfectly tailored shirt will potentially be very different across two people. On the other hand models that do not fit the data *too* well (i.e. they have a higher bias) often also have more consistent properties across new data sets (i.e. they have a low variance). This is analogous to the fact that two people are much more likely to pick the size size from a small, medium, or large choice of shirt, and these shirts are likely to fit each of these people pretty well. These issues can be understood with repsect to the extent to which our models can explain the behavior of in-sample dat and out-of-sample data. Your **in-sample** data is the data you used to fit the model, and the **out-of-sample** data is other data that you do not have access to. A statistical model is **overfit** when it corresponds too closely to your in-sample data, to the detriment of its ability to explain out-of-sample data. Recall that our models are usually used to carry our inductive reasoning: Going from a limited number of observations to the general case. In light of this, overfitting a model is exactly contrary to the goals of inductive inference. A model that does a poor job of generalizing to new observations is not useful to carry out induction. Further, a model that cannot explain out-of-sample data will not hold up to replication, which should be a concern for any researcher. This is because a researcher carrying out a replication will necessary be working with data that, from your perspective, is out-of-sample data. 

In this section we will offer a high-level conceptual understanding of Bayesian model comparison, along with an explanation of how to carry this out. For more information on the subject we recommend reading chapter X of the truly excellent *Statistical Rethinking* (cite). For an even more thorough treatment on the subject please see X and X. Much of this section is information is  provided in Gelman et al (2013), and with more mathematical detail.

### In-sample and out-of-sample prediction

In order to compare models in a quantitative way, we need some number that can calculated for different models. For Bayesian models, we begin with what is called the **log pointwise predictive density**, abbreviated `lpd` (or `lppd`). The `lpd` can be calculated in several ways, but equation \@ref(eq:61) presents an example of the general case. The `lpd` is the sum of the logarithm of the density for each of your data points. The symbol $\theta$ represents posterior estimates of all of the estimated parameters necessary to determine the probability density over each data point. This value is clearly related to the log posterior density, discussed in section \@ref(c3-log-posterior). Since we are adding logarithmic values, we know we are multiplying the underlying probabilities. If the data points are independent given our parameter values, the `lpd` represents the joint proability of observing the data given the model structure and the parameter estimates ($\theta$). Although prior probabilites are not included in this calculation they do factor into the estimation of $\theta$, and as a result do have an effect on values of `lpd`. 

$$
\begin{equation}
\mathrm{lpd} = \sum_{i=1}^{N} \mathrm{log} (p(y_i | \theta))
(\#eq:67)
\end{equation}
$$

Because $\mathrm{lpd}$ is a logarithmic value, we know large negative values are closer to zero. So, values of $\mathrm{lpd}$ closer to (or above) zero represent models that are more likely given the data. As a result, we might think that we should simply select the model with the highest $\mathrm{lpd}$ as our 'best' model. However, if we are not specifically interested in the in-sample prediction alone, then what we need to be concerned with is the possible characteristics of the out-of-sample prediction. We can think about this in terms of the **expected log pointwise predictive density**, abbreviated $\mathrm{elpd}$ (or $\mathrm{elppd}$), presented in \@ref(eq:62). Unlike the $\mathrm{lpd}$, the $\mathrm{elpd}$ is defined in terms of hypothetical out-of-sample data ($\tilde{y}$) instead of the in-sample data ($y$). The $\mathrm{E}()$ below represents the expected value function, which we have not really discussed to this point. The expected value of a variable is the ... [@@ SB - I don't know how to define this in the general case in an intuitive way]. So, the $\mathrm{elpd}$ basically represents the expected value of $\mathrm{lpd}$ for out-of-sample data. 

$$
\begin{equation}
\mathrm{elpd} = \sum_{i=1}^{N} \mathrm{E}(\mathrm{log} (p(\tilde{y}_i | \theta)))
(\#eq:68)
\end{equation}
$$

Of course, you can't actually know the value of $\mathrm{E}(\mathrm{log} (p(\tilde{y}_i | \theta))$ because you do not have access to the true properties of ($\tilde{y}$). As a result, you must settle for an estimate of $\mathrm{elpd}$, $\widehat{\mathrm{elpd}}$. The simplest, and worst, way to estimate out-of-sample prediction($\mathrm{elpd}$) is by simply looking at the in sample prediction ($\mathrm{lpd}$). One of the reasons that this does not generally work is because making our models more and more complicated will generally improve your in-sample prediction. However, if the predictors are not related to true characteristics of the underlying model, they will also tend to decrease the fit of the model to new data. Returning to the analogy of shirt fits discussed above. You can buy a shirt from the store and take it to a tailor to get it customized. The more the tailor changes this from the 'standard' shape, the better the shirt will fit you and the worse it will fit everyone else. Only alterations which conform to the 'true' average torso shape will increase the fit for people in general (and may in fact reduce the fit for you).

We can demonstrate this for some very simple models using simulations. In the code below, we generate three random variables consisting only of the values -1 and 1. We then use only the first variable (`x1`) to simulate some random data, and we add random Gaussian error to this fake variable. After this we generate two sets of data using the same data generating process, in_sample (`y`) and out-of-sample data (`y_tilde`). We then fit three models to both data sets. These models include increasingly more predictor variables, however, we know that only the first one is useful to understand the underlying process. The models are fit using a function that we have not discussed to this point. The `lm` (linear model) function uses maximum-likelihood estimation (see section X) to fit regression models with relatively simple structures. We use it here because it estimates regression parameters very quickly and therefore is reasonable to use when we want to fit 3000 models in no more than two seconds. 

```{r, cache = TRUE}
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
lpd = matrix (0, iter, 3)
elpd = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)
  x3 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out-of-sample" data
  y_tilde = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y ~ 1+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y ~ 1+x1 + x2)
    if (j==3) mod = lm (y ~ 1+x1 + x2 + x3)
    
    # find the predicted value (mu) for each data point
    mu = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    lpd[i,j] = sum (dnorm (y, mu, sigma, log = TRUE))
    elpd[i,j] = sum (dnorm (y_tilde, mu, sigma, log = TRUE))
  }
}
```

We use the predicted values (`mu`, $\mu$) and (`sigma`, $\sigma$) estimates provided by our maximum-likelihood models to estimate the $\mathrm{lpd}$ and $\mathrm{elpd}$ for each model. In each case, we do so by calculating as in \@ref(eq:63):

$$
\begin{equation}
\mathrm{lpd} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(y_i | \mu, \sigma))
(\#eq:69)
\end{equation}
$$
And \@ref(eq:64), where the $\tilde{y}$ is the out-of-sample data being predicted using the in-sample estimates of $\mu$ and $\sigma$.

$$
\begin{equation}
\mathrm{elpd} = \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(\tilde{y}_i | \mu, \sigma))
(\#eq:610)
\end{equation}
$$

In the formulas above we replace $p(\tilde{y}_i | \theta)$ with $\mathrm{N}(y_i | \mu, \sigma)$ because the simulations generate normally-distributed data. Our simulations result in three sets of $\mathrm{lpd}$ and three sets of $\mathrm{elpd}$, a pair of each for each of the models above. The average of each of these six sets of values is presented in figure X. Since logarithmic values closer to zero are closer to one, *more negative* log-likelihood values indicate a *less probable* outcome. We can see that the $\mathrm{lpd}$ increases as the model becomes more complicated, despite the fact that the $x_1$ and $x_2$ had absolutely no relationship to the data generating process or to the dependent variable. Why does this happen? One way to think about it is that the tallest person in California *and* Arizona will be *at least* as tall as the tallest person in Arizona. By increasing the number of ways it can explain your data, you more complex model will do *at least as well* as the model with fewer possible explanations. However, although the extra parameters increase the $\mathrm{lpd}$, they actually *decrease values* of $\mathrm{elpd}$. Why? The extra predictors included in our model in no way relate to our data, the 'answers' they provide can only be due to overfitting, the learning of chacracteristics that are specific to the in-sample data rather than consistent properties of the out-of-sample data. 

```{r F66, fig.height = 3, fig.width = 8, fig.cap=" (left) Average value of lpd and elpd for each model in the simulations above. (right) The same values as on the left, however, now the elpd is estimated based on the number of parameters.", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.6
################################################################################

means = round (apply (cbind(lpd,elpd), 2, mean),1)

layout (mat=t(c(1:3)), widths = c(.4,.4,.2))
par (mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))

plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "lpd",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
arrows (1,means[1]-0.2, 1,means[4]+0.2,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[5]+0.2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[6]+0.2,code=3, length=0.1,lwd=2)
text ((1:3)+.1, -71, labels = round(c(means[1]-means[4],means[2]-means[5],
                                means[3]-means[6]),2))
text (0.8,c(-68.8,-73.6), labels = c("More Likely","Less likely"), pos=4)

mtext (side=2,text = "log density", line = 2.75)
#legend (3.4,-70.5, legend= c("In sample","out-of-sample"), 
#        col=c(1,2), lwd=2,bty="n")

plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))

plot (0, bty='n', xaxt='n',yaxt='n',xlab="",type="n")
legend (.6,.2,legend = c("lpd","elpd","elpd_hat"), lwd=4, 
        col = c(1,2,2),cex=1.4,lty = c(1,1,3))
```

So, we see that adding 'unneccessary' models can decrease our out-of-sample performance. Ok, so could we just not ever include any 'unnecessary' predictors in our models? Obviously this would be ideal, however, there are some complications. In the example in figure \@ref(fig:F63) we knew the true underlying model and generated out-of-sample data that exactly conformed to the characteristics of our sample. In real life, researchers do not usually have access to out-of-sample data, they do not know the characteristics of the 'true' model that underlies their data, nor can they confirm that any out-of-sample data shares the exact underlying process as their in sample data. As a result, we can never *really* know what the difference is between in sample and out-of-sample prediction for our models. However, you may have noticed that: 1) The slopes of the lines representing $\mathrm{elpd}$ and $\mathrm{lpd}$ in figure \@ref(fig:F63) seem to have predictable slopes, 2) The lines diverge form each other at predictable rates based on the complexity of the models. Mathematicians have noticed this too, and have used this to *adjust* $\mathrm{lpd}$ calculate on in-sample data in order to estimate values of $\mathrm{elpd}$ for out-of-sample data. 

### Out-of-sample prediction: Adjusting predictive accuracy

The logic of adjusting $\mathrm{lpd}$ based on model complexity can be understood with reference to figure \@ref(fig:F63). Our goals is to select the model with the best out-of-sample prediction (i.e. the highest value on the red line), given only knowledge of the in-sample prediction (the values on the black line). In the left plot we can see that the difference between the black and red lines for each model increases by one for every parameter we add. Let us assume for the time being that this is a general property of all models. This means we can subtract one from the $\mathrm{lpd}$ of our least-complicated model to estimate the $\mathrm{elpd}$ for that model, and subtract three from the likelihood of the most-complicated model to estimate the $\mathrm{elpd}$ for that model. In general we can think of this as **penalizing** the value of $\mathrm{lpd}$ based on some penalty value $\mathrm{p}$. Different ways to estimate $\widehat{\mathrm{elpd}}$ differ in terms of how they estimate $\mathrm{lpd}$ and  $\mathrm{p}$. In the very simple example we are discussing here, we are setting $\mathrm{p}=k$, where $k$ is the number of parameters estimated by the model. If we set $\mathrm{p}=k$ and carry out the operation in \@ref(eq:65) usng the values in figure \@ref(fig:F63), we would arrived at values of $\mathrm{elpd}$ of -70.7 (-69.7-1) and -71.7 (-68.7-3) for the least and most complex models. If we chose models based on these estimates of $\widehat{\mathrm{elpd}}$ we would suggest the model with the worst $\mathrm{lpd}$, but the one that actually corresponded to the true data-degenrating process.

$$
\begin{equation}
\widehat{\mathrm{elpd}} = \mathrm{lpd} - \mathrm{p}
(\#eq:611)
\end{equation}
$$

Penalization will not always result in the highest values of $\widehat{\mathrm{elpd}}$ for the simplest model. In figure \@ref(fig:F64) we simulate new fake data, except now we include $x_2$ to the 'real' underlying data generating process (i.e., `y = 1 + x1 + rnorm (n, 0, 1)`). So, for this data we actually *do* need both $x_1$ and $x_2$ in the model. As seen below, penalization does not obscure the benefit of adding $x_2$ to our model when it is warranted. This is because the relatively small penalty associated with the increased model complexity does not overwhelm the large benefit due to the inclusion of a predictor that is actually related to our dependent variable. 

```{r F67, fig.height = 4, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.7
################################################################################
n = 50         # how many observations
iter = 1000    # how many simulations
# these will hold the model log likelihoods for each iteration
in_sample_ll = matrix (0, iter, 3)
out_of_sample_ll = matrix (0, iter, 3)

set.seed(1)
for (i in 1:iter){
  # create 3 random predictors
  x1 = sample (c(-1,1), n, replace=TRUE)
  x2 = sample (c(-1,1), n, replace=TRUE)

  # generate the observed (in sample) data with an  
  # underlying process that only uses the x1 predictor
  y_in = 1 + x1 + rnorm (n, 0, 1)
  # use the same process to simulate some "out-of-sample" data
  y_out = 1 + x1 + rnorm (n, 0, 1)
 
  for (j in 1:3){
    # fit three models, the first using the real underlying model
    if (j==1) mod = lm (y_in ~ 0+x1)
    # the next two include random useless predictors
    if (j==2) mod = lm (y_in ~ 1+x1)
    if (j==3) mod = lm (y_in ~ 1+x1 + x2)
    
    # find the predicted value (mu) for each data point
    yhat = mod$fitted.values
    # and the estimated sigma parameter
    sigma = summary(mod)$sigma
    
    in_sample_ll[i,j] = sum (dnorm (y_in, yhat, sigma, log = TRUE))
    out_of_sample_ll[i,j] = sum (dnorm (y_out, yhat, sigma, log = TRUE))
  }
}



means = round (apply (cbind(in_sample_ll,out_of_sample_ll), 2, mean),1)

par (mfrow = c(1,2), mar = c(4,1,1,.1),oma = c(.1,3,.1,.1))
plot (means[1:3], type = "b", lwd = 2, ylim = range(means)+c(-.5,.5),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "Model log-likelihhod",xlab="Predictors")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
mtext (side=2,text = "Log-likelihood", line = 2.5)
#legend (3.4,-70.5, legend= c("In sample","out-of-sample"), 
#        col=c(1,2), lwd=2,bty="n")

  plot (means[1:3], type = "b", lwd = 2, ylim =c(-73.5,-69),xaxt='n',
      pch=16, xlim = c(.8,3.2), ylab = "",xlab="Predictors",yaxt="n")
axis (side=1,at=1:3,labels = c("x1","x1, x2","x1, x2, x3"))
lines (means[4:6], col =2, lwd=2, type = "b",pch=16)
lines (means[1:3] - c(1:3), col =2, lwd=2, type = "b",pch=16, lty = 2)
arrows (1,means[1]-0.2, 1,means[1]+0.2-1,code=3, length=0.1,lwd=2)
arrows (2,means[2]-0.2, 2,means[2]+0.2-2,code=3, length=0.1,lwd=2)
arrows (3,means[3]-0.2, 3,means[3]+0.2-3,code=3, length=0.1,lwd=2)

text (1:3, means[1:3]+.25, labels = means[1:3])
text (1:3, means[4:6]-.25, labels = means[4:6])
text (1:3, means[1:3]-.25-c(1:3), labels = means[1:3]-c(1:3))
```

Historically, the $\mathrm{p}$ term is related to the number of independent parameters estimated by the model. Estimated parameters are those that are not specified a priori but instead depend on the data and structure of the model. This is not so straightforward in our multilevel models since parameters estimated with partial pooling (and shrinkage) are not fully independent. So, for example, a set of 10 'random effects' may in effect act as fewer than 10 totally independent parameters. In addition, the parameters and penalty terms involved in estimates of elpd have traditionally been *point estimates*, single values often based on maximum-likelihood estimation. Since our models consist of posterior distributions of parameters, we instead have *distributions* of lpd and p. In this section we are only going to discuss the **widely available information criterion** (WAIC) as calculated using the brms package. For a more complete treatement of other ways, and historical approaches, please see Gelman et al. (2013). 
To calculate WAIC we first find the average probability of each data point given our model parameters ($p(y|\theta)$) across all posterior samples ($S$). Then we find the logarithm of this value and add this up across all of our $i$ observations. This is presented in equation \@ref(eq:66).

$$
\begin{equation}
\widehat{\mathrm{lpd}} = \sum_{i=1}^{N} \mathrm{log} (\frac{1}{S} \sum_{s=1}^{S} p(y_i | \theta^s))
(\#eq:612)
\end{equation}
$$

Rather than count the number of parameters estimated in our model, waic estimates the penalty term based on the variance of the logarithm of the posterior probability of each data point across all posterior draws, for each data point as seen in equation \@ref(eq:67). For example, imagein your model contains 5000 posterior samples from the parameters, and 1000 data points. For the first data point, we calculate the log density for each posterior sample. Then, we find the variance of these values across the samples. Then, we repeate this process for all 1000 of our data points and add these values up.

$$
\begin{equation}
\mathrm{p_{waic}} = \sum_{i=1}^{N} Var_{s=1}^{\,S}(\mathrm{log} (p(y_i | \theta^s)))
(\#eq:613)
\end{equation}
$$

Why does this work? This is one of those thing that you may need to get used to rather than understand, at least for now, although explanations can be found in (?). The short, conceptual explanation is that more complex model exhibit more variation in their posterior probabilities. As a result, the variance of the log density across the posterior samples is a general way to estimate model complexity that avoids issues related to how many truly independent parameters a model estimates. Given our estimate of $\widehat{\mathrm{lpd}}$ and $\mathrm{p_{waic}}$, we can now estimate $\widehat{\mathrm{elpd}}_{waic}$ as in \@ref(eq:68).


$$
\begin{equation}
\widehat{\mathrm{elpd}}_{waic} = \widehat{\mathrm{lpd}} - \mathrm{p_{waic}}
(\#eq:614)
\end{equation}
$$
Below we load the sum-coded models we fit in the last chapter:

```{r, eval = FALSE}
# Or download it from the GitHub page:
model_sum_coding = bmmb::get_model ('5_model_sum_coding.RDS')
model_sum_coding_t = bmmb::get_model ('5_model_sum_coding.RDS_t')
```
```{r, include = FALSE}
model_sum_coding = readRDS ('../models/5_model_sum_coding.RDS')
model_sum_coding_t = readRDS ('../models/5_model_sum_coding_t.RDS')
```

And set our options to sum coding to match the coding we used when we fit the models:

```{r}
options (contrasts = c('contr.sum','contr.sum'))
```

We can use the `add_criterion` function in `brms`, and specify `criterion="waic"` to add the `waic` criterion to our model object. We do this for both our model with the normal and the t distributed error. 

```{r, cache = TRUE}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="waic")

model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="waic")
```

Adding the `waic` criterion to our model with Gaussian errors `model_sum_coding`) returns an error message. To understand why we get these errors we can investigate the model `waic` information, which we can see below:

```{r}
model_sum_coding$criteria$waic
```

The three rows represent the expected log pointwise predictive density (`elpd`), the penalty related to the flexibility of the model (`p_waic`) and the third column is the information criterion WAIC (`waic`) which is just -2 times the first row. The `p_waic` penalty term is sometimes referred to as the 'effective number of parameters', but Gelman (cite) cautions that this usage is somewhat figurative and should not be 'over interpreted'. The effective number of parameters relates to the complexity and flexibility of the model and its ability to adapt to new data. Our sum coding model has 114 estimated parameters (15 L + 1 lsd, 94 s + 1 ssd, 2 fe, 1 sigma, 114 = 16 + 95 + 2 +1) but only about 81 effective parameters. The statistics provided above are calculated individually for each data point. We can get the pointwise information as seen below: 

```{r}
model_waic_info = model_sum_coding$criteria$waic$pointwise
model_t_waic_info = model_sum_coding_t$criteria$waic$pointwise
```

Below we have a look at the first six values: 

```{r}
# first six data points
head (model_waic_info)
```

There are as many rows as data points were used to fit the model. The Estimates above simply correspond to the sum of each column. The standard error (SE) corresponds to the variance of the column times the square root of N (since the statistics are sums and not means). We recreate the summary values seen above:

```{r}
# the sum of each column
colSums (model_waic_info)
# the standard deviation of each column
apply (model_waic_info,2,sd) * sqrt(1386)
```

The error message said that "28 (2.0%) p_waic estimates greater than 0.4". Large `p_waic` suggest a poor fit between data points and the model. To investigate this we can get the posterior mean residuals from each of the models we are considering:

```{r, cache = TRUE}
resids = scale(residuals(model_sum_coding)[,1])
resids_t = scale(residuals(model_sum_coding_t)[,1])
```

We scale our residuals so that they represent standard deviations from the mean, and plot these against the `p_waic` value calculated for each data point by each model. Clearly, large residuals relate to large `p_waic` values. In other words, large `p_waic` values are relate to data points that are a very poor fit for our model. In our model with Gaussian errors this relationship is very predictable and quickly leads to large values in `p_waic`. For our model with t-distributed errors (`model_sum_coding_t`) residuals beyond about two standard deviations do not strongly affect `p_waic`. These differences are due to the different behaviors of the normal and t-distributions in their tails, as discussed in section X. 

```{r F68, fig.height = 4, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.8
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot(model_sum_coding$criteria$waic$pointwise[,2], resids, 
     ylim = c(-4.5,4.5), xlim = c(0,1.1), col=cols[6],pch=16)
grid()
abline (v = 0.4)
#abline (h=c(-2.5,2.5))

plot(model_sum_coding_t$criteria$waic$pointwise[,2], resids_t, 
     ylim = c(-4.5,4.5), xlim = c(0,1.1), col=cols[6],pch=16)
grid()
abline (v = 0.4)
```

The warning message suggest that we use `loo` (discussed in the next section) rather than `waic` for model comparison. The reason for this is that, as seen above, some very large residuals in our model with Gaussian error result in large `p_waic` values. As to why large `p_waic` values are bad, when the assumptions underlying `waic` are met, there should not be large `p_waic` values. As a result, large `p_waic` suggest that the assumptions used by this method are being violated. When this occurs, as the error message suggests, you shouldn't use `waic`, even if it seems reliable and looks 'normal'. Think of it this way, if a bridge say it has a weight limit of three tonnes and you're driving a truck that weighs four tonnes, should you drive across the bridge? Maybe you get across safely and save some time. Maybe you crash of the bridge into the river. When you use something like this despite being warned not to you run the same risk: Maybe what you report is a true and reliable analysis, and maybe its not and you are reporting nonsense (the academic equivalent of crashing into the river). 

### Out-of-sample prediction: Cross validation

- explain pareto k, diagnostics

The last way to evaluate models that we'll be discussing is called **cross validation**. Cross validation consists of dividing your data into two groups. You use one group to fit a model (i.e. your in-sample, or *training* data), and then use this model to predict the other group of data (i.e. the out-of-sample, or *testing* data). In this way, a cross validation is a way to simulate out-of sample rpediction using only the data you actually have. There are many ways to carry out cross validation, and these mostly differ in terms of the ways that they divide data into training and testing groups. **Holdout** cross validation selects makes one single partition and uses that to test and train. Obviously this is not ideal because the results will be highly dependent on the specific groups you made. In **k-fold** cross validation you split your data into k equal parts. You then train on parts $k_{-i}$ for iteration $i$, and predict the $ith$ group using that model. This approach can minimize on the problems of holdout cross validation since it uses k, rather than one, estimates of the out-of-sample performance. The logical endpoint of k-fold cross-validation is **leave-one-out** cross validation, sometimes referred to as LOO. In leave-one-out cross validation you leave out a single observation at a time. So, if you have $i$ observations you train the model on all observations but $i$ (i.e. $y_{-i}$), and then predict the lone held out point ($y_i$). Although LOO provides excellent estimates of out-of-sample prediction, in principle this is extremely computationally intensive since it involves the calculation of $i$ models for $i$ data points. For example, out relatively small data set (`notmen`) has 1386 observations, potentially requireing the fitting of 1386 models.

Luckily, advances in the past few years have resulted in fast ways to approximate LOO elpd without having to refit models repeatedly (cite). The `loo` R package, and the associated functions implemented in the `brms` package makes it easy to carry out a LOO cross-validation in a very fast an efficient manner. In addition, this approach is substantially more robust than using `waic` as outlined above. The calculation of $\widehat{\mathrm{elpd}}_{loo}$ using LOO is more complicated than what we can explain here, however, we can provide an approximate definition here. In \@ref(eq:63) we use the $\approx$ symbol because we are not providing an exact definition of how $\widehat{\mathrm{elpd}}_{loo}$ is calculated in practice, but only in principle. Equation \@ref(eq:63) states that the estimate of $\mathrm{elpd}$ provided by LOO cross validation is equal to the sum of the log density of eqach datapoint ($y_i$) based on the parameters estimated using all the data *except* for that point ($\theta_{y_{-i}}$). 

$$
\begin{equation}
\widehat{\mathrm{elpd}}_{loo} \approx \sum_{i=1}^{N} \mathrm{log} (\mathrm{N}(y_i | \theta_{y_{-i}}))
(\#eq:615)
\end{equation}
$$

We define the relationship between $\widehat{\mathrm{elpd}}_{loo}$ and $\widehat{\mathrm{lpd}}$ using the same relationship defined in equation X, pdated for LOO below. 

$$
\begin{equation}
\widehat{\mathrm{elpd}}_{loo} = \widehat{\mathrm{lpd}} - \mathrm{p_{\mathrm{loo}}}
(\#eq:617)
\end{equation}
$$

Based on this, we can estimate $\mathrm{p_{\mathrm{loo}}}$ by rearranging the terms of the equation as below. Just as with WAIC, the $\mathrm{p_{\mathrm{loo}}}$ parameters reflects the effective number of parameters, an estimate of model complexity. 

$$
\begin{equation}
\mathrm{p_{\mathrm{loo}}} = \widehat{\mathrm{lpd}} - \widehat{\mathrm{elpd}}_{loo}
(\#eq:617)
\end{equation}
$$
Estimates of $\widehat{\mathrm{elpd}}_{loo}$ and associated statistics can be added to our models using the code below:

```{r}
model_sum_coding = brms::add_criterion (model_sum_coding, criterion="loo")
model_sum_coding_t = brms::add_criterion (model_sum_coding_t, criterion="loo")
model_re_t = brms::add_criterion (model_re_t, criterion="loo")
```

We can see our new criteria using the code below. Notice that we no longer see the error message we got with WAIC above. 

```{r}
model_sum_coding$criteria$loo
```

Since we no longer have warnings, we can compare our models as seen below. This compares the values of $\widehat{\mathrm{elpd}}_{loo}$ for each data point across all our models, and presents the average difference and the standard error of the difference between the best performing model and the rest of the models. Below, `model_re_t` has an `elpd_diff` of 0, indicating that this is the best model. Values of `elpd_diff` for `model_sum_coding_t` and `model_sum_coding` are -204 and -222 respectively, indicating substantial differences in $\widehat{\mathrm{elpd}}_{loo}$ between our models.

```{r}
brms::loo_compare (model_sum_coding, model_sum_coding_t, model_re_t, criterion = "loo")
```

We can explore where these values come from by extracting the pointwise information from our models just as we did for WAIC. 

```{r}
model_loo_info = model_sum_coding$criteria$loo$pointwise
model_t_loo_info = model_sum_coding_t$criteria$loo$pointwise
model_re_t_lo_info = model_re_t$criteria$loo$pointwise
```

And we can see that the `elpd_diff` is just the sum of the `elpd` estimates across the two models. The standard error of the difference is simply the standard deviation of the difference times the square root of N. 

```{r}
sum(model_re_t_lo_info[,1]-model_t_loo_info[,1])
sd(model_re_t_lo_info[,1]-model_t_loo_info[,1]) * sqrt(1386)
```

We can use this knowledge to calculate the difference between our two worst performing models since these are not directly compared above: 

```{r}
sum(model_t_loo_info[,1]-model_loo_info[,1])
sd(model_t_loo_info[,1]-model_loo_info[,1]) * sqrt(1386)
```

And see that these values correspond to those obtained using the comparison function as seen below:

```{r}
brms::loo_compare (model_sum_coding, model_sum_coding_t, criterion = "loo")
```

Finally, we can check out the estimated number of parameters by summing the third column of our pointwise information for each model, which contains information about `p_loo` for each model. We can compare this to the number of actual parameters estimated by our models. An easy way to find the number of estimated parameters is to get the posterior draws for a model, which contains one column for each estimated parameter, plus one columns for `lp` (discussed in chapter 3). So, if we find the number of columns minus one, we can quickly count the number of estimated parameters for most kinds of models. Below, we compare the actual number of parameters to each model with our effective number of parameters. For example, we can see that going from Gaussian to t distributed errors results in the estimation of one additional parameter ($\nu$), and the number of estimated parameters goes up by about 2.6. Adding random effects for apparent age to our model required the addition of 17 parameters: 15 listener-specific age effects, a standard deviation for these effects, and the correlation between the listener specific age and intercept effects. We can see that this has led to an increase of about 24 effective parameters. We can potentially explain this by thinking about how the inclusion of random effects for apparent age may have affected the estimation of the other random effects already included in the model. However, we are not going to worry too much and 'overinterpret' the effective number of parameters. 

```{r}
# Actual and effective number of parameters for simplest model
ncol(brms::as_draws_matrix(model_sum_coding))-1
sum(model_loo_info[,'p_loo'])

# Actual and effective number of parameters for t model
sum(model_t_loo_info[,'p_loo'])
ncol(brms::as_draws_matrix(model_sum_coding_t))-1

# Actual and effective number of parameters for 'random effects' model
sum(model_re_t_lo_info[,'p_loo'])
ncol(brms::as_draws_matrix(model_re_t))-1
```

### Selecting a model

Which model should we use? Based on the comparison above, we see that `model_re_t` has an elpd that is 204 greater than the next best model. How large does an elpd difference need to be in order to be meaningful. There two things that need to be taken into account: The magnitude of any differences and the uncertainty in these differences. A general rule of thumb seems to be that a difference of at least 2-4 in elpd is necessary in order to signal a meaningful difference between the models (cite akis page and maybe something else if I can find it). However, this is simply a rule of thumb and does not mean that a difference of 1.99 does not matter and a difference of 2.01 does. On the other hand, even large differences between models may not be reliable in the presence of large standard error estimates. For example, below we see the difference between out two smaller models:

```{r}
brms::loo_compare (model_sum_coding, model_sum_coding_t, criterion = "loo")
```

The difference is 18, which is obviously large enough to warrant consideration as a meaningful difference. However, the standard error is 7.3. This means that the difference is only about 2.5 standard errors from zero. How many standard errors away from zero does it need to be in order to be 'real'? This is impossible to say, and in fact the 'reality' of any given effect can not be definitively established by any statistical test. Ok, so how many standard errors from zero should it be before we make a fuss about it? The answer to this seems to be 'several' (cite), somewhere in the neighborhood of 2-5 at least. If we think of improvements to our model as continuous, we can think of differences that are two standard errors away as 'small' and differences that are 5+ standard errors away as 'large'. The decision of where exactly to draw a line between improvements that matter and those that don't is generally up to the researcher. In addition to the consideration of differences in `elpd` in isolation, the researcher may also consider how theoretically or practically warranted the changes in their model are. For example, we more or less know that our data contains way too many outliers to be a proper Gaussian distribution. As a result, although the improvement provided by using t distributed errors is not large, we may choose to keep using t distributed errors in our model anyways. On the other hand, given the small difference between the estimates provided by both models and the not-huge difference in elpd between the models, a researcher may be warranted in simply using normally distributed errors in their model. In contrast to these small differences, we see a difference of 204 in elpd due to the addition of random effects for apparent age, a whopping 10.7 standard errors away from zero. Kepp in mind this does not mean that a researcher *must* include this predictor nor that this structure would necessarily be an aspect of the *real* model. What it *does* mean is that: 1) The addition of random effects for apparent age greatly affects the fit of our model to our data, 2) This increase in fit is very likely to extend to those data we did *not* observe, and 3) We have pretty good evidence that the difference between the models is reliable. We could add to this: 4) It makes *sense* that there would be listener-dependent variation in the effect for apparent age, for several different reasons. Taken together, in this particular situation all of this suggests that our model is substantially improved by the addition of these random effects and so they should be included in our model. 

```{r}
brms::loo_compare (model_sum_coding, model_sum_coding_t, model_re_t, criterion = "loo")
```

Before finishing our discussion of model comparison, we want to show an example of when adding a predictor does *not* help in practice. To do this we add a useless predictor to our data frame, as below. This predictor is a random sample of -1 and 1 with no relationship to our height judgments. We can see below that the average reported height is basically the same for both values of our useless predictor. 

```{r}
set.seed(1)
notmen$useless = sample (c(-1,1),nrow(notmen), replace = TRUE)
tapply (notmen$height, notmen$useless, mean)
```

Below we fit out t distributed random effects model again, however, this time we include the useless predictor in the model formula, and even include a random effect for it:

`height ~ A + useless + (A + useless|L) + (1|S)`

We haven't actually discussed including multiple predictors in our formulas, and in fact we will discuss this in detail in the following chapter. For now, we are only interested in considering how this useless predictor affects our model comparisons. We fit this model below:

```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t_tooBig =  
  brms::brm (height ~ A + useless + (A + useless|L) + (1|S), data = notmen, chains = 4, cores = 4,
       warmup = 1000, iter = 5000, thin = 4, prior = priors, family = "student")
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t_tooBig = bmmb::get_model ('6_model_re_t_tooBig.RDS')
```
```{r, include = FALSE}
model_re_t_tooBig = readRDS ('../models/6_model_re_t_tooBig.RDS')
```

And add the `loo` criterion to it. 

```{r}
model_re_t_tooBig = brms::add_criterion (model_re_t_tooBig, criterion="loo")
```

Finally, we compare this to our equivalent model, minus the useless predictor. 

```{r}
brms::loo_compare (model_re_t, model_re_t_tooBig)
```

We can see that the more-complex model in this case actually has a lower elpd, indicating that our added predictor does not contribute to out of sample prediction. In addition, we can look at a summary of the fixed effects and see that the value of the useless predictor is near zero (0.08), and the credible interval is wide and approximately symmetrical about zero. So, does this mean that this predictor should be excluded from the model? We would be justified in doing so, but may not necessarily want to. For example, if one of our important research questions centered round the value of the useless predictor, the fact that it is around zero and not some other value (e.g. -100) may be useful information. For example, we might want to keep the predictor in the model so we can say something like "So & So (2007) reported a value of -100 for the useless predictor. However, our results suggest that this predictor has an effect on apparent height that is much closer to zero [mean = 0.08, sd = 0.20, 95% CI = [-0.31,0.48])". 

```{r}
brms::fixef(model_re_t_tooBig)
```

## Answering our research questions

We've fit and interpreted updated models, and can now return to what this all 'means' in terms of our research questions:

  (Q1) How tall do speakers perceived as adult females sound? 
  
  (Q2) How tall do speakers perceived as children sound?
  
  (Q3) What is the difference in apparent height associated with the perception of adultness? 
 



An look at our results (and figures) so far suggests that: 

* The magnitude of between-listener and speaker variation is much smaller than the difference between the adult mean and the child mean (4.5 and 2.9 cm, vs 16 cm). This means that the group differences are not overwhelmed by random variation due to differences between the characteristics of speakers and the tendencies of listeners.  

* The magnitude of the random error, i.e. the variation given a certain listener, speaker *and* apparent age judgment, is 8.3 cm. This is larger that the between-listener and between-speaker variation in our data. This means that for any two adults or children selected at random, the expected difference between them will be smaller than the variability in repeated height estimation for any given voice. So, we see that our height judgments are noisy and that this noisiness overwhelms at least some of the systematic variation in our data

* However, the difference in apparent height due to apparent age (17 cm in total) is twice as large as the random error and larger than the between speaker and between listener variation. This means that the systematic variation in apparent height due to apparent age is expected to be quite salient even in the face of the noisiness of our data. 

If we were reporting this is in a paper, based on the sum coded model we might say something like:

> "The overall mean apparent height across all speakers was 156 cm (sd = 1.3, 95% CI = [154.2, 159.2]). Mean apparent height was 164.9 cm (sd = 2.9, 95% CI = [215, 226]) for adult females, and 148.5 cm (sd = 1.38, 95% CI = [146.0, 151.1]) for children. The difference between the group means was 16.4 cm (sd = 0.44, 95% CI = [15.5,  17.2]). The standard deviation of the listener and speaker effects were 4.5 cm (s.d = 1.0, 95% CI = [3.1, 6.9]) and 2.9 cm (s.d = 0.30, 95% CI = [2.3, 3.5]) respectively, while the estimated residual error was 8.3 cm (s.d = 0.1, 95% CI = [8.1, 8.6]). Overall, results indicate a reliable difference in apparent speaker height due to apparent age which is larger than the expected random variation in apparent height judgments".  

Notice that to report the difference between groups, we just double the value of the estimated effect for `adult1`. This is because this reflects the distance of each group to the intercept, and therefore *half* of the distance between the two groups.


 - update, this is from chapter 5

 - this model has a wider interval around our effect. 
 - when between speaker variation exists, this might infl
- here at the end I need a part about how does this invalidate the last model, that reasoning is a problem, bla bla
- model comparison?
- compare previous chapter and this model.
- 95% of published work fatures regular models which we see werent really apprpriate. does that make those fake and results invald. 
 - theres problems with this reasoning
 - also our model is also wrong
 



## Frequentist corner

In frequentist corner, we're going to compare the output of `brms` to some more 'traditional', often called *frequentist* approaches. We're not going to talk about the traditional models in much detail since there are hundreds of other sources for this, and this book is long enough as it is (see section @@ for suggestions). The focus of these sections is simply to highlight the similarities between different approaches, and to point out where to find equivalent information in the different models. If you are already familiar with these approaches, these sections may be helpful. If not, feel free to skip these sections of the book, although they may still be helpful.  

### Bayesian multilevel models vs. lmer

Here we compare the output of `brms` to the output of the `lmer` ("linear mixed-effects regression") function, a very popular function for fitting multilevel models in the `lme4` package in `R`. Below we fit a model that is analogous to our `model_sum_coding` model. Since we set contrasts to sum coding using the options above, this will still be in effect for this model. If you have not done so, run the line:









