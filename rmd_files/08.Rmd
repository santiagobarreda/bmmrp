\newpage
```{r, include = FALSE}
knitr::opts_chunk$set(
  dpi = 400, dev = "jpeg", collapse=TRUE
)
```

# Varying variances, more about priors, and prior predictive checks

So far our models have been a bit 'traditional' in one important way: They have featured a single error term ($\sigma$) for all conditions. This means that $\sigma$ is the same for all speakers, listeners, and conditions in our experiment. However, we might imagine a situation where one listener's responses are more widely distributed than another, resulting in a situation where $\sigma_{[1]} \neq \sigma_{[2]}$ [@@ NHS: these subscripts don't seem consistent with previous ways of indexing listeners...] for two listeners $n1$ and $n2$. In this situation, we may not want to use a single value of $\sigma$ for all listeners and may instead prefer to use a different value for each listener, e.g. $\sigma_{[i]}$ for listener $i$. In this chapter we're going to discuss models that allow for more variation in their $\sigma$ parameters. In addition, we're going to go into more detail about setting priors for our models and the use of prior predictive checks. 


## Chapter pre-cap

--


## Data and Research questions

Below we load the data for our experiment investigating apparent speaker height, in addition to the `brms` and `bmmb` packages. 

```{r, message=FALSE, error=FALSE}
library (brms)
library (bmmb)
data (exp_data)
options (contrasts = c('contr.sum','contr.sum'))
```

The models we will be considering below use the following variables from our data frame:

  * `L`: A number from 1-15 indicating which *listener* responded to the trial, being treated as a character. 
   * `height`: A number representing the *height* (in centimeters) reported for the speaker on each trial. 
  * `S`: A number from 1-139 indicating which *speaker* produced the trial stimulus. 
  * `G`: The *apparent gender* of the speaker indicated by the listener, `f` (female) or `m` (male). 
  * `A`: The *apparent age* of the speaker indicated by the listener, `a` (adult) or `c` (child). 

We're going to use models whose structure is similar to the final model we fit in chapter 7 (`model_interaction`). However, in this chapter we're going to focus on questions related to variation in our standard deviation parameters. We would like to know three things:

  Q1) Does our error standard deviation vary as a function of apparent speaker age?
  
  Q2) Does our error standard deviation vary as a function of listener?

  Q3) Do our speaker random effects exhibit category-specific standard deviations? 
  
## More about priors 

To this point our focus has been on understanding the components that make up our models, and how these are represented using our model parameters. Now that we've covered most of the essentials of using categorical predictors, we can focus a bit more on the prior distributions of the parameters in our models. The reason we've been able to get away with not talking about priors very much is that we have substantial **domain knowledge** regarding the distribution of human height as a function of age and gender. In addition, our models thus far have been relatively simple, making the consideration of prior distributions relatively straightforward. 

For example, based on the information in `height_data`, we know that 11 year-old boys and girls are about 150 cm tall, and women and men are 163 and 176 cm tall respectively, on average. This means that there is an difference of 19.5 cm ($(13+26)/2$) on average between adults and children between 11 and 12 years of age. A difference of 19.5 cm between age groups suggests about a 10 cm distance (half the group difference) between each age group and the overall intercept. In other words, if apparent height is similar to veridical height, we expect an $A$ predictor of about 10 cm in magnitude given sum coding. In contrast, we have an expected height difference of 0 cm across genders for children, and 12 cm across genders for adults for an average difference of 6 cm between males and females. This means that we expect an effect of about 3 cm for gender averaged across ages. Based on this, our prior standard deviation of 12 cm for our `b` (fixed-effects) parameters seems very reasonable. 

However, imagine a situation where there was less certainty about reasonable values for our priors. This can happen for many reasons. For example, imagine you carry out a lexical decisions task where participants listen to a combination of 'real' (e.g., 'map') and 'fake' words (e.g., 'marp') and have to decide if the word they heard is 'real' or not. You divide your real words into 5 groups based on a numerical measure of their frequency (how often they tend to appear in speech). To complicate matters further, you see that your reaction times are *heavily* skewed and so decide to model the logarithm of the reaction times. What should your priors be for your frequency groups? The prior distribution of a parameter represents the expected distribution of your parameters *a priori*. So what do we think are reasonable group differences in this situation? Even in this relatively simple case, setting prior distributions using only your intuitions would require understanding plausible variation in the *logarithm* of reaction times across word groups. This may not be realistic in this situation nor in many others.

### Prior predictive checks {#c8-prior-prediction}

A **prior predictive check** can help understand the consequences of the prior distributions of our parameters, especially in situations where they are otherwise difficult to understand. To carry out a prior predictive check using `brms` you fit your model in the same way you normally would, except for setting `sample_prior="only"`. When you do this, your model knows to sample only from the prior, generating parameter estimates and expected values (e.g., $\mu$) based only on the prior distributions of the model parameters. The prior predictive check consists of generating fake data (i.e., $\tilde{y}$) based on linear predictors (i.e. $\mu$) simulated by sampling *only* from the prior. Conceptually, this is very similar to a posterior predictive check (discussed in section \@ref(c7-posterior-prediction)), save for the fact that prior predictive checks are not influenced by the model likelihood (or the data). 

Below we fit `model_interaction` from chapter 7 again, except this time we sample only from the prior. Imagine that we did not have much knowledge about speaker heights other than that humans tend to be between 1 and 2 meters tall. Based on this we decided to be cautious and use relatively *uninformative* priors and set all prior standard deviations to 1000 cm (10 meters). 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 1000)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 1000)", class = "b"),
           brms::set_prior("student_t(3,0, 1000)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (1000)", class = "cor"), 
           brms::set_prior("student_t(3,0, 1000)", class = "sigma"))

# Fit the model yourself
options (contrasts = c('contr.sum','contr.sum'))
prior_uninformative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)
```

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
prior_uninformative = bmmb::get_model ('8_prior_uninformative.RDS')
```
```{r, include = FALSE}
#  saveRDS (prior_uninformative,"8_prior_uninformative.RDS")
prior_uninformative = readRDS ('../models/8_prior_uninformative.RDS')
```

We can use the `predict` function to get the prior predictions made by our model. 

```{r, cache = TRUE}
pp_uninformative = predict (prior_uninformative, summary = FALSE)
```

When we get 'unsummarized' predictions we get a prediction for each set of sampled parameters in our model. These predictions vary across samples by row and across data points by column. So, to get predictions for a single posterior sample we need to observe a single row of this matrix. This could be done using a single histogram, as below:

```{r, eval = FALSE}
hist (pp_uninformative[1,])
```

However, the `bmmb` package has a simple function called `p_check` that can be used to consider the density of multiple samples at once. By default this compares 10 random predictions, however the number of predictions can be changed, and the user may also specify specific samples to consider. 

```{r, eval = FALSE}
bmmb::p_check (pp_uninformative)
```

In the left plot of figure \@ref(fig:F8-1) we see the result of using `p_check` in ten evenly spaced samples from our 'uninformative' prior prediction. As can be seen, our 'cautious' approach results in a range of simulated heights that make no sense even given our relatively limited prior knowledge, in this case that most humans are between 100 and 200 cm tall. In fact, our simulated data contains not only large negative values, which make no sense in the context of height, but also substantial values above 5000 cm, which is about the size of a 10-story office building.  

```{r F8-1, fig.height = 3, fig.width=8, fig.cap = "(left) Densities of 10 posterior predictions of apparent height for the uninformative prior. (middel) Densities of 10 posterior predictions of apparent height for the mildly informative prior. (right). Densities of 10 posterior predictions of apparent height for the conservative prior.", echo = FALSE, cache = FALSE}

###############################################################################
### Figure 8.1
###############################################################################

prior_mildly_informative  = readRDS ("../models/8_prior_mildly_informative.RDS")
prior_conservative = readRDS ("../models/8_prior_conservative.RDS")

par (mfrow = c(1,3), mar = c(4,4,1,1))
p_check (prior_uninformative, samples = seq(1,4000,length.out=10))
p_check (prior_mildly_informative, samples = seq(1,4000,length.out=10), xlim = c(-100,400))
p_check (prior_conservative, samples = seq(1,4000,length.out=10), xlim = c(-100,400))
```

It is difficult to *prove* that such a wide prior is 'bad', and in fact some authors recommend the use of very uninformative priors (Krushcke 2014). However, it's difficult to deny that a prior that results in prior predictions that are more in line with our domain knowledge is generally better for at least two reasons. First, more informative priors can help the model converge on credible parameter values, especially for complicated models with many parameters. Second, more informative priors can help improve out-of-sample prediction by providing actual information about plausible parameter values for our model. Basically, no fancy statistical model is going to convince anyone that 50 meters tall is a plausible apparent height for human speakers because human speakers are simply not nearly that tall. As a result, a model that acts as if 50 meters is a plausible apparent height for human speakers is likely to offer worse out-of-sample prediction than a model that correctly assigns little to no prior belief to this range of apparent heights. For a more in-depth discussion on the role and function of prior probabilities in multilevel Bayesian models see  (Gelman et al. 2017). Below we again sample from the prior using the setting we've been using so far, and that we used in the previous chapter. 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (12)", class = "cor"), 
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
prior_mildly_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)
```

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
prior_mildly_informative = bmmb::get_model ('8_prior_mildly_informative.RDS')
```
```{r, include = FALSE}
#  saveRDS (prior_mildly_informative,"8_prior_predictions_mildly_informative.RDS")
prior_mildly_informative = readRDS ('../models/8_prior_mildly_informative.RDS')
```

We can make prior predictions again using the `predict` function, and plot these using the `p_check` function. The results of this are shown in middle plot of figure \@ref(fig:F8-1). 

```{r, cache = TRUE, eval = FALSE}
yhat_better = predict (model_interaction_prior_better, summary = FALSE)
```

The priors we've been using do allow for some implausibly large or small apparent height judgments. However, they are constraining the bulk of these to values between 100-200 cm, which is our approximate target. In our opinion, these priors are 'good enough'. However, we may be unhappy that even some implausible values are being generated and want to test out even tighter priors. Below, we sample from priors that are half as wide as those of our mildly informative model above. 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 6)", class = "b"),
           brms::set_prior("student_t(3,0, 6)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 6)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
prior_conservative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = exp_data, chains = 1, cores = 1, warmup = 1000, 
             iter = 5000, thin = 1, prior = priors)
```


```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
prior_conservative = bmmb::get_model ('8_prior_conservative.RDS')
```
```{r, include = FALSE}
#  saveRDS (prior_uninformative,"8_prior_uninformative.RDS")
prior_conservative = readRDS ('../models/8_prior_conservative.RDS')
```

The prior predictions made by this model are presented in the right plot of figure \@ref(fig:F8-1). This time, we see that our prior predictions are even more tightly clustered within our desired range. For the sake of comparison, we fit the three models specified above by erasing `sample_prior="only"` from the function call. We've named these models `model_uninformative`, `model_mildly_informative`, and `model_conservative` respectively.  Below we load the models:

```{r, include = FALSE}
model_uninformative = readRDS ("../models/8_model_uninformative.RDS")
model_mildly_informative = readRDS ("../models/8_model_mildly_informative.RDS")
model_conservative = readRDS ("../models/8_model_conservative.RDS")
```
```{r, eval = FALSE}
model_uninformative = bmmb::get_model ("8_model_uninformative.RDS")
model_mildly_informative = bmmb::get_model ("8_model_mildly_informative.RDS")
model_conservative = bmmb::get_model ("8_model_conservative.RDS")
```

And use `brmplot` to compare the fixed effects estimates for the three models. As can be seen, there is little to no difference in parameter estimates or the credible intervals around them as a function of our prior probabilities. This is in part due to the large amount of data we have, and to the fact that our model is fairly simple relative to the information contained in our data. However, there does appear to be a small reduction in the `A1` parameter as we decrease the value of the prior standard deviation of our fixed effects parameters.  

```{r, eval = FALSE, include = FALSE}
priors = c(brms::set_prior("student_t(3,156, 1000)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 1000)", class = "b"),
           brms::set_prior("student_t(3,0, 1000)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 1000)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
model_uninformative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_uninformative,"../models/8_model_uninformative.RDS")

priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
model_mildly_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_mildly_informative,"../models/8_model_mildly_informative.RDS")

priors = c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 6)", class = "b"),
           brms::set_prior("student_t(3,0, 6)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 6)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
model_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_informative,"../models/8_model_informative.RDS")
```

```{r F8-2, fig.height = 3, fig.width=8, fig.cap = "Comparison of fixed effects estimates and 95% credible intervals for the uninformative (UI), mildly informative (MI), and conservative (C) models.", echo = FALSE, cache = FALSE}

###############################################################################
### Figure 8.2
###############################################################################

model_uninformative = readRDS ("../models/8_model_uninformative.RDS")
model_mildly_informative = readRDS ("../models/8_model_mildly_informative.RDS")
model_conservative = readRDS ("../models/8_model_conservative.RDS")

fixef_1 = fixef(model_uninformative)
fixef_2 = fixef(model_mildly_informative)
fixef_3 = fixef(model_conservative)

par (mfrow = c(1,4), mar = c(4,4,2.5,1))
brmplot (rbind(fixef_1[1,],fixef_2[1,],fixef_3[1,]), ylim = c(154.5,161.5),
         main = "Intercept",labels = c("UI", "MI", "C"),cex.lab=1.3,cex.axis=1.3)
brmplot (rbind(fixef_1[2,],fixef_2[2,],fixef_3[2,]), ylim = c(6.5,13.5),
         main = "A1",labels = c("UI", "MI", "C"),cex.lab=1.3,cex.axis=1.3)
brmplot (rbind(fixef_1[3,],fixef_2[3,],fixef_3[3,]), ylim = c(-6,1),
         main = "G1",labels = c("UI", "MI", "C"),cex.lab=1.3,cex.axis=1.3)
brmplot (rbind(fixef_1[4,],fixef_2[4,],fixef_3[4,]), ylim = c(-6,1),
         main = "A1:G1",labels = c("UI", "MI", "C"),cex.lab=1.3,cex.axis=1.3)
```

### More specific priors 

The final model above was named `conservative` because its priors may have been *too* small. As noted above we actually expect an effect for age of about 10 cm based on variation in veridical height between children and adults, and about 3 cm for the gender effect. When we set our priors using the `class` parameter in the `prior` function, we lose the ability to fine-tune priors for specific parameters. However, we might want to a model with a prior with a standard deviation of 10 cm for the age effect, and a prior standard deviation of 3 cm for the gender effect. We can use the `prior_summary` function in `brms` to see what priors we can set for our model. The output of this function is analogous to that for the `get_priors` function, discussed in sections \@ref(c3-specifying-priors) and \@ref(c4-fitting-1)). 

```{r, eval = FALSE}
brms::prior_summary(model_mildly_informative)
```

We can copy the values of the `class`, `coeff`, and `group` values in the output above to set specific prior probabilities for our `A1` and `G1` parameters, and for the standard deviations of the `A1` and `G1` random effects. In the specification below, the parameters with a standard deviation represent the 'default' case, parameters with a standard deviation of 6 represent age-related effects, and parameters with a standard deviation of 3 represent gender-related effects.

```{r, eval = FALSE}
priors = 
  c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
    brms::set_prior("student_t(3,0, 6)", class = "b"),
    brms::set_prior("student_t(3,0, 10)", class = "b", coef = "A1"),
    brms::set_prior("student_t(3,0, 3)", class = "b", coef = "G1"),
    brms::set_prior("student_t(3,0, 10)", class = "sd"),
    brms::set_prior("student_t(3,0, 3)", class = "sd", coef = "A1", group = "L"),
    brms::set_prior("student_t(3,0, 6)", class = "sd", coef = "G1", group = "L"),
    brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
    brms::set_prior("student_t(3,0, 6)", class = "sigma"))

options (contrasts = c('contr.sum','contr.sum'))
prior_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = exp_data, chains = 1, cores = 1, warmup = 1000, 
             iter = 5000, thin = 1, prior = priors)
```

If we inspect the priors for this model and compare this to the previous one, we can see that now there is between-parameter variation in our priors.

```{r, eval = FALSE}
brms::prior_summary(prior_informative)
```

## Heteroskedasticity and distributional (or mixture) models

Most 'typical' regression models assume that the error variance is the same for all observations, all conditions, all listeners, etc. The property of having a homogeneous variance is called **homoscedasticity**, and models that make this assumption can be said to be **homoscedastic**. Historically, the homogeneity of error variance has been assumed in models because it makes them simpler to work with and understand, and not because it is 'true' for all (or any) data. With Bayesian models it is straightforward to relax and test this assumption, and to build models that exhibit **heteroscedasticity**, differences in the error variance across different subsets of data. These sorts of models are sometimes called **mixture models** because they can be thought of as a mixture of different distributions or **distributional models** because they allow us to model variation in all of the parameters of the data distribution (e.g. $\sigma$ in addition to in $\mu$. 

Consider the residuals for the random effects model we just fit to our data, which we can get from the model using the `residuals` function:

```{r, include = TRUE, eval = FALSE}
# download model
model_interaction = bmmb::get_model ('7_model_interaction.RDS')
```
```{r hi1, include = FALSE}
model_interaction = readRDS ('../models/7_model_interaction.RDS')
```

```{r h12, cache = TRUE}
# get residuals
residuals_interaction = residuals (model_interaction)
```

If we make a boxplot of the posterior means of our residuals we see that their distribution is not exactly equal for all listeners, and also appears to vary based on apparent speaker age. For example, the interquartile range for listener 9 is nearly as wide as the entire distribution of residuals for listener 12. The model we fit at the end of chapter 7 (`model_interaction`) allowed us to estimate variation in apparent height based on apparent age, and based on listener-specific information. In this section we will fit models with age and listener-specific error variances ($\sigma$). By allowing the random error to vary as a function of listener and apparent age, our model may be able to provide more-reliable information regarding our data, in addition to letting us ask questions like "is listener 12's error variance actually smaller than listener 9's, or does it just seem that way?" in a more formal manner. 


```{r F8-3, fig.height = 4, fig.width = 8, fig.cap="(top left) Boxplots of residuals for each listener. (top right) Boxplot of residuals for apparent adults (a) and apparent children (c). (bottom) Boxplots of residuals for each listener, divided according to apparent children and apparent adults. Each color represents a different listener and the left box in each pair represents apparent adults.", cache = FALSE, echo = FALSE}

################################################################################
### Figure 8.3
################################################################################

par (mar = c(2,3,1,1), oma = c(1,2,.1,1))
layout (mat = matrix (c(1,3,1,3,2,3),2,3))
boxplot(residuals_interaction[,1] ~ model_interaction$data$L, cex.lab = 1.3,cex.axis = 1.2,
        col = cols, ylim = c(-20,20), outline=FALSE,xlab='',ylab='')
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$L, 
        col = cols, ylim = c(-20,20), outline=FALSE, add=TRUE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.2)

boxplot(residuals_interaction[,1] ~ model_interaction$data$A, 
        col = cols[2:3], ylim = c(-20,20), outline=FALSE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.3)
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$A, 
        col = cols[2:3], ylim = c(-20,20), outline=FALSE, add=TRUE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.3)
#axis (side=1, at = 1:2, labels = c()
boxplot(residuals_interaction[,1] ~ model_interaction$data$A + model_interaction$data$L, 
        col = rep(cols, each = 2), ylim = c(-32,30), outline=FALSE,xlab='',
        xaxt='n',ylab='', cex.lab = 1.3,cex.axis = 1.3)
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$A + model_interaction$data$L,ylab='', 
        col = rep(cols, each = 2), ylim = c(-32,30), outline=FALSE, add=TRUE,
        xlab='',xaxt='n', cex.lab = 1.3,cex.axis = 1.3)
axis (side = 1, at = seq(1.5,29.5,2), labels = 1:15,cex.axis = 1.3)

mtext (side=2, line = 0.3,outer = TRUE, "Residuals (in centimeters)")
```

To this point our models have all looked something like \@ref(eq:8-1), with the generic predictors $x_n$. In these models $\mu$ got a subscript because it varied from trial to trial and was being predicted, but $\sigma$ has never received a subscript because it hasn't varied in our models.  

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma) \\ 
\mu_{[i]} = \mathrm{x}_1 + \mathrm{x}_2+ \dots + \mathrm{x}_n \\ 
\end{split}
(\#eq:8-1)
\end{equation}
$$

Instead, we can fit models that helps us understand systematic variation in both the mean *and* the $\sigma$ parameters in our model. We can imagine this by adding a subscript to $\sigma$ and predicting its value from trial to trial using predictors, as seen in \@ref(eq:8-2). We've given these predictors little $\sigma$ subscripts just to distinguish them conceptually from those predicting $\mu$, but they could be the same predictors used to understand $\mu$ or a different set of predictors. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\mu_{[i]} = \mathrm{x}_1 + \mathrm{x}_2+ \dots + \mathrm{x}_n \\ 
\sigma_{[i]} = \mathrm{x}_{\sigma1} + \mathrm{x}_{\sigma2} + \dots + \mathrm{x}_{\sigma n} \\ \\ 
\end{split}
(\#eq:8-2)
\end{equation}
$$

## A 'simple' model: Error varies according to a single fixed effect

We will begin with a 'simple' heteroscedastic model that allows for variation in $\sigma$ based on apparent age. After this, we will fit a model that includes listener-dependent variation in $\sigma$ as well.

### Description of our model

The `brms` package makes it exceptionally easy to fit heteroscedastic models by letting you write formulas for the error just as you do for the mean. For example, the model formula for our previous model was:

`height ~ A*G + (A*G|L) + (1|S)`

This told `brms` "model apparent height as a function of an intercept, an effect for apparent age, an effect for apparent gender, the interaction of the two, listener-specific adjustments to all predictors, and speaker-specific intercepts". Implicitly, we know that all of this models variation in the $\mu$ parameter of a normal distribution with a fixed $\sigma$. We want our model to consider the possibility that the value of $\sigma$ may vary based on whether the speaker is judged to be an adult or a child. We can do this by including a separate formula for our error (called `sigma` by `brms`) using the following formula:

`sigma ~ A`

This formula tells `brms` "model the standard deviation of the error as varying around an overall intercept and deviations of this based on the `A`, a predictor in indicating apparent age". We can 'stick' our two model formulas together using the `bf` (Bayes formula) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ A)
```

The problem with modeling the standard deviation parameter directly is that negative values are impossible, so that standard deviations are bounded by zero. To deal with this, `brms` models `log(sigma)`, the logarithm of the error standard deviation, rather than `sigma` directly. Logarithms help model standard deviations because they map all numbers from 0 to 1 to values between negative infinity and 0. This allows one to model very small values of $\sigma$ while at the same time never worrying about estimating a negative standard deviation (for more on logarithms see section \@ref(c2-logarithms)). Since we are modeling log-transformed sigmas, we now specify the priors for sigma in logarithmic values. Previously, our prior for sigma was a t-distribution with a standard deviation of 12. This meant that the majority of the mass of the distribution was going to be between 0 and 24 cm (two standard deviations from the mean). Now, we set the distribution to 1.5, meaning that we expect most predictors related to $\sigma$ to be between $exp(-3)=0.05$ and $exp(3)=20.1$. The full model specification is provided below: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\mu_{[i]},\sigma, \nu) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1 + G1 + A1 \colon G1 + L_{[\mathsf{L}_{[i]}]} \\ + A1 \colon L_{[\mathsf{L}_{[i]}]} + G1 \colon L_{[\mathsf{L}_{[i]}]} + A1 \colon G1 \colon L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\ 
\\ \sigma_{[i]} = \mathrm{Intercept_{\sigma}} + A1_{\sigma} \\ \\ 
\mathrm{Priors:} \\ 
S_{[\bullet]} \sim t(3,0,\sigma_S) \\
\begin{bmatrix} L_{[\bullet]} \\ A \colon L_{[\bullet]} \\ G \colon L_{[\bullet]} \\ A \colon G \colon L_{[\bullet]} \end{bmatrix} \sim \mathrm{MVNormal} \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \Sigma \right) \\ \\ 
\mathrm{Intercept} \sim t(3,156,12) \\
A, G, A \colon G \sim t(3,0,12) \\
\sigma_L, \sigma_{A \colon L}, \sigma_{G \colon L}, \sigma_{A \colon G \colon L}, \sigma_S \sim t(3,0,12) \\
\sigma \sim t(3,0,12) \\
\nu \sim gamma(2, 0.1) \\ 
R \sim \mathrm{LKJCorr} (2) \\ 
\mathrm{Intercept_{\sigma}} \sim t(3, 0,1.5) \\
A1_\sigma \sim t(3, 0,1.5) \\
\end{split}
(\#eq:8-3)
\end{equation}
$$

This model is quite large, but it only contains five new (or modified) lines compared to model presented in \@ref(eq:7-8). These lines are presented in \@ref(eq:8-4). First, our model now uses a trial-specific error term for trial $i$ ($\sigma_{[i]}$), whereas it previously always used a constant standard deviation. Second, we are now modeling this trial-specific $\sigma$ parameter using an intercept specific to our sigma term ($Intercept_{\sigma}$), and a deviation from this base on apparent age ($A1_{\sigma}$). Finally, in the last two lines, we specify the prior for our sigma intercept, and for the \sigma related effect for apparent age. In a sense, the model expressed in \@ref(eq:8-3) is effectively the model in \@ref(eq:7-8), with the additional structure in \@ref(eq:8-4) added to it. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + A1_{\sigma} \\ \\
\mathrm{Intercept_{\sigma}} \sim t(3, 0,1.5) \\
A1_\sigma \sim t(3, 0,1.5) \\
\end{split}
(\#eq:8-4)
\end{equation}
$$

### Prior predictive checks

Recall that when we use the `set_priors` function we can use the `class` parameter to specify priors for whole classes of parameters at a time. The classes we have discussed so far are:  

-   `Intercept`: this is a unique class, only for intercepts.
-   `sd`: this is for our standard deviation parameters that relate to 'batches' of parameters, e.g. `sd(Intercept)` for `L` ($\sigma_{L}$).
-   `sigma`: the error term.
-   `cor`: priors for 'random effect' correlation terms. 

For the first time we need to specify priors for terms for $\sigma$ rather than $\mu$. We can do this by setting `dpar="sigma"` in the `set_prior` function. As noted just above, you can see which priors need setting, and which parameters need to be specified for a model, by using the `get_prior` function and providing your formula, data, and family.

```{r, eval = FALSE}
brms::get_prior (brms::bf(height ~ A*G + (A*G|L) + (1|S),
                          sigma ~ A),
                 data = exp_data, family="student")
```

If you run the line above you will see that we have two parameters where `dpar=sigma`: An overall intercept (`class = "Intercept", dpar = "sigma"`), and an age term for sigma (`class = "b",coef="A1",dpar="sigma"`). We can set priors for these element using the lines:

```{r, eval = FALSE}
brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma")
brms::set_prior("student_t(3, 0, 1.5)", class = "b", dpar = "sigma")
```

Notice that we set these priors using the classes we have already discussed, `Intercept` and `b` respectively. However, now we set `dpar` (distributional parameter) to `sigma` to tell `brm` that these priors are specifically for parameters related to the model error term, and not to variation in predicted values (i.e. $\nu$). Below we fit the model sampling only from our priors:

```{r, eval = FALSE}
# Fit the model yourself
# set the contrasts
options (contrasts = c('contr.sum','contr.sum'))

model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ A)

priors = c(brms::set_prior("student_t(3, 156, 12)", class = "Intercept"),
          brms::set_prior("student_t(3, 0, 12)", class = "b"),
          brms::set_prior("student_t(3, 0, 12)", class = "sd"),
          brms::set_prior("gamma(2, 0.1)", class = "nu"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "b", dpar = "sigma"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"))

prior_A_sigma = 
  brms::brm (model_formula, data = exp_data, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors, sample_prior = "only")
```

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
prior_A_sigma = bmmb::get_model ('8_prior_A_sigma.RDS')
```
```{r, include = FALSE}
#  saveRDS (prior_uninformative,"8_prior_A_sigma.RDS")
prior_A_sigma = readRDS ('../models/8_prior_A_sigma.RDS')
```

Make, and investigate our prior predictions:

```{r, eval = FALSE, cache = TRUE}
pp_A_sigma = predict (prior_A_sigma, summary = FALSE)
p_check (pp_A_sigma, samples = seq(400,4000,400))
```

We're not going to print them out this time, but we assure you they are reasonable. 

### Fitting and interpreting the model

For the first time below, we specify both the model formula (using the `bf` function) and the priors (using the `set_prior` function) outside of the call to `brm`. We do this to preserve the legibility of the code in general. 

```{r, eval = FALSE}
# Fit the model yourself
# set the contrasts
options (contrasts = c('contr.sum','contr.sum'))

model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ A)

priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "b", dpar = "sigma"))

model_A_sigma = 
  brms::brm (model_formula, data = exp_data, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_A_sigma = bmmb::get_model ('8_model_A_sigma.RDS')
```
```{r, include = FALSE}
# saveRDS (model_A_sigma, '../models/8_model_A_sigma.RDS')
model_A_sigma = readRDS ('../models/8_model_A_sigma.RDS')
```

If we look at the model 'fixed' (`population-Level1) effects:

```{r}
fixef (model_A_sigma)
```

We see a two new lines reflecting our new model parameters, both in the `Population-level effects`: `sigma_Intercept` and `sigma_A1`. These terms represent the intercept of our `sigma` ($\sigma$) term and the effect for apparent adultness on `sigma`. These effects can be interpreted in a very similar way as we interpret the corresponding effect for $\mu$: `Intercept` and `A1`. The value of `sigma_Intercept` represents the grand mean for our `sigma` ($\sigma$) term. The value of `sigma_A1` reflects the difference in `sigma` between the intercept and the value when speakers were perceived as adults. Since we are using sum coding `A2`, the effect for apparent children, is not estimated. However, we can recover `A2` since `A1 = -A2`. One easy way to do this is using the `hypothesis` function (or its clone in `bmmb`) as follows:

```{r}
sigmas = short_hypothesis(model_A_sigma, 
                          c("exp(sigma_Intercept)=0",
                            "exp(sigma_Intercept+sigma_A1)=0",
                            "exp(sigma_Intercept-sigma_A1)=0"))
sigmas
```

Since our model estimates `log(sigma)` and not `sigma` directly, to get our values of sigma we need to exponentiate our parameter estimates. We can do this directly in the `hypothesis` function as seen above. The three estimated error terms are presented in figure \@ref(fig:F8-4). Clearly, there are substantial differences in the value of $\sigma$ based on the perceived adultness of the speaker: The value of $\sigma$ was 61% greater (7.1/4.4) when listeners identified the speaker as a child. 

```{r F8-4, fig.height = 3, fig.width = 8, fig.cap="(left) Error standard deviation estimates in all situations, for apparent adults and for apparent children. (right) Comparison of some fixed-effects parameters shared by our heteroscedastic and homoscedastic models.", cache = FALSE, echo = FALSE}

################################################################################
### Figure 8.4
################################################################################

par (mfrow = c(1,2), mar = c(2.2,4,0.5,1))
brmplot (sigmas, col = cols[4:6], labels = c("Overall","Adults","Children"),
         ylab = "Centimeters", ylim = c(3.7,8))
brmplot (fixef(model_A_sigma)[c(3:5),], col = cols,nudge= -.1,pch=17,
         labels = c("A1","G1","A1:G1"),ylab = "Centimeters")
brmplot (fixef(model_interaction)[2:4,], col = cols, add = TRUE,
         nudge=.1,labels="")
#abline (h = 7.71)
legend (1.7,10, legend = c("Heteroscedastic","Homoscedastic"), pch = c(16,17), bty='n')
```

We might wonder two things about this new model. First, has it had any effect on our fixed-effect parameters? Second, is the additional model complexity justified? We can compare this heteroskedastic model to the equivalent homoskedastic model we fit in chapter 7 (`model_interaction`, loaded above). We can assess the first question visually using the `brmplot` function as seen in the right plot of figure \@ref(fig:F8-4). It doesn't seem like the change has had much impact on our parameter estimates of the intervals around them. We can make a more formal comparison using cross validation (discussed in section \@ref(c6-out-sample-crossval)). We add the `loo` (leave one out) criterion to each model and make a comparison.     

```{r, collapse = TRUE, cache = TRUE}
model_interaction = add_criterion(model_interaction,"loo")
model_A_sigma = add_criterion(model_A_sigma,"loo")

loo_compare (model_interaction, model_A_sigma)
```

We can see that there is a substantial difference in $\mathrm{elpd}$ between models, and that this difference just over four times the size of the standard deviation. Based on this, it would seem that using the more complex model is justifiable. We will leave a discussion of whether the difference 'matters' for the section on answering our research questions. 

## A 'complex' model: Error varies according to fixed and random effects

In the previous section we fit a relatively 'simple' heteroscedastic model. This model allowed for error variances to vary based on the perceived adultness, and so was very similar to the models we fit in chapter 5. Here, we're going to fit a substantially more complex model that predicts variation in $\sigma$ using listener-dependent 'random effects' in the same way we would for our prediction of $\mu$. 

### Description of our model {#c8-description-2}

Our previous model had the following formula for the `sigma` term:

`sigma ~ A`

This told `brms` "model `sigma` as a function of an intercept and an effect for apparent age". We can add to our formula by specifying a 'random effects' term for listeners, and a 'random slope' for age (i.e. an interaction between apparent age and listener), both estimated using partial pooling.

`sigma ~ A + (A|L)`

This formula tells `brms` "model `sigma` as varying around an overall intercept, an effect for apparent age, listener-specific deviations, and the interaction of apparent age and listener". Effectively, we have two separate model formulas: One for a model with two categorical predictors and an interaction (like those in chapter 7), and one for a model comparing two groups (like those in chapter 6). We can 'stick' our two model formulas together using the `bf` (or `brmsformula`) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ A + (A|L))
```

The full model specification is provided below (omitting the 'constructions' of the $\Sigma$ terms): 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1 + G1 + A1 \colon G1 + L_{[\mathsf{L}_{[i]}]} + A1 \colon L_{[\mathsf{L}_{[i]}]} + G1 \colon L_{[\mathsf{L}_{[i]}]} + A1 \colon G1 \colon L_{[\mathsf{L}_{[i]}]} + S_{[\mathsf{S}_{[i]}]} \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + A1 + A1 \colon L_{\sigma[L_{[i]}]} + L_{\sigma[L_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
\begin{bmatrix} L_{[\bullet]} \\ A1 \colon L_{[\bullet]} \\ G1 \colon L_{[\bullet]} \\ A1 \colon G1 \colon L_{[\bullet]} \\  \end{bmatrix} \sim \mathrm{MVNormal} \left( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \Sigma_{L} \right) \\ 
\\
\begin{bmatrix} L_{\sigma[\bullet]} \\ A1 \colon L_{\sigma[\bullet]} \end{bmatrix} \sim \mathrm{MVNormal} \left( \begin{bmatrix} 0 \\ 0 \\ \end{bmatrix}, \Sigma_{\sigma} \right) \\ 
\\
S_{[\bullet]} \sim N(0,\sigma_S) \\ \\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma, \sigma_L, \sigma_{A1 \colon L}, \sigma_S \sim N(0,12) \\
\\
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma}, \sigma_{A1 \colon L_\sigma} \sim N(0,1.5) \\\\
R \sim \mathrm{LKJCorr} (2)
\end{split}
(\#eq:8-5)
\end{equation}
$$

Notice that the parameters drawn related to sigma ($L_{\sigma}, A1 \colon L_{\sigma}$) are drawn from a different multivariate normal relative to the other listener-dependent parameters. This means that we get estimates of the standard deviations of $L_{\sigma}$ and $A1 \colon L_{\sigma}$, and the correlation of these to each other, but not the correlation of these to the other listener-dependent parameters.

### Fitting and interpreting the model

We need to specify a prior distribution for the standard deviation of our listener-dependent `sigma` parameters, $\sigma_{L_\sigma}$ and $\sigma_{A1 \colon L_\sigma}$. We can do this by specifying a prior for `class = "sd"` and `dpar = "sigma"`. Note that this is very similar to the way we fit priors for the intercept and fixed effects for `sigma`.

```{r, eval = FALSE}
# Fit the model yourself
# set the contrasts
options (contrasts = c('contr.sum','contr.sum'))

model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ A + (A|L))

priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "b", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "sd", dpar = "sigma"))

model_A_L_sigma = 
  brms::brm (model_formula, data = exp_data, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_A_L_sigma = bmmb::get_model ('8_model_A_L_sigma.RDS')
```
```{r, include = FALSE}
# saveRDS (model_A_L_sigma, '../models/8_model_A_L_sigma.RDS')
model_A_L_sigma = readRDS ('../models/8_model_A_L_sigma.RDS')
```

If we look at the model output we see see the same `sigma_Intercept` and `sigma_A1` we saw before. There are also a few new lines in the listener `Group-Level Effects`. First, we see `sd(sigma_Intercept)` and `sd(sigma_A1)`, which represent the standard deviations of the listener-specific intercepts (for `sigma`) and of the age effects (for `sigma`). Second, we see the correlations between our new random effects (i.e. `cor(sigma_Intercept,sigma_A1)`). 

```{r, eval = FALSE}
bmmb::short_summary (model_A_L_sigma)
```

Our listener-specific adjustments to the sigma intercept were fit with partial pooling, i.e. they are 'random' effects. As such, we can get these in the same way as for other random effects terms (see section \@ref(c5-manipulating-random-effects)). Here, we will use the `hypothesis` function to get the listener random effects themselves (`sigmas_centered`, $L_{\sigma}$), and the sum of these with the sigma intercept (`sigmas`), resulting in the logarithm of the listener-specific $\sigma$ parameter ($\mathrm{Intercept}_{\sigma} + L_{\sigma[i]}$). We compare these in figure \@ref(fig:F8-5), exponentiating the recreated `sigma` values to get the listener-specific $\sigma$ terms. 

```{r}
sigmas_centered = short_hypothesis(model_A_L_sigma, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(model_A_L_sigma, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

```{r F8-5, fig.height = 3, fig.width = 8, fig.cap="(left) Values of $L_{\\sigma}$, listener-dependent variations from the `sigma` intercept (expressed as a logarithm). (right) Listener-specific sigma parameters. The result of $\\exp(\\mathrm{Intercept}_{\\sigma} + L_{\\sigma[i]})$ for listener $i$. We exponentiate the value in the plot so it reflects `sigma` rather than `log(sigma)`.", cache = FALSE, echo = FALSE}

################################################################################
### Figure 8.5
################################################################################

par (mfrow = c(1,2), mar = c(3,4,1,1))
brmplot (sigmas_centered, col = cols,labels = "")
axis (side=1, at = 1:15)
brmplot (exp(sigmas[,1:4]), col = cols, ylim = c(1,13),labels="")
abline (h = exp(1.74))
brmplot (exp(sigmas[,1:4]), col = cols, add=TRUE, labels="")
axis (side=1, at = 1:15)
```

It certainly seems as though there is legitimate between-listener variation in the size of $sigma$. We might wonder, is this added complexity a good thing? We can add the `loo` criterion to our model:

```{r}
model_A_L_sigma = add_criterion(model_A_L_sigma, "loo")
```

And compare this model, the previous 'simple' heteroscedastic model (`model_A_sigma`), and the homoscedastic model we fit in the previous chapter (`model_interaction`). 

```{r}
loo_compare (model_interaction, model_A_sigma, model_A_L_sigma)
```

This time we find a very large difference in $\mathrm{elpd}$, 7.8 times greater than the standard error of the difference. Again, it seems like the more complex is justified and arguably 'better'. 


## Answering our research questions

The research questions we posed above were:

  Q1) Does our error standard deviation vary as a function of apparent speaker age?
  
  Q2) Does our error standard deviation vary as a function of listener?

Based on the information we've already provided, we can answer these questions: Yes, yes, and yes. We think these results are reasonable based on what we know about the heights of adults and children and about the average speaker's average knowledge of the heights of adults and children. For example, we did think it was reasonable to expect that listener's height judgments would be more variable for children than for adults. We think most people have a good handle on how tall adults tend to be. Does the average person know how tall 10-12 year-old children tend to be? We don't think so. In addition, 10-12 year olds can show a lot of variation in height based on differences in growth rates, which might also lead people to provide more variable estimates for the height of children. We think it also 'makes sense' that different listeners could be more or less systematic (i.e. predictable) than others, for many reasons. As for the final question, based on our discussion so far we also think its reasonable that the average rated heights for different children vary more with respect to those of adults. 

We're going to add another, more meta, question: Which should we report? We can approach this from the perspective that the best model according to $\mathrm{elpd}$ is the 'real' model. Based on the comparison of $\mathrm{elpd}$  above, it seems that the last model we fit (`model_A_L_sigma`) is the 'best' model. Does this mean it is the 'real' model? And does it mean that this is the one we ought to report? The short answers to these questions are no and no. The fit between a model and data can't 'prove' that the model represents the 'real' relationship underlying the data. The reason for this is that there are potentially a large number of other, slightly different, models that provide just as good a fit to the data, or perhaps a better one. If the best model is the real one, how can you ever know you have the best model? Further, even if you did somehow find the best model for the data you have, how could you guarantee that it will also be the best model for the data you don't have? It's impossible to know if some new data might come along that will not fit your model as well as one of the other model, making the 'best' model contingent on the data you have seen so far.  

Ok, so we can't prove that our best model is the real one, fine. But its still the best one we have. Do we necessarily need to report the 'best' model? This sort of reasoning can lead to serious problems because there is almost always a better 'truer' model out there, and we often don't try to find it. For example, many people would never think to fit a heteroscedastic model and so would not even worry about reporting it. Does the existence of a hypothetical better model mean that the model they report is invalid? We don't think so. If it doesn't invalidate *their* simpler model, then why should this be the case for the researcher that *did* think to try the heteroscedastic model? 

In general, we can imagine that 10 people might approach any given research question in 10 different ways, a concept sometimes referred to as *researcher degrees of freedom* (Gelman and Loken 2013) since it reflects model variation due to variation in researcher decisions rather than the data. Slight differences in model structure and data processing results in slight differences in model outputs, resulting in a sort of 'distribution' for any given result. How can a fixed underlying reality result in a distribution or results? When they are all slightly wrong. 

Rather than think of our models as representations of *reality*, we can think of them as mathematical implementations of our research questions. The model we report should include the information and structure that we think are necessary to represent and investigate our research questions. Using a different model can result in different results given the same data, but asking a different question can also lead to different results given then same data. When interpreting a statistical analysis, it is very useful to keep in mind that results are contingent on:

  1) The data you collected. Given other data you may have come to different conclusions.
  
  2) The model you chose. Given another model you may have come to different conclusions.
  
So which model should we report? One way to think about our models is that they provide different *information* about our data. If we are primarily interested in the fixed effects then maybe we are not interested in this additional information. However, if we want to know about differences in error variation then we need one of the more complicated models. It's also worth considering how the differing model structures might affect the information they share in common, in this case the model 'fixed effects'. Figure \@ref(fig:F8-6) compares the fixed effects, random effects, and predicted values made by three models: `model_interaction`, `model_A_sigma`, and `model_A_L_sigma`. 

```{r, include = FALSE, cache = TRUE}
ranefs1 = ranef (model_interaction)
ranefs2 = ranef (model_A_sigma)
ranefs3 = ranef (model_A_L_sigma)

fixefs1 = fixef (model_interaction)
fixefs2 = fixef (model_A_sigma)
fixefs3 = fixef (model_A_L_sigma)

yhat1 = predict (model_interaction)
yhat2 = predict (model_A_sigma)
yhat3 = predict (model_A_L_sigma)

fixefs1[1,c(1,3,4)] = fixefs1[1,c(1,3,4)] - 159
fixefs2[1,c(1,3,4)] = fixefs2[1,c(1,3,4)] - 159
fixefs3[1,c(1,3,4)] = fixefs3[1,c(1,3,4)] - 159
```

```{r F8-6, fig.height = 4, fig.width = 8, fig.cap="(left) Comparison of fixed effect estimates and 95% credible intervals for three models. We subtracted 159 from the model intercept so that it would fit on the same scale as the other parameters. (right) Plots showing comparisons of parameter estimates for pairs of the same models presented in the left plot. Each row compares a different pair od models and each column presents a different set of parameters. The columns represent all listener random effects (left), all speaker random effects (middle), and the predictions made by each model (right).", cache = FALSE, echo = FALSE}

################################################################################
### Figure 8.6
################################################################################

par (mar = c(2,4,1,2.5), oma = c(1,1,1,.1))

layout (m = matrix (c(1,1,1,1,1,1,2,5,8,3,6,9,4,7,10),3,5))

brmplot (fixefs1, col=cols[2],labels = "", nudge= -0.1, ylim = c(-5,15))
brmplot (fixefs2[c(1,3,4,5),], add=TRUE, nudge=0, col=cols[3],labels = "")
brmplot (fixefs3[c(1,3,4,5),], add=TRUE, nudge=0.1, col=cols[4],labels = "")

axis (side=1,at=1:4,c("Int.","A1","G1","A1:G1"),cex.axis=1.3)
legend (2,6,legend = c("M1:model_interaction", "M2:model_A_sigma", "M3:model_A_L_sigma"), 
        col = cols[2:5], pch=16,pt.cex=1.3, bty = "n")
mtext (side=3,outer=FALSE, "Fixed Effects",cex=.9)

par (mar = c(1.5,2,1.3,1))

plot (ranefs1$L[,1,], ranefs2$L[,1,1:4],pch=16,col=4,
      xlim=c(-10,10),ylim=c(-10,10),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
mtext (side=3,outer=FALSE, "Listener Effects",cex=.9)
text (-2,-6,"x=M1,y=M2",pos=4)
plot (ranefs1$S[,1,], ranefs2$S[,1,], pch=16,col=4,
      xlim=c(-11,5),ylim=c(-11,5),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
mtext (side=3,outer=FALSE, "Speaker Effects",cex=.9)
plot (yhat1[,1],yhat2[,1],pch=16,col=4,cex=1.25,
      xlim=c(120,190), ylim=c(120,190),xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
mtext (side=3,outer=FALSE, "Predictions",cex=.9)

plot (ranefs1$L[,1,], ranefs3$L[,1,1:4], pch=16,col=4,
      xlim=c(-10,10),ylim=c(-10,10),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
text (-2,-6,"x=M1,y=M3",pos=4)
plot (ranefs1$S[,1,], ranefs3$S[,1,], pch=16,col=4,
      xlim=c(-11,5),ylim=c(-11,5),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
plot (yhat1[,1],yhat3[,1],pch=16,col=4,cex=1.25,
      xlim=c(120,190), ylim=c(120,190),xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)

plot (ranefs2$L[,1,1:4], ranefs3$L[,1,1:4], pch=16,col=4,
      xlim=c(-10,10),ylim=c(-10,10),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
text (-2,-6,"x=M2,y=M3",pos=4)
plot (ranefs2$S[,1,], ranefs3$S[,1,], pch=16,col=4,
      xlim=c(-11,5),ylim=c(-11,5),cex=1.25,lwd=2,xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)
plot (yhat2[,1],yhat3[,1],pch=16,col=4,cex=1.25,
      xlim=c(120,190), ylim=c(120,190),xlab="",ylab="")
abline (0,1,col=2,lwd=1,lty=2)

```

We can see that the differences in the fixed effects are so small as to be largely meaningless. The three models also provide nearly identical listener 'random effects' and predicted values. Based on this, if we care primarily about the fixed effects and understanding apparent height judgments, any of the three models will work equally well. Where the models differ is in their estimates of the speaker effects. The inclusion of heteroscedasticity seems to reduce the magnitude of the speaker effects relative to the homoscedastic model (`model_interaction`). If we were specifically interested in these effects then we would need to take the time to understand the source, and meaning, of these differences. Of course, the experiment we ran was not designed to investigate speaker differences, so it does not make much sense to delve to far into it in our analysis. Our experiment was designed to investigate the perception of apparent height and the role of speaker age and gender in these judgments. From this perspective, it seems that either of the three models is suitable. 


## Building identifiable and supportable models

Before finishing this chapter we want to talk about an extremely important issue that we have not discussed to this point: Whether you *should* fit the complex model you are thinking of for your data. To this point we haven't discussed this much, but as our models get more and more complicated it is something we need to think about. There are generally two constraints on the sorts of models you can fit to your data. 

The first of these is issues related to the **identifiability** of different model parameters, that is, the ability of your model to estimate unique and independent values for each of your model parameters. Models with parameters that are not **identifiable** mean that these parameters cannot be estimated no matter how much data you have available. We have actually seen several examples of this issue already. When we discussed contrasts (section X), and when we introduced modeling factors with many levels (section X), we talked about the fact that you can't estimate all levels of a factor. The cause of these problems is a lack of *identifiability* in the parameters. 

The second general constraint is related to whether is **supported** by your data, meaning you have enough data to realistically estimate all of your model parameters. In many cases involving repeated-measures data this will not be an issue as there may be a large number of observations for each group (e.g. subject), and a medium to large number of groups (>30) in the experiment. However, it is still necessary to be aware of these limitations and the problems they can cause for model fitting. 

Because of the incredible array of regression models that can be designed, it's very difficult to give a general set of guidelines that will ensure that any model will be identifiable and supported by your data. In addition, it's difficult to really explain model identifiability without talking about matrix algebra. However, we will provide some examples of general cases that we hope are illustrative. You can get the models presented below by running the following line: 

```{r, eval = FALSE}
bmmb::get_model ('8_badmodels.Rda')
```
```{r, include = FALSE}
load ('../models/8_badmodels.Rda')
```

### Collinearity

We've referenced the idea of *linear dependence* before (section X). We can provide a more formal definition of this now. Each of your predictors is a vector, a sequence of $i$ values that predict your dependent variable for observation $i$. A set of predictor vectors ($x_1, x_2, \dots, x_n$) is linearly dependent when there exists a vector of non-zero real numbers ($a_1, a_2, \dots, a_n$) that can be multiplied by our vectors such that the sum of the vectors equals zero for all elements of the vector. This is presented below: 

$$
\begin{equation}
\begin{split}
x_1 \times a_1 + x_2 \times a_2 + \dots + x_n \times a_n = 0
\end{split}
(\#eq:8-6)
\end{equation}
$$

When we introduced linear dependence we gave the example that you can't include height in meters and height in centimeters as predictors. Now we can say that it is because these are *linearly dependent*, and they can be combined to add up to zero. For example, $height_{cm} \times 100 + height_{meters} \times -1 = 0$. We only introduce the use of quantitative predictors in regression in the next chapter, but we can still show how inclusion of both of these predictors will ruin our model. First we make the new predictor, VTL in meters: 

```{r}
exp_data$vtl_m = exp_data$vtl / 100
```

And then we try to fit a model predicting using both VTL in centimeters and VTL in meters.

```{r, eval = FALSE}
model_bad_1 =  
  brms::brm (height ~ vtl_m + vtl, data = exp_data, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 50)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))
```

Our model will not be able to find a satisfactory solution, and you will see a large list of warnings when trying to fit it. If you check out the model print statement you will see strange things in the population-level effects:

```
Population-Level Effects: 
          Estimate Est.Error   l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept    45.69      2.37      41.17     50.47 1.00     4031     4425
vtl_m      1383.91 246800.93 -531293.80 343951.54 2.12        5       13
vtl          -5.28   2468.01   -3430.88   5321.56 2.12        5       13
```

Above we see comically small effective sample sizes (ESS) indicating that we have basically no information about our parameters. We also see an astronomically high value for the `vtl_m` predictor, especially given knowledge that human speaker height tends to range from 100 cm to 200 cm at most. So, we see that if you try to include two quantitative predictors that are linearly dependent, this will render your model useless. 

Linear dependence is binary, a set of vectors is linearly dependent or it is not. However, we can also talk about the degree to which vectors are linearly *related*, with linear dependence being one extreme. We measure linear relatedness using correlation coefficients (section X) where correlations of 1 or -1 indicating linear dependence. Our model above failed because our two predictors are linearly dependent, they have a correlation of 1. 

```{r}
cor (exp_data$vtl_m, exp_data$vtl)
```

In the code below we add a small amount of random error to our VTL (in meters) predictor. This is set to 1/10th the value of the variance of the predictor itself. This random noise brings down between VTL in meters and VTL in centimeters to 0.9946, almost perfectly correlated but *not* linearly dependent.

```{r}
set.seed(1)
exp_data$vtl_m_noise = exp_data$vtl_m + 
  rnorm (length(exp_data$vtl_m),0,sd(exp_data$vtl_m)/10)

cor (exp_data$vtl, exp_data$vtl_m_noise)
```

We can fit the model with the new predictor:

```{r, eval = FALSE}
model_bad_2 =  
  brms::brm (height ~ vtl_m_noise + vtl, data = exp_data, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 50)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))
```

If we inspect the population-level effects we see that this tiny tiny change had made our model substantially better. We get no warnings or error messages when we try to fit this model, and we have a very respectable ESS for our parameters. 

```
Population-Level Effects: 
            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept      45.75      2.36    41.16    50.35 1.00     4523     4087
vtl_m_noise  -201.86    175.11  -544.11   139.03 1.00     3504     3697
vtl            10.57      1.76     7.14    14.01 1.00     3439     3866
```

At this point our model might not seem that bad except for the large effect (and huge interval) around `vtl_m_noise` (VTL in meters, plus noise). The estimate for `vtl` in the interval around it may seem plausible, especially in comparison to our last model. However, we might consider the 'real' estimate for our `vtl` predictor in a model without this redundant parameter:

```{r, eval = FALSE}
model_good =  
  brms::brm (height ~ vtl, data = exp_data, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 50)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))
```

We see that we get a much narrower credible interval around the VTL parameter. 

```
Population-Level Effects: 
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept    45.72      2.38    41.04    50.40 1.00     5052     4679
vtl           8.55      0.18     8.21     8.90 1.00     5072     4831
```

The above is not to say that correlated quantitative predictors should never be included in the same regression models. However, it's important to think about the consequences of doing this. In addition, when possible, its a good idea to not paint yourself into a corner by designing an experiment that will specifically lead to, or require, highly correlated predictors.  

### Predictable values of categorical predictors

Linear dependence can cause other sorts of problems that are less transparent when making predictions using only categorical variables. Your model parameters will also not be identifiable if you include categorical predictors whose values can be predicted based on the value of other categorical predictors. For example, we know that we can't estimate both group means *and* the intercept in two group models. We can now say that this is because if you know that an observation is not in the first group, you know it must be in the second. Underlyingly, this is a problem of linear dependence.

For example, below we make a small matrix representing predictor variables in a simple regression comparing two groups. The first column represents the intercept for every observation (section X). The second column is a variable equal to 1 when the observation was in the first group and 0 if not. The third column is equal to 1 when the observation was in the second group and 0 if not. Obviously, the second column will equal 1 is the third column is zero and 0 if the third column is 1. 

```{r}
predictors = cbind (intercept=rep(1,4), x1=rep(c(1,0),2), x2=rep(c(0,1),2))
predictors
```

If we look at the numbers above, we can take the first column and add it to the negative of the second and third columns and this will always equal zero. 

```{r}
predictors[,1] + predictors[,2]*(-1) + predictors[,3]*(-1)
```

That means that these three parameters are linearly dependent, and that is why we can't include separate predictors for the intercept and each group in a two group model. This also explains why we can't estimate all four levels of our `C` (apparent category) parameter, as we see in the previous chapter. Below we make a small matrix of 'predictors' for each of our four categories and the intercept. The predictors with `C` in their name equal zero when the observation belongs to the first, second, third, and fourth groups. 

```{r}
predictors = cbind (intercept=rep(1,4), C1=c(1,0,0,0), C2=c(0,1,0,0),
                    C3=c(0,0,1,0),C4=c(0,0,0,1))
predictors
```

We can see that the same approach can be used to make all rows add up to zero, indicating that these predictors are linearly dependent. The predictors cannot be made to sum to zero if we omit the fifth column of the matrix above, which is why our models will not estimate `C4` when `C` is included as a predictor in our models. 

```{r}
predictors[,1] + predictors[,2]*(-1) + predictors[,3]*(-1) + 
  predictors[,4]*(-1) + predictors[,5]*(-1)
```

This sort of thing also causes problems when you include multiple different factors that mutually explain each other. For example, last chapter we initially fit a model using `C`, apparent speaker category, as a predictor. We then fit models using `A` and `G`, apparent age and gender as predictors. However, we did not fit models that included apparent age, gender, *and* speaker category. This is because if a speaker is a woman, the are an adult female. If they are a male child they are a boy, and if they are a female child they are a girl. So, given combinations of some categorical predictors, we can perfectly guess the values of *other* categorical predictors. Mathematically, this means that the 'secret' predictors representing each category (see section X) are linearly dependent. 

Below we make our small predictor matrix containing 

```{r}
predictors = cbind (intercept=rep(1,4), C1=c(1,0,0,0), C2=c(0,1,0,0),
                    C3=c(0,0,1,0),A1=c(0,0,1,1), G1=c(0,1,0,1),A1G1=c(1,0,0,1))
predictors
```

This matrix actually features a large number of combinations of predictors that can be combined to equal zero. For example we can combine the first, second, third and fifth predictors:

```{r}
predictors[,1]*1 + predictors[,2]*(-1) + predictors[,3]*(-1) + 
   predictors[,5]*(-1)
```

Or the first, third, fourth, and seventh:

```{r}
predictors[,1]*1 + predictors[,3]*(-1) + predictors[,4]*(-1) + 
  predictors[,7]*(-1)
```

We can that if we try to fit a model that includes all of these predictors: 

```{r, eval = FALSE}
# Fit the model yourself, or
model_bad_3 =  
  brms::brm (height ~ C + A*G, data = exp_data, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))
```

We get a model with the low ESS values, the large Rhat values, and the very large coefficient values and intervals. 

```
Population-Level Effects: 
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept   157.98      0.20   157.57   158.35 1.06       61       78
C1         1066.31   2604.72 -4431.79  5158.68 2.07        5       18
C2          496.91   1751.32 -2543.19  3708.83 1.87        6       13
C3           74.39   2086.76 -4113.31  2801.74 1.99        5       21
A1          793.68   1363.24 -2383.78  3019.96 1.90        6       12
G1          567.63   2171.69 -4107.90  3364.23 2.04        5       18
A1:G1       283.89   1112.60 -2295.30  2141.73 2.26        5       18
```

This concludes our discussion of linear dependence and the many ways it can cause problems for your models. If you try to fit a model and you get errors, divergent transitions, low ESS values and high Rhats, you *may* have a problem with linear dependencies in your predictors, and it is worth carefully considering this possibility. A better understanding of model identifiability and linear dependence can be gained by learning a little linear algebra. We recommend cite [@@ Noah].

### Saturated, and 'nearly-saturated', models

The models in this chapter sought to estimate five parameters for each listener ($Intercept, A1, G2, A1:G1, \sigma$) based on 139 observations for each listener. This seems like a reasonable ratio of observations to parameters. If we had only 10 observations per listener, this model would be identifiable. However, it might still not be such a good idea to fit a model with so many listener-dependent parameters given so little data. In general, before fitting a model that you *can* fit, it's a good idea to think whether you *should* fit it given the nature of your data and the number of observations you have overall (and in different conditions).

We can think of the logical extreme of a model with too many parameters and too little data. Imagine we wanted to fit a model with speaker effects, listener effects, *and* the interaction between the two. This model will be able to represent the 15 listener effects, 139 speaker effects, and all of the listener-dependent speaker effects. This means that at a minimum, this model would require 15 x 139 = 2085 parameters to represent these effects. However, we only have 2085 data points, one observation per speaker per listener. As a result, we would have a different parameter for every single observation leading to what's known as a **saturated model**. 

In a model without shrinkage, saturated models lead to the perfect prediction of data. This means that there is no data-level error ($\sigma$), or in fact, any random variation at all. This is problematic because the existence of a non-zero $\sigma$ is a pre-requisite for the calculation of the likelihood function of our regression parameters (see section X, chapter 2). For example, the model below doesn't report errors because it doesn't fit at all, it repeatedly crashed R and Stan when we attempted to fit it.

```{r, eval = FALSE}
exp_data$S = factor (exp_data$S)
exp_data$L = factor (exp_data$L)
# Fit the model yourself, or
model_bad_4  =  
  brms::brm (height ~ S*L, data = exp_data, chains = 4, cores = 4,
       warmup = 1000, iter = 3500, thin = 2,
       prior = c(brms::set_prior("normal(176, 15)", class = "Intercept"),
                 brms::set_prior("normal(0, 15)", class = "sigma")))

saveRDS(model_bad_4, "model_bad_4.Rmd")
```

This may be easier to conceive of if we use a much smaller example. Imagine two people run the coffee and reading times experiment we discussed in earlier chapters. Two speakers read a passage after drinking decaf, and another after drinking coffee. This results in four observations. We can estimate the speaker effect, this will be the average of the reading times for each speaker across both drink conditions. The error for this estimate will be the distribution of the coffee and decaf reading times around the speaker average. We can also estimate the effect for drink condition, the average of both speaker reading times within each condition. The error for this will be the distribution of speaker reading times around the mean for each drink condition. 
However, we cannot estimate the speaker by drink condition interaction. This is because once we know the mean reading time for each speaker for each drink condition, *there is no error*. This is because we only have one observation within this condition so that observation *is* the mean. There is no error distribution because we have no other observations. 

Ok, so what if we had not one but two observations per speaker per drink condition? This would not be a saturated model, but we could think of it as a *nearly-saturated* model, a term that may not be 'real' but gets the point across. A model with only two observations per person per drink condition may technically allow us to model the listener by drink condition interaction, however each of the interaction parameters would be based on two observations. 

Back to our example of estimation of the speaker effects. We have 139 observations per listener but only one per listener per speaker. Rather thank think of our $n$ (number of observations) as a single value, we can think of our effective $n$ for different parameters. If we plan to investigate the effect of apparent age on apparent height, we have >40 observations per listener per apparent age group (`table (exp_data$A, exp_data$L)`). We can think of our $n$ per listener for the estimation of the apparent age parameter as >40, a reasonable amount of data. On the other hand, our $n$ for the estimation of the listener by speaker interaction is only 1 (`table (exp_data$S, exp_data$L)`), regardless of whether we have 139 observations per listener. 

If we wanted to make claims about listener-dependent judgments of individual speakers and variation in these, we would need to design an experiment that featured a 'reasonable' number of observations per listener per speaker. Establishing a 'reasonable' number of repetitions depends on the amount of variation in the data, the type of variation in the data, and the number of groups in your data, among other things. In general, the noisier your data is the more observations you will need to get precise (and useful) estimates for your parameters.


## References 

Gelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19(10), 555.

Kruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.


## Exercises

The analyses in the main body of the text all involve only the unmodified 'actual' resonance level (in `exp_data`). Responses for the stimuli with the simulate 'big' resonance are reserved for exercises throughout. You can get the 'big' resonance in the `exp_ex` data frame, or all data in the `exp_data_all` data frame. 

Fit and interpret one or more of the suggested models:


1) Easy: 

2) Medium:

2) Hard: 


In any case, describe the model, present and explain the results, and include some figures.

