
\newpage

# Varying variances, more about priors, and prior predictive checks

This far our models have been a bit 'traditional' in one important way, they have featured a single error term ($\sigma$) for all conditions. This means that $\sigma$ is the same for boys, girls, men, and women, and for every listener in our experiment, whether this is true in our data or not. In addition, our models have assumed that each batch of our 'random effects' (i.e. predictors fit with partial pooling) has a single standard deviation for all estimated parameters, meaning that we expect all of these to be distributed about zero in the same way. For example, our models have typically included a predictor for speaker and the standard deviation $\sigma_S$. Such a model assumes that the parameters representing boys, girls, men, and women have the same standard deviation ($\sigma_S$). We might instead wonder if, for example, boys and girls tend to exhibit wider variation so that a model including four standard deviation parameters, one for each category $\sigma_{S==b}$,$\sigma_{S==g}$,$\sigma_{S==m}$,$\sigma_{S==w}$, might be preferable. In this chapter we're going to discuss models that allow for more variation in their $\sigma$ parameters. In addition, we're going to go into more detail about setting priors for our models and the use of prior predictive checks. 

## Data and Research questions

Below we load the data for our experiment investigating apparent speaker height, in addition to the `brms` and `bmmb` packages. 

```{r, message=FALSE, error=FALSE}
library (brms)
library (bmmb)
data (height_exp)
options (contrasts = c('contr.sum','contr.sum'))
height_exp = height_exp[height_exp$R=='a',]
```

We're going to focus on models whose structure is similar to the final model we fit in chapter 7 (`model_interaction`). However, in this chapter we are going to focus on questions related to variation in our parameters. We would like to know two things:

  Q1) Does our error standard deviation vary as a function of speaker category?
  
  Q2) Does our error standard deviation vary as a function of listener?

  Q3) Do our speaker random effects exhibit category-specific standard deviations? 
  

## More about priors 

To this point our focus has been on understanding the components that make up our models, and how these are represented using parameters by regression models. Now that we've covered most of the essentials of using categorical predictors, we can focus a bit more on the prior distributions of the parameters in our models. The reason we've been able to get away with not talking about priors very much is because we have substantial **domain knowledge** regarding the distribution of human height as a function of age and sex. In addition, our models thus far have been relatively simple making the consideration of prior distributions straightforward. For example, the $A$ parameter in the last model we considered last chapter (`model_interaction`) represented half the difference in apparent height between adults and children, averaged across females and males. Since we know that this difference is about X cm for *veridical* height (cite), it seems reasonable that we might expect that this value in *apparent* height might be in the vicinity of X cm. 

### Prior predictive checks

However, imagine a situation where you were less sure about reasonable values for your priors. This can happen for many reasons. For example, imagine you carry out a lexical decisions task where participants listen to a combination of 'real' (i.e. 'map') and 'fake' words (i.e. 'marp') and have to decide if the word they heard is 'real' or not. You divide your real words into 5 groups based on a numerical measure of their frequency (how often they ten to appear in speech). To complicate matters you see that your reactions times are *heavily* skewed and so decide to model the logarithm of reaction times. What should your priors be for your frequency groups? The prior distribution of a parameter represents the expected distribution of your parameters *a priori*. So what do we think are reasonable group differences in this situation? Even in this relatively simple case, setting prior distributions using only brain power would require having some intuitions regarding plausible variation in the *logarithm* of reaction times across word groups. 

A **prior predictive check** can help understand the consequences of the prior distributions of our parameters, especially in situations where they are otherwise difficult to understand. To carry out a prior predictive check using `brms` you can fit your in the exact same way you normally would, except for setting `sample_prior="only"`. When you do this, your model knows to sample only from the prior, generating parameter values and expected values ($\mu$) based only on prior expectations. The prior predictive check consists of generating fake data ($\tilde{y}$) based on the values expected values ($\mu$) resulting only from the prior. This is very similar to posterior predictive checks (discussed in section X) save for the fact that they are not influenced by the model likelihood. 

For example, imagine we fit `model_interaction` from chapter 7 again, except this time we sample only from the prior. Imagine that we did not have much knowledge about speaker heights other than that humans tend to be between 1 and 2 meters tall. Based on this we decided to be cautious and use relatively *uninformative* priors and set all prior standard deviations to 1000 cm (10 meters). 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 1000)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 1000)", class = "b"),
           brms::set_prior("student_t(3,0, 1000)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (1000)", class = "cor"), 
           brms::set_prior("student_t(3,0, 1000)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
prior_uninformative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = height_exp, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#  saveRDS (prior_uninformative,"model_uninformative.RDS")
```

```{r}
prior_uninformative = readRDS ("../models/8_prior_uninformative.RDS")
```


We can use the `predict` function to get the prior predictions made by our model. 

```{r, cache = TRUE, eval = FALSE}
pp_uninformative = predict (prior_uninformative, summary = FALSE)
```

When we get 'unsummarized' predictions we get one for each set of samples in our model. These predictions vary across samples by row and across data points by column. So, to get predictions for a single posterior sample we need to observe a single row of this matrix. This could be done using a single histogram, as below:

```{r, eval = FALSE}
hist (pp_uninformative[1,])
```

However, the `bmmb` package has a simple function called `p_check` that can be used to consider the density of multiple samples at once. By default this compares 10 random predictions, however the number of predictions can be changed, and the user may also specify specific samples to consider. 

```{r, eval = FALSE}
bmmb::p_check (pp_uninformative)
```

In the left plot of figure \@ref(fig:F81) we see the result of using `p_check` in ten evenly spaced samples from our prior prediction. As can be seen, our 'cautious' approach results in a rango of simulated heights that make no sense even given our relatively limited prior knowledge, in this case that most humans are between 100 and 200 cm tall. In fact, our simulated data contains not only large negative values, which make no sense in the context of height, but also substantial values about 5000 cm, which is about the size of a 10-story office building.  

```{r F81, fig.height = 3, fig.width=8, fig.cap = " -- ", echo = FALSE, cache = TRUE}

###############################################################################
### Figure 7.9
###############################################################################

prior_mildly_informative  = readRDS ("../models/8_prior_mildly_informative.RDS")
pp_mildly_informative = predict (prior_mildly_informative, summary = FALSE)

prior_conservative = readRDS ("../models/8_prior_conservative.RDS")
pp_informative = predict (prior_conservative, summary = FALSE)

par (mfrow = c(1,3), mar = c(4,4,1,1))
p_check (pp_uninformative, samples = seq(1,4000,length.out=10))
p_check (pp_mildly_informative, samples = seq(1,4000,length.out=10), xlim = c(-100,400))
p_check (pp_informative, samples = seq(1,4000,length.out=10), xlim = c(-100,400))
```

It is difficult to *prove* that such a wide prior is 'bad', and in fact some authors recommend the use of very uninformative priors (cite krushcke). However, it is difficult to argue that a prior that results in predictions that are more in line with our domain knowledge is generally better for at least two reasons (cite gelman prior). First, more informative priors can help the model converge on credible parameter values, especially for complicated models with many parameters. Second, more informative priors can help improve out-of-sample prediction by providing actual information about plausible parameter values for our model. Basically, no fancy statistical model is going to convince anyone that 50 meters tall is a plausible apparent height for human speakers because human speakers are simply not nearly that tall. As a result, a model that acts as if 50 meters is a plausible apparent height for human speakers is likely to offer worse out-of-sample prediction that a model that correctly assigns little to no prior belief to this range of apparent heights. Below we again sample from the prior using the setting we have been using so far, and that we used in the previous chapter. 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (12)", class = "cor"), 
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
prior_predictions_mildly_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = height_exp, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#  saveRDS (prior_predictions_mildly_informative,"prior_predictions_mildly_informative.RDS")
#  prior_predictions_mildly_informative = readRDS ("prior_predictions_mildly_informative.RDS")
```

We can make prior predictions again using the `predict` function, and plot these using the `p_check` function. The results of which are shown in middle plot of figure \@ref(fig:F81). 

```{r, cache = TRUE, eval = FALSE}
yhat_better = predict (model_interaction_prior_better, summary = FALSE)
```

The priors we have been using do allow for some implausibly large or small apparent height judgements. However, they are constraining the bulk of these to values between 100-200 cm, which is our approximate target. In our opinion, these priors are more than 'good enough', however, you may be unhappy that even some implausible values are being generated and want to test out even tighter priors. 

```{r, eval = FALSE, warnings=FALSE,error=FALSE}
priors = c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 6)", class = "b"),
           brms::set_prior("student_t(3,0, 6)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 6)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
prior_predictions_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = height_exp, chains = 1, cores = 1, warmup = 1000, 
             iter = 5000, thin = 1, prior = priors)

#  saveRDS (prior_predictions_informative,"prior_predictions_informative.RDS")
#  prior_predictions_informative = readRDS ("model_interaction_prior_better.RDS")
```

The prior predictions made by this model are presented in the right plot of figure \@ref(fig:F81). bla bla more detail. 

For the sake of comparison, we fit the three models specified above by erasing `sample_prior="only"` from the function call. We've named these models `model_uninformative`, `model_mildly_informative`, and `model_conservative` respectively.  Below we load the models:

```{r, include = FALSE}
model_uninformative = readRDS ("../models/8_model_uninformative.RDS")
model_mildly_informative = readRDS ("../models/8_model_mildly_informative.RDS")
model_conservative = readRDS ("../models/8_model_conservative.RDS")
```
```{r, eval = FALSE}
model_uninformative = bmmb::get_model ("8_model_uninformative.RDS")
model_mildly_informative = bmmb::get_model ("8_model_mildly_informative.RDS")
model_conservative = bmmb::get_model ("8_model_conservative.RDS")
```

And use the `brmplot` function to compare the fixed effects estimates for the three models. As can be seen, there is little to no difference in parameter estimates or the credible intervals around them as a function of our prior probabilities. This is in part due to the large amount of data we have, and that our model is relatively simple relative to the information contained in our data. 

```{r, eval = FALSE, include = FALSE}
priors = c(brms::set_prior("student_t(3,156, 1000)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 1000)", class = "b"),
           brms::set_prior("student_t(3,0, 1000)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 1000)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_uninformative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = height_exp, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_uninformative,"../models/8_model_uninformative.RDS")

priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_mildly_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = height_exp, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_mildly_informative,"../models/8_model_mildly_informative.RDS")

priors = c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 6)", class = "b"),
           brms::set_prior("student_t(3,0, 6)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 6)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), 
             data = height_exp, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors)

#saveRDS (model_informative,"../models/8_model_informative.RDS")
```


```{r F82, fig.height = 3, fig.width=8, fig.cap = " -- ", echo = FALSE}

###############################################################################
### Figure 8.2
###############################################################################

model_uninformative = readRDS ("../models/8_model_uninformative.RDS")
model_mildly_informative = readRDS ("../models/8_model_mildly_informative.RDS")
model_conservative = readRDS ("../models/8_model_conservative.RDS")

fixef_1 = fixef(model_uninformative)
fixef_2 = fixef(model_mildly_informative)
fixef_3 = fixef(model_conservative)

par (mfrow = c(1,4), mar = c(4,4,1,1))
brmplot (rbind(fixef_1[1,],fixef_2[1,],fixef_3[1,]), ylim = c(154.5,161.5))
brmplot (rbind(fixef_1[2,],fixef_2[2,],fixef_3[2,]), ylim = c(6.5,13.5))
brmplot (rbind(fixef_1[3,],fixef_2[3,],fixef_3[3,]), ylim = c(-6,1))
brmplot (rbind(fixef_1[4,],fixef_2[4,],fixef_3[4,]), ylim = c(-6,1))
```

### More specific priors 

The final model above was named `conservative` because its priors may have been *too* small. Based on the information in `height_data` (based on cite), we know that 11 year-old boys and girls are about 150 cm tall, and women and men are 163 and 256 cm tall respectively, on average. This means that there is an difference of 19.5 cm ($(13+26)/2$) on average between adults and children between 11 and 12 years of age. A difference of 19.5 cm between age groups suggests about a 10 cm distance (half the group difference) between each age group and the overall intercept. In other words, if apparent height is similar to veridical height, we expect an $A$ predictor of about 10 cm in magnitude given sum coding (this is about what we see in our models). In contrast, we have an expected height difference of 0 cm across genders for children, and 12 cm across genders for adults for an average difference of 6 cm between males and females. This means that we expect an effect of about 3 cm for gender (again, similar to what we see in our model). 

When we set our priors using the `class` parameter in the `prior` function, we lose the ability to fine-tune priors for specific parameters. So, for example, we might want to a prior with a standard deviation of 10 cm for the age effect, and a standard deviation of 3 cm for the gender effect. We can use the `prior_summary` function in `brms` to see what priors we can set for our model. The output of this function is discussed in detail in section X. 

```{r}
brms::prior_summary(model_mildly_informative)
```

We can copy the values of the `class`, `coeff`, and `group` values in the ouput above to set specific prior probabilities for our `A1` and `G1` parameters, and for the standard deviations of the `A1` and `G1` random effects.

```{r}
priors = c(brms::set_prior("student_t(3,156, 6)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 6)", class = "b"),
           brms::set_prior("student_t(3,0, 10)", class = "b", coef = "A1"),
           brms::set_prior("student_t(3,0, 3)", class = "b", coef = "G1"),
           brms::set_prior("student_t(3,0, 10)", class = "sd"),
           brms::set_prior("student_t(3,0, 3)", class = "sd", coef = "A1", group = "L"),
           brms::set_prior("student_t(3,0, 6)", class = "sd", coef = "G1", group = "L"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("student_t(3,0, 6)", class = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
prior_predictions_informative =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|S), sample_prior="only",
             data = height_exp, chains = 1, cores = 1, warmup = 1000, 
             iter = 5000, thin = 1, prior = priors)

#  saveRDS (prior_predictions_informative,"prior_predictions_informative.RDS")
#  prior_predictions_informative = readRDS ("model_interaction_prior_better.RDS")

```

If we inspect the priors for this model and compare this to the previous one, we can see that now there is between-parameter variation in our priors.

```{r}
brms::prior_summary(prior_predictions_informative)
```

## Heteroskedasticity and distributional (or mixture) models

Most 'typical' regression models assume that the error variance is the same for all observations, all conditions, all listeners, etc. The property of having a homogeneous variance is called **homoscedasticity**, and models that make this assumption can be said to be **homoscedastic**. Historically, homogeneity of variance has been assumed in models because it makes models simpler to understand, and not because it is 'true' for all (or any) data. With Bayesian models it is straightforward to relax and test this assumption, and to build models that exhibit **heteroscedasticity**, differences in the error variance across different conditions. Consider the residuals for the random effects model we just fit to our data, which we can get from the model using the `residuals` function:

```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_interaction = bmmb::get_model ('7_model_interaction.RDS')
```
```{r, include = FALSE}
model_interaction = readRDS ('../models/7_model_interaction.RDS')
```


```{r, cache = TRUE}
residuals_interaction = residuals (model_interaction)
```

If we make a boxplot of the residuals we see that the distribution of errors is not exactly equal for all listeners. For example, the interquartile range for listener 9 is almost as wide as the entire distribution of residuals for listener 12. The model we just fit at the end of chapter 7 (`model_interaction`) allowed us to estimate between-listener differences in the apparent age effect on apparent height. In this section we will fit a model with listener-specific error variances ($\sigma$) in addition to listener-specific parameters related to specific effects. By allowing the random error to vary as a function of listener, our model may be able to provide more-reliable information regarding our data, in addition to letting us ask questions like "is listener 12's error variance actually smaller than listener 9's, or does it just seem that way?" in a more formal manner. 

```{r F68, fig.height = 5, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}
################################################################################
### Figure 6.8
################################################################################

par (mar = c(3,3,1,1), oma = c(1,2,1,1))
layout (mat = matrix (c(1,3,1,3,2,3),2,3))
boxplot(residuals_interaction[,1] ~ model_interaction$data$L, cex.lab = 1.3,cex.axis = 1.2,
        col = cols, ylim = c(-30,30), outline=FALSE,xlab='',ylab='')
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$L, 
        col = cols, ylim = c(-30,30), outline=FALSE, add=TRUE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.2)

boxplot(residuals_interaction[,1] ~ model_interaction$data$A, 
        col = cols[2:3], ylim = c(-25,25), outline=FALSE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.3)
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$A, 
        col = cols[2:3], ylim = c(-25,25), outline=FALSE, add=TRUE,xlab='',ylab='',
        cex.lab = 1.3,cex.axis = 1.3)
#axis (side=1, at = 1:2, labels = c()

boxplot(residuals_interaction[,1] ~ model_interaction$data$A + model_interaction$data$L, 
        col = rep(cols, each = 2), ylim = c(-40,30), outline=FALSE,xlab='',
        xaxt='n',ylab='', cex.lab = 1.3,cex.axis = 1.3)
grid()
boxplot(residuals_interaction[,1] ~ model_interaction$data$A + model_interaction$data$L,ylab='', 
        col = rep(cols, each = 2), ylim = c(-40,30), outline=FALSE, add=TRUE,
        xlab='',xaxt='n', cex.lab = 1.3,cex.axis = 1.3)
axis (side = 1, at = seq(1.5,29.5,2), labels = 1:15,cex.axis = 1.3)

mtext (side=2, line = 0.3,outer = TRUE, "Residuals (in centimeters)")
```

Models that allow for heteroskedastic error are sometimes referred to as **mixture models** or **distributional models**. Mixture models refers to the fact that these models can be thought of resulting from a mixture of several different error distributions. One distribution provides the error in some cases and another in other cases, resulting in heteroskedastic error. Distributional models are those which seeks to model systematic variation in the error, just as more traditional models focus on systematic variation in means. In this section we will fit a distributional model to our data, we will seek to understand systematic variation in both the mean and standard deviation parameters in our model. 

Before continuing, we need to talk about whether our (or your) data can realistically support the estimation of such a model. To this point we haven't discussed this much, but as our models get more and more complicated it is something we need to think about. Our data has 139 observations for each listener, and seeks to estimate four predictors ($Intercept, A1, G2, A1:G1$) parameter, and error variance. So, we want to estimate five listener-dependent parameters using 139 observations, which is reasonable. However, if we had only 10 observations per listener perhaps it may not be such a good idea to fit a heteroskedastic model to our data, or to add many more listener-specific parameters. In general, before fitting a model that you *can* fit, it is a good idea to think whether you *should* fit it given the nature of your data and the number of observations you have overall, and in different conditions. 

## A 'simple' model: Error varies according to a single fixed effect

### Description of our model

Our heteroskedastic model is at the same time more complicated, but also more of the same. The `brms` package makes it exceptionally easy to fit models of this kind by letting you write formulas for the error just as you do for the mean. For example, the model formula for our previous model was:

`height ~ A*G + (A*G|L) + (1|S)`

This told `brms` "model apparent height as a function of an intercept, an effect for apparent age, an effect for apparent gender, the interaction of the two, listener-specific adjustments to all predictors, and speaker-specific intercepts". Implicitly, we know that all of this models variation in the $\nu$ parameter of a normal distribution with a fixed $\sigma$. We want our model to consider the possibility that the value of $\sigma$ for our data may be different based on whether the speaker is judgmed to be an adult or a child. We can do this by including a separate formula for our error, called `sigma` by `brms` using the following formula:

`sigma ~ A`

This formula tells `brms` "model the standard deviation of the error as varying around an overall intercept and deviations of this based on the `A`, a predictor in dicating apparent age". We can 'stick' our two model formulas together using the `bf` (Bayes formula) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ A)
```

Rather than estimate the `sigma` parameter directly, `brms` models `log(sigma)`, the logarithm of the error standard deviation (although it still calls it sigma). The problem with trying to model the standard deviation parameter directly is that it is bounded by zero. As a result, effects need to get smaller and smaller as standard deviations approach zero, otherwise they may push standard deviations into negative territory (which is impossible). One way to deal with this is to log transform standard deviation (or variance) parameters and model those instead. The log transform helps model standard deviations because, just like standard deviations, logarithms are bounded by zero. Logarithms map all numbers from 0 to 1 to values between negative infinity and 0. So, when you model the logarithm of `sigma`, an effect can *always* results in a smaller sigma, a value towards, but never quite at, negative infinity. 

Since we are modeling log-transformed sigmas, we now specify the priors for sigma in a log space. Previously, our prior for sigma was a t-distribution with a standard deviation of 12. This meant that the majority of the mass of the distribution was going to be between 0 and 24 cm. Now, we set the distribution to 1.5, however, these are logarithmic units. This means we expect the mass of the distribution to be between $exp(-3)=0.05$ and $exp(3)=20.1$. The full model specification is provided below: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\mu_{[i]},\sigma, \nu) \\ 
\mu_{[i]} = \mathrm{Intercept} + A_{[A_{[i]}]} + G_{[G_{[i]}]} + L_{[L_{[i]}]} + A \colon L_{[L_{[i]}]} + G \colon L_{[L_{[i]}]} + A \colon G \colon L_{[L_{[i]}]} + S_{[S_{[i]}]} \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + A1_{\sigma} \\ \\ 
\\ 
\mathrm{Priors:} \\ 
S_{[\bullet]} \sim t(3,0,\sigma_S) \\
\begin{bmatrix} L_{[\bullet]} \\ A \colon L_{[\bullet]} \\ G \colon L_{[\bullet]} \\ A \colon G \colon L_{[\bullet]} \end{bmatrix} \sim \mathrm{MVNormal} ( \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \Sigma) \\ \\
\\
\mathrm{Intercept} \sim t(3,156,12) \\
A, G, A \colon G \sim t(3,0,12) \\
\sigma_L, \sigma_{A \colon L}, \sigma_{G \colon L}, \sigma_{A \colon G \colon L}, \sigma_S \sim t(3,0,12) \\
\sigma \sim t(3,0,12) \\
\nu \sim gamma(2, 0.1) \\ 
R \sim \mathrm{LKJCorr} (2) \\ \\
\mathrm{Intercept_{\sigma}} \sim t(3, 0,1.5) \\
A1_\sigma \sim t(3, 0,1.5) \\
\end{split}
(\#eq:8aa)
\end{equation}
$$

This model is quite large, but it only contains five new (or modified) lines compared to model presented in \@ref(eq:61). These lines are presented in \@ref(eq:68). First, our model now uses a trial-specific error term for trial $i$ ($\sigma_{[i]}$), whereas it always used a constant standard deviation previously. Second, we are now modeling this trial-specific $\sigma$ parameter using an intercept specific to our sigma term ($Intercept_{\sigma}$), and a listener-dependent deviation from this ($L_{\sigma[L_{[i]}]}$). Third, we specify the prior for our listener-dependent sigma effects. Finally, in the fourth and fifth lines and specify the prior for our sigma intercept, and for the standard deviation of the listener-specific sigma terms. In a sense, the model expressed in \@ref(eq:68) is effectively the model in \@ref(eq:61), with the additional structure in \@ref(eq:68) added to it. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:68)
\end{equation}
$$

### Prior predictive checks

Recall that when we use the `set_priors` function we can use the `class` parameter to specify priors for whole classes of parameters at a time. The classes we have discussed so far are:  

-   `Intercept`: this is a unique class, only for intercepts.
-   `sd`: this is for our standard deviation parameters that relate to 'batches' of parameters, e.g. `sd(Intercept)` for `L` ($\sigma_{L}$).
-   `sigma`: the error term.
-   `cor`: priors for 'random effect' correlation terms. 

For the first time we need to specify priors for terms for $\sigma$ rather than $\mu$. We can do this by naming `sigma` when we specify priors using the `dpar` (distributional parameter) parameter in the `set_prior` function. As noted just above, you can see which priors need setting, and what parameters need to be specified for a model, by using the `get_prior` function and providing your formula, data, and family.

```{r, eval = FALSE}
brms::get_prior (brms::bf(height ~ A*G + (A*G|L) + (1|S),
                          sigma ~ A),
                 data = height_exp, family="student")
```

If you run the line above you will see that we have two parameters where `dpar=sigma`: An overall intercept (`class = "Intercept", dpar = "sigma"`), and a age term for sigma (`class = "b",coef="A1",dpar="sigma"`). We can set priors for these element using the lines:

```{r}
brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma")
brms::set_prior("student_t(3, 0, 1.5)", class = "b", dpar = "sigma")
```

Notice that we set these priors use the classes we have already discussed, `Intercept` and `sd` respectively. However, now we set `dpar` (distributional parameter) to `sigma` to tell `brm` that these priors are specifically for parameters related to the model error term, and not to variation in predicted values (i.e. $\nu$). Below we fit the model sampling only from our priors:

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ A)

priors = c(brms::set_prior("student_t(3, 156, 12)", class = "Intercept"),
          brms::set_prior("student_t(3, 0, 12)", class = "b"),
          brms::set_prior("student_t(3, 0, 12)", class = "sd"),
          brms::set_prior("gamma(2, 0.1)", class = "nu"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "b", dpar = "sigma"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_age_sigma = 
  brms::brm (model_formula, data = height_exp, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors, sample_prior = "only")

#  saveRDS (priors_sigma_informative,"../models/8_priors_sigma_informative.RDS")
#  priors_sigma_informative = readRDS ("../models/8_priors_sigma_informative.RDS")
```

Make our prior predictions:

```{r}
pp_sigma_informative = predict (model_age_sigma, summary = FALSE)
```

And investigate them 

```{r}
p_check (pp_sigma_informative, samples = seq(400,4000,400))
```


### Fitting the model

For the first time below, we specify both the model formula (using the `bf` function) and the priors (using the `set_prior` function) outside of the call to `brm`. We do this to preserve the legibility of the code in general. Our prior specification includes two new lines to specify the priors for our sigma intercept ($\mathrm{Intercept_{\sigma}}$) and the standard deviations of our sigma-related terms ($\sigma_{L_\sigma}$). 

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ A)

priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "b", dpar = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_A_het = 
  brms::brm (model_formula, data = height_exp, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_A_het = bmmb::get_model ('8_model_A_het.RDS')
```
```{r, include = FALSE}
# saveRDS (model_re_t_het, '../models/8_model_A_het.RDS')
model_A_het = readRDS ('../models/8_model_A_het.RDS')
```

### Interpreting the model

If we look at the model output:

```{r}
bmmb::short_summary (model_re_t_het)
```

We see a two new lines reflecting our new model parameters. The first new line is in the `Group-level effects` for `L`. This line indicates the standard deviation of the listener-specific sigma terms (`sd(sigma_Intercept)`), which we see is 0.34 (remember this is in a log scale). The second new line is in the `Population-level effects`, and it indicates the overall intercept for our sigma term (also on a log scale). If we exponentiate the value of `sigma_Intercept` ($exp(1.75)$), we get 5.X, the value of the sigma term in bla bla. 

Our listener-specific adjustments to the sigma intercept were fit with partial pooling, i.e. they are 'random' effects. As such, we can get these in the same two ways indicated for other random effects terms in section X. First, we can get these with the `ranef` function, which returns a matrix representing our listener random effects terms.

```{r}
str (ranef(model_re_t_het)$L)
```

After this, we can get just the sigma intercepts, the first five of which are presented below. 

```{r}
ranef(model_re_t_het)$L[1:5,,"sigma_Intercept"]
```

Alternatively we can use the `hypothesis` function to get the listener random effects themselves, or the sum of these with the sigma intercept. 

```{r}
sigmas_centered = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

We compare these in the plot below,  exponentiating the recreated sigma values to get the listener-specific standard deviation terms. 

```{r F69, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.9
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
brmplot (sigmas_centered, col = cols)
brmplot (exp(sigmas), col = cols, ylim = c(1,13))
#abline (h = 7.71)
```

## A 'complex' model: Error varies according to fixed and random effects

### Description of our model

Our heteroskedastic model is at the same time more complicated, but also more of the same. The `brms` package makes it exceptionally easy to fit models of this kind by letting you write formulas for the error just as you do for the mean. For example, the model formula for our previous model was:

`height ~ A*G + (A*G|L) + (1|S)`

This told `brms` "model apparent height as a function of an intercept, an effect for apparent age, an effect for apparent gender, the interaction of the two, listener-specific adjustments to all predictors, and speaker-specific intercepts". Implicitly, we know that all of this models variation in the $\nu$ parameter of a normal distribution with a fixed $\sigma$. We want our model to fit listener-specific error terms ($\sigma_L$), and we want to estimate these with partial pooling since it involves estimating 15 different parameters. We can do this by including a separate formula for our error, called `sigma` by `brms` using the following formula:

`sigma ~ 1 + (1|L)`

This formula tells `brms` "model the standard deviation of the error as varying around an overall intercept and listener-specific deviations (estimated with partial pooling)". We can 'stick' our two model formulas together using the `bf` (Bayes formula) function as shown below:

```{r, eval = FALSE}
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))
```

Rather than estimate the `sigma` parameter directly, `brms` effectively models `log-sigma`, the logarithm of the error standard deviation (although hit still calls it sigma). The problem with trying to model the standard deviation parameter directly is that it is bounded by zero. As a result, effects need to get smaller and smaller as standard deviations approach zero, otherwise they may push standard deviations into negative territory (which is impossible). One way to deal with this is to log transform standard deviation (or variance) parameters and model those instead. The log transform helps model standard deviations because, just like standard deviations, logarithms are bounded by zero. Logarithms map all numbers from 0 to 1 to values between negative infinity and 0. So, when you model the logarithm of `sigma`, an effect can *always* results in a smaller sigma, a value towards, but never quite at, negative infinity. 

Since we are modeling log-transformed sigmas, we now specify the priors for sigma in a log space. Previously, our prior for sigma was a t-distribution with a standard deviation of 12. This meant that the majority of the mass of the distribution was going to be between 0 and 24 cm. Now, we set the distribution to 1.5, however, these are logarithmic units. This means we expect the mass of the distribution to be between $exp(-3)=0.05$ and $exp(3)=20.1$. We can use a prior with the same ranges for the standard deviation of our listener-dependent error adjustments ($\sigma_{L_\sigma}$). The full model specification is provided below: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\mu_{[i]} = \mathrm{Intercept} + A1_{[A_{[i]}}  + L_{[L_{[i]}]} + A1 \colon L_{[A_{[i]}]} + S_{[S_{[i]}]} \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\ 
\mathrm{Priors:} \\ 
L_{[\bullet]} \sim N(0,\sigma_L) \\
A1 \colon L_{[\bullet]} \sim N(0,\sigma_{A1 \colon L}) \\
S_{[\bullet]} \sim N(0,\sigma_S) \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\
\\
\mathrm{Intercept} \sim N(156,12) \\
A1 \sim N(0,12) \\
\sigma \sim N(0,12) \\
\sigma_L \sim N(0,12) \\
\sigma_{A1 \colon L} \sim N(0,12) \\
\sigma_S \sim N(0,12) \\ \\ 
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\
\end{split}
(\#eq:67)
\end{equation}
$$

This model is quite large, but it only contains five new (or modified) lines compared to model presented in \@ref(eq:61). These lines are presented in \@ref(eq:68). First, our model now uses a trial-specific error term for trial $i$ ($\sigma_{[i]}$), whereas it always used a constant standard deviation previously. Second, we are now modeling this trial-specific $\sigma$ parameter using an intercept specific to our sigma term ($Intercept_{\sigma}$), and a listener-dependent deviation from this ($L_{\sigma[L_{[i]}]}$). Third, we specify the prior for our listener-dependent sigma effects. Finally, in the fourth and fifth lines and specify the prior for our sigma intercept, and for the standard deviation of the listener-specific sigma terms. In a sense, the model expressed in \@ref(eq:68) is effectively the model in \@ref(eq:61), with the additional structure in \@ref(eq:68) added to it. 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{[i]}) \\ 
\sigma_{[i]} = \mathrm{Intercept_{\sigma}} + L_{\sigma[L_{[i]}]} \\ \\
L_{\sigma[\bullet]} \sim N(0,\sigma_{L_{\sigma}}) \\ \\
\mathrm{Intercept_{\sigma}} \sim N(0,1.5) \\
\sigma_{L_\sigma} \sim N(0,1.5) \\

\end{split}
(\#eq:68)
\end{equation}
$$

### Prior predictive checks

Recall that when we use the `set_priors` function we can use the `class` parameter to specify priors for whole classes of parameters at a time. The classes we have discussed so far are:  

-   `Intercept`: this is a unique class, only for intercepts.
-   `sd`: this is for our standard deviation parameters that relate to 'batches' of parameters, e.g. `sd(Intercept)` for `L` ($\sigma_{L}$).
-   `sigma`: the error term.
-   `cor`: priors for 'random effect' correlation terms. 

for the first time we need to specify priors for terms for $\sigma$ rather than $\mu$. We can do this by naming `sigma` when we specify priors using the `dpar` (distributional parameter) parameter in the `set_prior` function. As noted just above, you can see which priors need setting, and what parameters need to be specified for a model, by using the `get_prior` function.

```{r, eval = FALSE}
brms::get_prior (brms::bf(height ~ A*G + (A*G|L) + (1|S),
                          sigma ~ 1 + (1|L)),
                 data = height_exp, family="student")
```

If you run the line above you will see that we have two parameters where `dpar=sigma`: An overall intercept (`class = "Intercept", dpar = "sigma"`), and a standard deviation for listener-specific intercepts (`class = "Intercept",group = "L", dpar = "sigma"`). We can set priors for these element using the lines:

```{r}
brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma")
brms::set_prior("student_t(3, 0, 1.5)", class = "sd", dpar = "sigma")
```

Notice that we set these priors use the classes we have already discussed, `Intercept` and `sd` respectively. However, now we set `dpar` (distributional parameter) to `sigma` to tell `brm` that these priors are specifically for parameters related to the model error term, and not to variation in predicted values (i.e. $\nu$). Below we fit the model sampling only from our priors:

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A*G + (A*G|L) + (1|S),
                         sigma ~ 1 + (A|L))

priors = c(brms::set_prior("student_t(3, 156, 12)", class = "Intercept"),
          brms::set_prior("student_t(3, 0, 12)", class = "b"),
          brms::set_prior("student_t(3, 0, 12)", class = "sd"),
          brms::set_prior("gamma(2, 0.1)", class = "nu"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("student_t(3, 0, 1.5)", class = "sd", dpar = "sigma"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
priors_sigma_informative = 
  brms::brm (model_formula, data = height_exp, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors, sample_prior = "only")

#  saveRDS (priors_sigma_informative,"../models/8_priors_sigma_informative.RDS")
#  priors_sigma_informative = readRDS ("../models/8_priors_sigma_informative.RDS")
```

Make our prior predictions:

```{r}
pp_sigma_informative = predict (priors_sigma_informative, summary = FALSE)
```

And investigate them 

```{r}
p_check (pp_sigma_informative, samples = seq(400,4000,400))
```


### Fitting the model

For the first time below, we specify both the model formula (using the `bf` function) and the priors (using the `set_prior` function) outside of the call to `brm`. We do this to preserve the legibility of the code in general. Our prior specification includes two new lines to specify the priors for our sigma intercept ($\mathrm{Intercept_{\sigma}}$) and the standard deviations of our sigma-related terms ($\sigma_{L_\sigma}$). 

```{r, eval = FALSE}
# Fit the model yourself
model_formula = brms::bf(height ~ A + (A|L) + (1|S),
                         sigma ~ 1 + (1|L))


priors = c(brms::set_prior("normal(156, 12)", class = "Intercept"),
          brms::set_prior("normal(0, 12)", class = "b"),
          brms::set_prior("normal(0, 12)", class = "sd"),
          brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
          brms::set_prior("normal(0, 1.5)", class = "Intercept", dpar = "sigma"),
          brms::set_prior("normal(0, 1.5)", class = "sd", dpar = "sigma"))

set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_re_t_het = 
  brms::brm (model_formula, data = height, chains = 4, cores = 4,
             warmup = 1000, iter = 3500, thin = 2, family="student",
             prior = priors)
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_re_t_het = bmmb::get_model ('6_model_re_t_het.RDS')
```
```{r, include = FALSE}
model_re_t_het = readRDS ('../models/6_model_re_t_het.RDS')
```

### Interpreting the model

If we look at the model output:

```{r}
bmmb::short_summary (model_re_t_het)
```

We see a two new lines reflecting our new model parameters. The first new line is in the `Group-level effects` for `L`. This line indicates the standard deviation of the listener-specific sigma terms (`sd(sigma_Intercept)`), which we see is 0.34 (remember this is in a log scale). The second new line is in the `Population-level effects`, and it indicates the overall intercept for our sigma term (also on a log scale). If we exponentiate the value of `sigma_Intercept` ($exp(1.75)$), we get 5.X, the value of the sigma term in bla bla. 

Our listener-specific adjustments to the sigma intercept were fit with partial pooling, i.e. they are 'random' effects. As such, we can get these in the same two ways indicated for other random effects terms in section X. First, we can get these with the `ranef` function, which returns a matrix representing our listener random effects terms.

```{r}
str (ranef(model_re_t_het)$L)
```

After this, we can get just the sigma intercepts, the first five of which are presented below. 

```{r}
ranef(model_re_t_het)$L[1:5,,"sigma_Intercept"]
```

Alternatively we can use the `hypothesis` function to get the listener random effects themselves, or the sum of these with the sigma intercept. 

```{r}
sigmas_centered = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                                   scope = "ranef",group="L")
sigmas = short_hypothesis(model_re_t_het, "sigma_Intercept = 0", 
                          scope = "coef",group="L")
```

We compare these in the plot below,  exponentiating the recreated sigma values to get the listener-specific standard deviation terms. 

```{r F69, fig.height = 3, fig.width = 8, fig.cap="--", cache = TRUE, echo = FALSE}

################################################################################
### Figure 6.9
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
brmplot (sigmas_centered, col = cols)
brmplot (exp(sigmas), col = cols, ylim = c(1,13))
#abline (h = 7.71)
```


## Variation in 'random effect' variances


```{r}

rf = ranef(model_interaction)

pa = tapply (height_exp$A=='a', height_exp$S,mean)
pm = tapply (height_exp$G=='m', height_exp$S,mean)
vtl = tapply (height_exp$vtl, height_exp$S,mean)
brmplot (rf$S[,,1])

plot (pa[1:50], rf$S[1:50,1,1])
plot (pm[1:50], rf$S[1:50,1,1])
plot (vtl[1:50], rf$S[1:50,1,1])

```


### Description of our model


### Fitting the model

```{r}
brms::get_prior (height ~ A + G + A:G + (A + G + A:G|L) + (1|gr(S, by=C_v)), 
                     data = height_exp, family = "student")
```


```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,156, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))
model_interaction =  
  brms::brm (height ~ A + G + A:G + (A + G + A:G|L) + (1|gr(S, by=C_v)), data = height_exp, chains = 4, cores = 4,
       warmup = 1000, iter = 5000, thin = 4, prior = priors, family = "student")

#saveRDS (model_interaction,"model_het_group.RDS")
# model_interaction = readRDS ("model_het_group.RDS")

```




### Interpreting the model






## Answering our research questions









