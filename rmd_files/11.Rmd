\newpage
```{r, include = FALSE}
knitr::opts_chunk$set(
  dpi = 300, dev = "jpeg", collapse=TRUE
)
#options(knitr.duplicate.label = "allow")
```
# Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA

In this chapter we introduce models with multiple quantitative predictors, and interactions between them. After that, we will have covered the modeling concepts necessary to fit most models of the sort used for experimental data. The models you fit to you own data will include some combination of the elements covered in the previous chapters, and can potentially result in large models with hundreds of parameters. Traditionally, models with many predictors have had three general problems:

  1) A model may return spurious values for the 'extra' predictors, leading to incorrect conclusions.
  2) The model may not fit/converge, meaning it can't find 'good' estimates for the model coefficients. 
  3) It can be difficult to interpret a model with *hundreds* of parameters.

In this chapter we're going to cover a Bayesian approach to working with large models. We're going to discuss how working with multilevel Bayesian models can naturally help us with problems (1) and (2) above, and we're going to discuss an easy way to approach problem (3) as well. 
  Before continuing, we should note that designs with many quantitative predictors, factors, and interactions between these can result in very complicated models, which then have to be interpreted. However, the researcher has a big role to play in the complexity of the eventual analysis that they are faced with. Once when Santiago was buying a backpack for traveling, he was looking for the biggest backpack possible. One of the reviews said "1/5 stars, it was way too heavy when I filled it all the way up with my stuff". However, if we fill a backpack up with many heavy things, it doesn't seem fair to blame the poor backpack when it becomes difficult to carry. In the same way, if you are faced with a complex model that you then need to interpret, you shouldn't blame the model, `brms`, or us for your predicament. In order to avoid a situation where you end up with data you can't analyze or a model you can't interpret, it's worth considering the following questions before advancing to the data collection stage of your experiment: 

  * How will I analyze the data? Will I be able to? 
  * What will the model structure be? 
  * What kind of results am I expecting? How will this be reflected by the model parameters?
  * How would different results manifest in my regression model?
  
## Models with multiple quantitative predictors

Our regression models so far have only involved a single quantitative predictor. This means that the relationship between the dependent variable and our predictor formed surfaces with a single dimension: Lines. In the left and middle plots in figure \@ref(fig:F11-1) we see the linear relationships between speaker vocal-tract length (VTL) and f0 (voice pitch) with apparent speaker height. In each case, the expected value for the y-axis variable is the value of the line at each x-axis location. The residual, for each observation is the vertical distance between the line the observation.   
  Rather than think of the effect of VTL and f0 on apparent height individually, we can think about apparent height, VTL, and f0 together at the same time. Every point in figure \@ref(fig:F11-1) has a specific value of f0, VTL, and apparent height. These three quantitative variables can be thought of as defining a single location in a 3-dimensional space. For example, consider any given location in the room you are sitting in right now. Any such location can be specified using three variables that determine its location along the width, depth, and height of the room. Imagine you had a clear plastic cube containing points arranged as in \@ref(fig:F11-1) inside it, where each dimension along the cube represented one of the dimensions in the figure. Looking 'through' the cube at different orientations would result in arrangements just like the plots below. The first plot show the view through the VTL side, and the second plot shows the view down the f0 side, a 1/4 rotation of the cube. The final plot shows the view down through the top of the cube.  
  
```{r F11-1, fig.height = 3, fig.width = 8, fig.cap='(left) Average apparent height reported for each speaker plotted against speaker VTL. Point size reflects average apparent height. (middle) Same as the left plot but comparing apparent height to f0. (right) A comparison of VTL and f0 for each speaker.', echo = FALSE, cache = TRUE}

###############################################################################
### Figure 11-1
###############################################################################

agg_data = aggregate (height~f0+vtl+C_v, data = bmmb::exp_data, FUN=mean)
agg_data = agg_data[nrow(agg_data):1,]

ptcex = agg_data[,4] - min(agg_data[,4])
ptcex = 1 + (ptcex / max(ptcex))*1.5

par (mfrow = c(1,3), mar = c(4,4,1,1))
plot (agg_data[,c(2,4)],pch=16,col = bmmb::cols[2:5][factor(agg_data[,3])], 
      cex=ptcex)
abline (lm(agg_data[,c(4)]~agg_data[,c(2)]),lwd=2,lty=2)
grid()
plot (agg_data[,c(1,4)],pch=16,col = bmmb::cols[2:5][factor(agg_data[,3])], cex=ptcex)
abline (lm(agg_data[,c(4)]~agg_data[,c(1)]),lwd=2,lty=2)
grid()

plot (agg_data[,c(1,2)],pch=16,col = bmmb::cols[2:5][factor(agg_data[,3])], 
      cex=ptcex)
grid()

```

It may be easier to see what we mean by considering figure \@ref(fig:F11-2), which attempts to present our point in three dimensions. The left plot of figure \@ref(fig:F11-2) corresponds to the left plot in figure \@ref(fig:F11-1), while the right plot of figure \@ref(fig:F11-2) corresponds to the middle plot in figure \@ref(fig:F11-1). When we have two quantitative predictors, our models predict values along **planes** rather than lines. If we want to predict apparent height based on speaker f0 and VTL, that is basically asking: Can we predict the height of a point in our 3-dimensional space based on its x and y-axis location? 
  For example, imagine the points in figure \@ref(fig:F11-2) were floating in the room with you, arranged just as in the figure. You are given a large flat board (a *plane*) and asked to find the 'best' orientation for the board. When we fit lines, we preferred those that tended to minimize the residuals, the y-axis distance of the points to the line. The same principle holds when we fit models with two (or more) quantitative predictors. In general, planes are 'better' when they minimize the distance from each point to the surface of the plane along the axis representing the dependent variable. Though it's a bit more complicated than this for multilevel models, this general principle (i.e. the minimization of the error) still applies to our models.

```{r F11-2, fig.height = 5, fig.width = 8, fig.cap='(left) A three-dimensional plot of the variables presented in the figure above. (right) The same as the left plot but the cube has been rotated 90 debrees counter-clockwise.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.2
################################################################################
agg_data = aggregate (height~f0+vtl+C_v, data = bmmb::exp_data, FUN=mean)

par (mfrow = c(1,2), mar = c(0,1,0,2), oma = c(0,0,0,0))

tmp = agg_data[,c(2,1,4)]
s3d = scatterplot3d::scatterplot3d (tmp, color = bmmb::cols[2:5][factor(agg_data[,3])],
                     pch=16, angle = 55,type = "p",ylim=c(90,310), cex.symbols=rev(ptcex)*.9)
my.lm <- lm(height ~ f0+vtl, data = agg_data)

s3d = scatterplot3d::scatterplot3d (agg_data[,-3], color = bmmb::cols[2:5][factor(agg_data[,3])],
                     pch=16, angle = 55,type = "p",xlim=c(90,310), cex.symbols=rev(ptcex)*.9)
my.lm <- lm(height ~ f0+vtl, data = agg_data)
#s3d$plane3d(my.lm, col = 2)
```

In \@ref(eq:11-1) we see that that the height of the plane along the $z$ axis is determined by an intercept ($a$) plus the product of the x-axis coordinate and its slope ($b$) and the product of the y-axis coordinate and its slope. When we fit a model that includes two quantitative predictors, the model represents the planes it estimates using their $a$, $b$, and $c$ parameters. 
  
$$
\begin{equation}
z =  \mathrm{a} + \mathrm{b} \times x + \mathrm{c} \times y 
(\#eq:11-1)
\end{equation}
$$

Since the plane has two dimensions it has two slopes: A field can be uphill/downhill away from you, but also be up/downhill left to right. The slope coefficients for each quantitative predictor change the slope of the plane independently for each dimension. In fact, the slope of each predictor reflects the expected change according to that predictor when all other predictors are *held constant*. The intercept will slide the planes up/down without changing their slopes along either dimension. 
  Our discussion above has been entirely about planes, however, your model can include any number of quantitative predictors. The interpretation of these models is a simple continuation of the expansion of models specifying lines to those specifying planes. If your model has $n$ quantitative predictors, your data specifies points in an $n+1$ dimensional space, where dimension $n+1$ represents the dependent variable. Residuals in such models would represent the difference between the surface of the $n$ dimensional shape specified by the predictors and the position of each point along the $n+1$ dimension. 
  
### Interactions between quantitative predictors

Models with two quantitative predictors model variation in the dependent variable along planes. The 'interaction' between quantitative variables in regression models is represented by the **cross-product**, the multiple, of the two variables. We can add this term to equation representing our model with two quantitative predictors as in the equation below. It's common to refer to this term as the 'interaction' between our quantitative predictors, and we will adopt this convention, just keep in mind that we are actually referring to the cross product of the two variables.  

$$
\begin{equation}
z =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{c} \times y) + (\mathrm{d} \times x \times y)
(\#eq:F11-2)
\end{equation}
$$

In general, interaction terms are parameters that allow for the effect of a predictor to vary according to the value of some other predictor. When it comes to quantitative predictors, an interaction means that the slope of each predictor continuously increase/decreases as a function of the value of the *other* predictor. Consider the equation below which omits the terms for our $y$ variable for the sake of simplicity. First, let's factor out the $x$ form the second and third terms on the right. When we do this, we can see that the slope for the $x$ dimension, $b$, will vary as a function of the value of $y$.

$$
\begin{equation}
\begin{split}
z =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{d} \times x \times y) \\
z =  \mathrm{a} + (\mathrm{b} + \mathrm{d} \times y) \times x \\
\end{split}
(\#eq:F11-3)
\end{equation}
$$

Imagine a case where the slope ($b$) along the $x$ dimension is fixed at 2 and the interaction term ($d$) is equal to zero. When this happens the slope of $x$ is constant and does not change based on the value of $y$. This is a case where the is no interaction between $x$ and $y$. 

$$
\begin{equation}
\begin{split}
b = 2, \; d = 0 \\
z =  \mathrm{a} + (2 + 0 \times y) \times x \\
z =  \mathrm{a} + 2  \times x \\
\end{split}
(\#eq:F11-4)
\end{equation}
$$

When the interaction term has some non-zero value (e.g. 1), then we *will* see the slope along $x$ change as a function of the value of $y$. Below we see that as $y$ increases from 1 to 5, the effective slope along the $x$ axis increases in value.   

$$
\begin{equation}
\begin{split}
d = 1 , \; y = 1   \\
z =  \mathrm{a} + (2 + 1 \times 1) \times x \\
z =  \mathrm{a} + 3 \times x \\ \\ 
d = 1 , \; y = 5   \\
z =  \mathrm{a} + (2 + 1 \times 5) \times x \\
z =  \mathrm{a} + 7 \times x \\
\end{split}
(\#eq:F11-5)
\end{equation}
$$

This reasoning also holds for the $y$ dimension and also means that negative interactions result in a *decrease* in slopes as the value of variables increases. This is obviously not flat and not a plane anymore, but rather a **hyperbolic parabaloid**. Sometimes these sorts of surfaces are referred to as **saddle surfaces** because they contain what's known as a **saddle point**, a point where the slopes along the x and y dimension are both equal to zero. Informally, a saddle point can be thought of as the flat spot in the middle of a shape that curves along two dimensions in a manner resembling a horse saddle. We're often going to refer to hyperbolic parabaloids as *saddles* because their full name is a bit of a mouthful, and because we are mainly interested in contrasting these with planes. However, it's important to keep in mind that hyperbolic parabaloids are one among many types of saddle surface and not the *only* type of saddle surface. 
  The function for a hyperbolic parabaloid can be seen below, where the height of the surface is equal to the product of some parameter $d$ and its $x$ and $y$ coordinates. 

$$
\begin{equation}
z = d \times x \times y 
(\#eq:F11-6)
\end{equation}
$$

So, the interaction of quantitative predictors results in a saddle shape as defined in \@ref(eq:F11-6). When our models include effects for $x$, $y$, *and* their interactions, we are effectively modeling a surface that combined a plane and a saddle shape, as in \@ref(eq:F11-2). In figure \@ref(fig:F11-3) we can see a comparison of a plane and two saddle shapes. In the middle row on the left we can imagine that if we were walking 'into' the plot the ground would first be sloping down left to right. However, as we proceeded into the plot the ground would gradually change so that it it sloping up from left to right further into the figure. 

```{r F11-3, fig.height = 6, fig.width = 8, fig.cap='(top row) Three perspectives of the same plane. (middle row) Three perspectives of the same saddle shape. (bottom row) Three perspectives of the sum of the plane and the saddle shape in the top two rows.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.3
################################################################################

f1 = function (a,b,c) a * 1 + b *1  
f2 = function (a,b,c) a*b*2
f3 = function (a,b,c) a * 1 + b *1 + a*b*2

x <- y <- seq(-1, 1, length= 12)

par (mfrow = c(3,3), mar = c(1,1,0,0), oma = c(0,0,0,0))

z <- outer(x, y, f1)
persp(x, y, z, col = bmmb::cols[2],theta = 0,zlim = c(-3,3))
persp(x, y, z, col = bmmb::cols[2],theta = -90,zlim = c(-3,3))
persp(x, y, z, col = bmmb::cols[2],theta = 45,zlim = c(-3,3))

z <- outer(x, y, f2)
persp(x, y, z, col = bmmb::cols[3],theta = 0,zlim = c(-3,3))
persp(x, y, z, col = bmmb::cols[3],theta = -90,zlim = c(-3,3))
persp(x, y, z, col = bmmb::cols[3],theta = 45,zlim = c(-3,3))

z <- outer(x, y, f3)
persp(x, y, z, col = bmmb::cols[4],theta = 0,zlim = c(-3,5))
persp(x, y, z, col = bmmb::cols[4],theta = -90,zlim = c(-3,5))
persp(x, y, z, col = bmmb::cols[4],theta = 45,zlim = c(-3,5))

```

We can combine a plane and a saddle, as in the bottom row of figure \@ref(fig:F11-3). The general formula for such a shape is presented below, and we've placed the terms in parentheses just to make it easier to interpret the equation. 

$$
\begin{equation}
z =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{c} \times y) + (\mathrm{d} \times x \times y)
(\#eq:F11-7)
\end{equation}
$$

When you include interactions between quantitative predictors in your regression models, it is very important to center these predictors, if only for use in the cross product term. For example, consider below where we generate two samples of 100 random Gaussian variables (`x1, x2`) with a mean of 500 and a standard deviation of 1. These variables are almost totally uncorrelated and yet their cross product (`x1x2`) is strongly correlated with both of them. This is because when variables are far from zero, the cross product between two variables will always be greater when one of the variables is greater. 

```{r}
set.seed(1)
x1 = rnorm (100,500,1)
x2 = rnorm (100,500,1)
x1x2 = x1*x2

cor (cbind(x1,x2,x1x2))
```

On the other hand, consider a situation where `x1` and `x2` were centered (`x1_c, x2_x`). Now, their cross product (`x1x2_c`) will not always be greater when either variable is greater, it will only be large when both variables have values above their mean and may be negative when even one does not. The result, as can be seen below, is that centering can dramatically reduce the correlation between quantitative predictors and the interaction between them. Given this and that lack of a compelling reason to not center in most cases, you should consider centering quantitative predictors routine whenever you plan to include interactions between these in your models. 

```{r}
x1_c = x1 - mean (x1)
x2_c = x2 - mean (x2)
x1x2_c = x1_c*x2_c

cor (cbind(x1_c,x2_c,x1x2_c))
```

## Data and research questions

The models we fit in chapter 9 and 10 were missing an obvious and important predictor: The fundamental frequency of the speaker's voice (f0). The fundamental frequency of a sound is the main acoustic correlate of perceived pitch. We know from previous studies that, in general, speakers with lower speaking f0s tend to be identified as taller (see chapter X for a discussion of this). In this section, we're going to fit a model that tries to predict apparent height from VTL and f0, and the interaction of the two. 
  Below we load our packages and data, and center our quantitative predictors. We also scale f0 so that it has a reasonably similar magnitude as the VTL effect. Specifically, the average difference in VTL between adult females and males is about 2 cm, and the difference in f0 is about 100 Hz. This means that is we divide f0 by 100 the difference between men and women is about 1 unit, comparable to the 2 unit difference in VTL. 
  
```{r, warning=FALSE, message=FALSE}
library (brms)
library (bmmb)
options (contrasts = c('contr.sum','contr.sum'))

data (exp_data)

exp_data$vtl_original = exp_data$vtl
exp_data$vtl = exp_data$vtl - mean (exp_data$vtl)

exp_data$f0_original = exp_data$f0 
exp_data$f0 = exp_data$f0 - mean(exp_data$f0)
exp_data$f0 = exp_data$f0 / 100
```

We're not going to have well-defined research questions this time. Instead, we have a meta research question that we will try to deal with:

Q1) What do we do with all these parameters? How do we know what to focus on and where to begin?

### Description of the model

We're going to begin with a model that includes all possible interactions between our predictors, and also includes listener-dependent versions of all of our 'fixed' effects predictors. Models of this sort are sometimes referred to as **maximal** models because they include all predictors, interactions, and 'random effects' supported by the data. Our model formula is:

`height ~ vtl*f0*A*G + (vtl*f0*A*G|L) + (1|S)`

We're going to use the same priors we used for our regression models in chapter 9. We won't present the full model description as this is way too long at this point. However, we can talk about all of our model coefficients and what they mean for our model. Below is an 'expanded' version of our model that spells out all of the included parameters.

```
height ~ Intercept + vtl + f0 + A + G + A:G1 + vtl:f0 + vtl:A + vtl:G1 + f0:A + 
         f0:G1 + vtl:f0:A + vtl:f0:G1 + vtl:A:G1 + f0:A:G1 + vtl:f0:A:G1
```

We can group these parameters together based on the way they affect the surfaces they represent: Intercept parameters (`a`), VTL slope parameters (`b`), f0 parameters (`c`), and VTL:f0 interaction parameters (`d`).

```
a = Intercept + A        + G        + A:G 
b = vtl       + vtl:A    + vtl:G    + vtl:A:G
c = f0        + f0:A     + f0:G     + f0:A:G
d = vtl:f0    + vtl:f0:A + vtl:f0:G + vtl:f0:A:G
```

We said we weren't going to provide a formal definition of this model,but if we had the first few lines might have looked something like this: 

$$
\begin{equation}
\begin{split}
height_{[i]} \sim \mathrm{t}(\mu_{[i]},\sigma, \nu) \\ 
\mu_{[i]} = a_{[i]} +  (b_{[i]} \times \mathrm{vtl}_{[i]}) + (c_{[i]} \times \mathrm{f0}_{[i]}) + (d_{[i]} \times \mathrm{f0}_{[i]} \times \mathrm{vtl}_{[i]}) \\ \\
\mathrm{and \; more} \ldots
\end{split}
(\#eq:11-8)
\end{equation}
$$

And the lines below that would have described $a$, $b$, $c$, and $d$ as the sum of the appropriate model parameters. Notice that there is a symmetry to the parameters for each 'dimension' in our model. Each one contains a 'main effect' term (`Intercept,A, G, A:G`), an interaction with apparent age (`A, vtl:A, f0:A, vtl:f0:A`), an interaction with apparent gender (`G, vtl:G, f0:G, vtl:f0:G`), and an interaction with apparent age *and* gender (`A:G, vtl:A:G, f0:A:G, vtl:f0:A:G`). 
  Notice also that this relatively complex model has been built up out of parts: We discussed the decomposition of intercepts into multiple factors, as in $a$ in chapter 7. We discussed the inclusion of quantitative predictors and the decomposition of variation in these, as in $b$ in chapter 9. Now, we discuss the addition of more quantitative predictors ($c$) and the interactions between these ($d$). Effectively, we have been incrementally adding complexity to our models and from here on increasing complexity mostly consists of sticking together these basic components in more complicated manners. 
  
### Fitting the model

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))

priors = c(brms::set_prior("student_t(3,160, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

model_height_vtl_f0 =  
  brms::brm (height ~ vtl*f0*A*G + (vtl*f0*A*G|L) + (1|S), data = exp_data, 
             chains = 4, cores = 4, warmup = 1000, iter = 5000, thin = 4, 
             prior = priors, family = "student")
```
```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_height_vtl_f0 = bmmb::get_model ('11_model_height_vtl_f0.RDS')
```
```{r, include = FALSE, eval = TRUE}
# saveRDS (model_height_vtl_f0, '../models/11_model_height_vtl_f0.RDS')
model_height_vtl_f0 = readRDS ('../models/11_model_height_vtl_f0.RDS')
```

Normally this is around where we would discuss and interpret the characteristics of our model. We're going to leave this for chapter 14, where we present a model very similar to this in a format similar to what you might see in an academic journal. Instead, we will discuss some advantages of working with Bayesian models over some more 'traditional' (although still modern) approaches. 

### Advantages of Bayesian multilevel models for large models

At this point we have enough model components to build very large and complicated models, like the one we fit above. Traditionally, models with many predictors have presented three general problems. First, a model with 'too many' predictors may return spurious values for the 'extra' predictors. Sometimes these spurious values are difficult to distinguish from the 'real' parameters, leading to incorrect conclusions. Second, the model may not be able to fit/converge on a solutions, meaning you can't get the model coefficients. Regardless of the approach to parameter estimation, more-complicated models make it more and more difficult for any estimator to 'find' the optimal parameter values given the data. Third, it can be difficult to interpret a model with *hundreds* (or thousands) of parameters. This becomes especially problematic when considered together with problem (1), that some of these values may simply represent noise. In this section we're going to discuss how working with multilevel Bayesian models can naturally help us with problems (1) and (2) above. In the following section we will discuss how our models can also help us resolve the third problem above.  
  The Bayesian models fit by `brms` have three properties that help resolve the first two problems above. First, the use of prior probabilities and shrinkage, when properly applied, tend to 'pull' weakly-supported parameter values closer to the group mean (or to zero). This can help reduce many of the problems associated with models with large numbers of parameters (cite). Second, the fact that confidence intervals are provided for all parameters helps distinguish random variation from variation that is unlikely to be zero. Third, the fact that our models do not try to find *the* best solution, but simply sample the posterior distribution, relieves much of the pressure associated with finding *the* best solution for models with large numbers of parameters. 
  To this point we have not discussed the `lmer` function very much, apart from in 'Frequentist corner' at the end of some chapters. The `lmer` function (linear mixed-effects regression) is an extremely popular and extremely useful function. In general, `lmer` and an equivalent model specified in `brms` should provide reasonably similar answers when fitting the same models. However, there are some important differences between the two approaches. First, rather than provide a distribution of sample for each parameter, `lmer` returns *point estimates* representing the *best* values of parameters. This can cause a problem when parameters are bounded or grow without bound. 
  For example, standard deviation parameters (e.g., $\sigma$) cannot be 0 or negative, but sometimes they are very, very small. So, when `lmer` tries to find the 'best' value, it can get closer, and closer, and closer to zero, leading to problems involving calculations with very small values (e.g., as $x$ approaches $0$, $1/x$ approaches $\infty$). In contrast, Bayesian models don't try to find the single 'best' value but instead collect a series of samples. As a result, the models are less likely to encounter problems when they try to estimate values that are very close to boundaries (they just 'bounce' of the boundary as they randomly sample). 
  A second difference between the `lmer` estimation method and that of our Bayesian multilevel models is that `lmer` doesn't use prior probabilities to estimate any of its parameters. This can cause some problems when estimating a large number of parameters without large amounts of data. In contrast, `brms` applies 'shrinkage' to all its parameters (at least in principle), which can help avoid some of the problems encountered by `lmer`. 
  We're going to compare the output of `lmer` and `brms` for the model we fit above. Just to be clear, our intention in comparing `lmer` and `brms` is not to compare different statistical *philosophies* or epistemological claims. Our aim is much, much more modest than that. We simply wish to compare `brms`, an approach that 1) uses priors, 2) provides confidence intervals, and 3) estimates based on posterior samples, to a broadly similar approach that does not (`lmer`). Below we fit a model equivalent to `model_height_vtl_f0` using `lmer`.

```{r, eval = FALSE}
lme_model_height_vtl_f0 =
  lme4::lmer (height ~ vtl*f0*A*G + (vtl*f0*A*G|L) + (1|S), 
              data=exp_data,verbose = TRUE,
              control=lme4::lmerControl(optCtrl=list(maxfun=20000),optimizer="bobyqa"))
```
```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
lme_model_height_vtl_f0 = bmmb::get_model ('11_lme_model_height_vtl_f0.RDS')
```
```{r, include = FALSE, eval = TRUE}
#saveRDS (lme_model_height_vtl_f0, '11_lme_model_height_vtl_f0.RDS')
lme_model_height_vtl_f0 = readRDS ('../models/11_lme_model_height_vtl_f0.RDS')
```

We won't show the model print statements as they are both quite large, but we compare the estimates of our fixed effects in figure \@ref(fig:F11-4). Clearly, the two approaches provide very similar estimates for the model fixed effects. That's reassuring because if results differed dramatically for approximately the same model based on the software used, we would need to think very carefully about the causes and possible meaning of these differences.
  Since our model has a large number of parameters we're going to focus on interpreting those that seem likely to result in 'meaningful' differences in apparent height. This is a similar approach to Kusch's Region of Practical Equivalence (ROPE, cite). Rather than worrying about whether things are exactly zero or not, Kruschke suggests we can think about when things are so small that they may as well be zero. For human height, apparent and veridical, we define meaningful differences as those that 1) are likely to be different from zero, *and* 2) have magnitudes of at least around 1 cm (about 0.5 inches). We establish this lower limit simply based off the fact that people may describe themselves as 175 cm (or 5'10.5") but rarely make distinctions smaller than that (e.g. people rarely distinguish 5'10.5" and 5'10.75"). In addition, governments, doctors and even institutions like the NBA which are extremely interested in height will rarely measure to within less than 1 cm (half an inch). As a result, although a difference of 0,1 cm may very well be consistently different from zero, in the context of apparent height it likely has little to no practical significance.
[SB - update results below.] 
  We will discuss the Bayesian model fixed effects, which you can see by running `fixef(model_height_vtl_f0)`. Our model suggests non-zero slopes for our plane along the VTL (mean = 3.12, s.d. = 0.64, 95% C.I = [1.85, 4.4]) and f0 dimensions (mean = -3.52, s.d. = 1.45, 95% C.I = [-6.32, -0.63]). However, there is not much reliable evidence that the `vtl:f0` reliably affects outcomes (mean = -1.28, s.d. = 1.05, 95% C.I = [-3.32, 0.81]). This means that, overall, our responses can be predicted by a plane along f0 and VTL without curving the plane into a saddle shape. The `vtl:A1` interaction suggests a differing use of VTL based on apparent adultness (mean = -2.02, s.d. = 0.51, 95% C.I = [-3.03, -1.02]), but there do not appear to be any other meaningful interactions between VTL, f0 and the other predictors. In terms of intercept terms (i.e. those not interacting with quantitative predictors), there is a large effect for apparent age (mean = 7.04, s.d. = 1.18, 95% C.I = [4.69, 9.35]) but no main effect for apparent gender (mean = -0.17, s.d. = 0.75, 95% C.I = [-1.63, 1.36]). However, the interaction between apparent age and apparent gender (`A1:G1`) had a 95% credible interval that did not overlap with zero and had a posterior mean value of -1.72 cm (s.d. = 0.61, 95% C.I = [-2.9, -0.5]). This indicates that although apparent gender had an average effect of about 0 cm on apparent height it may have had meaningful, but opposite, effects based on the apparent age of the speaker. 
    In order to interpret our predictors in the presence of interactions, we need to consider the simple effects. For example, in order to consider the `vtl:A1` interaction we need to consider the simple effect of VTL for apparent children, and then for apparent adults. This is done in the manner outlined in chapter 9 and 10 for quantitative predictors, independently for each predictor and interaction (i.e. cross-product) between predictors.  We're not going to go over the interpretation of the coefficients and the reconstruction of the simple effects here, as this will be carried out in chapter 14.  

```{r F11-4, fig.height = 3.5, fig.width = 8, fig.cap='A comparison of fixed effect estimates provided by the brms (red) and lmer (black) models. The brms intervals are the 95% credible intervals, those for lmer are twice the standard error of the parameter estimate.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.4
################################################################################
pts = fixef (lme_model_height_vtl_f0)[-1]
err_bars = summary (lme_model_height_vtl_f0)$coefficients[-1,2]

fixefs = fixef (model_height_vtl_f0)[-1,]
good = (fixefs[,3] < -0.5 & fixefs[,4] < -0.5) | (fixefs[,3] > 0.5 & fixefs[,4] > 0.5) 
pchs = ifelse (good, 16,1)

par (mfrow = c(1,1), mar = c(6,4,1,1))
bmmb::brmplot (fixef (model_height_vtl_f0)[-1,], ylim = c(-7,10),las=2, pch=pchs,lwd=2)
points (fixef (model_height_vtl_f0)[-1,1], pch=pchs,lwd=2,cex=1.5)
points ((1:15)+.2, pts, cex=1.5,lwd=2,col=2,pch=16)
segments((1:15)+.2, pts-2*err_bars,(1:15)+.2, pts+2*err_bars,lwd=2,col=2)

abline (h = c(-0.5,0.5), lty = 3, col = bmmb::cols[6],lwd=2)
```

Figure \@ref(fig:F11-5) presents some ways in which the information provided by `lmer` and `brms` differ. In the top row we see a comparison of the 'random effects' standard deviation estimates provided by our two models, and the error terms estimated by our two models. For example, the standard deviation of the listener f0 'random effect' ($\sigma_{f0 \colon L}$) represents the amount of variation around zero between the listener-specific effects for f0 in each model. As we can see, the models provide reasonably similar standard deviation estimates for most parameters. However, note that the `brm` models provide credible intervals for all parameters, while the `lmer` model only provides point estimates for these parameters. 
  The lack of intervals on parameter estimates makes it difficult to 'rule out' variance parameters since they will *never* equal exactly zero. So, we will always have non-zero numbers for these parameters, and 'secretly' some of these are zero or nearly zero. In addition, variance components very close to zero can cause problems when estimating our models. For example, fitting our model with `lmer` resulted in the following error: 

`boundary (singular) fit: see help('isSingular')`

Which warns us that some of the variance components we are trying to estimate are quite small. Because `brm` simply samples from the posterior and doesn't try to find *the* best model parameters, it does not run into these problems. 
  In addition, we can use our credible intervals to figure out which variance components are unlikely to matter: Variance components whose credible intervals are concentrated near zero. There are several such components in figure \@ref(fig:F11-5). In the *best case*, many of these components reflect a tiny amount of systematic variation in our outcomes and so are unlikely to matter much. Notice that it is difficult to predict which standard deviation parameters are distinguishable from zero based on their magnitude alone. This makes point estimates of limited utility for this purpose in the absence of intervals around our estimates.
  In the middle panel of figure  we see a comparison of the listener-dependent `A1` 'random effects' estimated using the two approaches. Just as with the estimates of the fixed effects in the middle panel of figure \@ref(fig:F11-4) we see a close alignment between the two. However, just as for our standard deviation terms we see that a lack of intervals around our estimates makes it difficult to compare our parameter estimates to specific values (such as zero). 

```{r F11-5, fig.height = 6, fig.width = 8, fig.cap='Points and intervals represent means and 95% credible intervals for brms parameter estimates for `model_height_vtl_f0`. Crosses indicate point estimates provided in `lme_model_height_vtl_f0`. (top) Estimates of random effect standard deviations. (middle) Estimates for the listener dependent effects of apparent age. (bottom) Estimates of correlations between random effects.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.5
################################################################################

Ssd = attr(VarCorr(lme_model_height_vtl_f0)[[1]],'stddev')
Lsd = attr(VarCorr(lme_model_height_vtl_f0)[[2]],'stddev')
lmer_vars = c(Lsd, Ssd, sigma(lme_model_height_vtl_f0))
vars_final = bmmb::getsds(model_height_vtl_f0)

layout (m=1:3, heights = c(.4,.3,.3))
par (mar = c(7,4,.5,1))
bmmb::brmplot (vars_final, ylim = c(0,8.5), las = 2,cex.axis=1.3,
               col=bmmb::cols[14],yaxs="i")
points (lmer_vars, cex=3,lwd=3,col=bmmb::cols[2],pch=4)

pts = ranef(lme_model_height_vtl_f0)$L[,4]

par (mar = c(.5,4,.5,1))
bmmb::brmplot (ranef(model_height_vtl_f0)$L[,,4],labels="", col = bmmb::cols[8])
points ((1:15), pts, cex=3,lwd=3,col=bmmb::cols[7],pch=4)


corrs_lme = attr(VarCorr(lme_model_height_vtl_f0)$L,"correlation")
corrs_lme = corrs_lme[lower.tri (corrs_lme)]
corrs_brms = bmmb::getcorrs(model_height_vtl_f0, "L")

bmmb::brmplot (corrs_brms, ylim = c(-.97,.97), las = 2, labels = "",line=FALSE,col=bmmb::cols[4])
abline (h=0)
points (corrs_lme, cex=1.5,lwd=3,pch=4, col = bmmb::cols[12])

```

Finally, in the bottom panel of figure \@ref(fig:F11-5) we compare the correlations for the listener random effects across the two models. Since there were 16 listener-dependent parameters, we needed to estimate 120 unique correlations between these parameters. As with our standard deviation parameters, `brms` gives us intervals while `lmer` returns point estimates. In addition, for the first time we see a substantial difference between the estimates provided by `lmer` and those provided by `brms`. This is likely a result of the fact that `lmer` does not apply something like partial pooling to its correlation estimates, either through the use of prior probabilities or other mechanisms. As a result, we see that the `lmer` estimates vary substantially around 0 while the `brms` estimates are all close to zero, and mostly have intervals that include zero. We see that in this case our Bayesian model protects us by: 1) Providing credible intervals for all parameters, letting us accept values of zero as likely, and 2) the LKJ prior we used for our random effect correlation matrices pulls weakly-supported correlations to zero, thereby protecting against spurious results. 
  The fact that most of our correlation estimates are nearly zero not only lets us rule some out, but also focuses our attention on those correlations that *do* deviate from this pattern. For example, the third correlation from the left in the bottom panel of figure \@ref(fig:F11-5) represents the correlation between the listener-dependent intercepts and effects for apparent age. Although its 95% credible interval includes some very small values (mean = -0.38, s.d. = 0.17, 95% C.I = [-0.69, -0.01]), it seems reasonable that this correlation may in fact be negative and non-zero. In fact, we found this correlation in a previous model (section X) and also discussed why we think this correlation 'makes sense' (section X). 
  
## Bayesian Analysis of Variance

The information we presented in the previous section can be used to consider which predictors, or groups of predictors, are important for understanding variation in the dependent variable. This approach becomes more and more useful as our models increase in complexity and we end up with dozens or hundreds of parameters that we then need to interpret. The framework to be presented here is outlined by by Gelman and colleagues in Gelman anova paper, Gelman and hill chapter 22, and Gelman et al BDA3 chapter. 
  The analysis of variance (ANOVA) is a set of modeling techniques meant to help understand the components and sources of variation in a dependent variable. A 'traditional' ANOVA tries to **decompose** variation in the dependent variable into independent components so that it can relate combinations in order to carry out different statistical tests. If that statement makes no sense to you, that is because that approach is fundamentally different to the sorts of things we have been doing with our multilevel Bayesian models. We're not going to talk about a 'traditional' ANOVA in any detail here as that would involve the introduction of a *parallel universe* of statistical concepts and jargon that has not been discussed in this book. In addition, there are many excellent treatments on the subject including (cite myers and well and cite others). That being said, we can discuss how ANOVA features concepts that are very useful for Bayesian multilevel models. 
    At its core ANOVA consists of thinking of variation in the dependent variable as the sum of a set of **components of variation** related to the predictors in our model. For example, consider the formula for `model_four_groups` we fit in chapter 7:

`height ~ C + (C|L) + (1|S)`

This formula implies a model with a large number of parameters. These parameters can be thought of as consisting of 'batches' of thematically related coefficients. For example, we can think of the following batches of parameters:

  * 3 parameters representing the fixed effects for apparent speaker category `C`.
  * 15 parameters representing the listener-dependent intercepts. 
  * 45 (15 x 3) parameters representing the listener-dependent effects for apparent speaker category.  
  * 139 parameters representing the speaker-dependent intercepts. 

We can think of the total variation in apparent height judgments as being composed of variation in each of these batches of parameters (among other things). In addition, each of these batches has a thematic or semantic link; these are not just parameters grouped at random. When we break up variation into thematically-grouped parameters, we are doing an **ANOVA-like decomposition** of variation in the dependent variable. We introduced this approach in chapter 7 without directly referring to ANOVA and have been using this approach to understand variation in our predictors in most of our models since then.  
  Here's what Gelman and Hill have to say about the analysis of variance in the context of multilevel models:

> "When moving to multilevel modeling, the key idea we want to take from the analysis of variance is the estimation of the importance of different batches of predictors (“components of variation” in ANOVA terminology). As usual, we focus on estimation rather than testing: instead of testing the null hypothesis that a variance component is zero, we estimate the standard deviation of the corresponding batch of coefficients. If this standard deviation is estimated to be small, then the source of variation is minor—we do not worry about whether it is exactly zero. In the social science and public health examples that we focus on, it can be a useful research goal to identify important sources of variation, but it is rare that anything is truly zero." (p. 490)

> "the standard deviation of a set of coefficients gives a sense of their predictive importance in the model. An analysis-of-variance plot, which shows the relative scale of different variance components, can be a useful tool in understanding a model." (p. 492)

  Parameters that vary a lot reflect large differences in our dependent variable. These will be represented by large standard deviations. In contrast, parameters that do not vary much will be represented by small standard deviations. These parameters do not have a large effect on our data (since they do not vary much). For example, we can see in figure \@ref(fig:F11-5) that the standard deviation of listener-dependent intercepts ($\sigma_L$) is much larger than the listener-dependent interaction between VTL and apparent gender ($\sigma_{VTL \colon G \colon L}$). We can see that this directly relates to the variation of each batch of 'random effects' around zero, as shown in figure \@ref(fig:F11-6). Whether or not $\sigma_{VTL \colon G \colon L}$ is exactly zero, we can look at figure \@ref(fig:F11-6) and see that this predictor does not predict much variation in our dependent variable. As a result of this, by inspecting the standard deviations in figure \@ref(fig:F11-5) we can see that although there are a large number of predictors, only a couple are having any meaningful effect on apparent height judgments. 
  
```{r F11-6, fig.height = 3, fig.width = 8, fig.cap='Listener-dependent intercepts (circles) and `vtl:G1` interactions (triangles).', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.6
################################################################################

par (mfrow = c(1,1), mar = c(4,4,1,1))

brmplot (ranef (model_height_vtl_f0)$L[,,"Intercept"], col = bmmb::cols[9], 
         line=TRUE)
brmplot (ranef (model_height_vtl_f0)$L[,,"vtl:G1"], add = TRUE, 
         nudge = 0.1, labels = "", col = bmmb::cols[10],pch=17)
```
  
Gelman and Hill distinguish two types of standard deviations for a 'random' effect with $J$ levels (p. 459) (emphasis ours):

> "The **superpopulation** standard deviation, which represents the variation among the modeled probability distribution from which the were drawn, is relevant for determining the uncertainty about the value of a new group not in the original set of J." 

> "The **finite-population** standard deviation of the particular J values of [$\alpha_{[subj]}$] describes variation within the existing data." 

  So, the *finite-population* standard deviation terms reflect the variation we observe in our actual model parameters. This concept can be applied to batches of 'random' effects estimated with adaptive partial pooling and to 'fixed' effects estimated with fixed priors. The *superpopulation* standard deviation estimates correspond to the specific $\sigma_\beta$ term estimated by our model for some batch $\beta$ of 'random' effects. No analogue exists for our 'fixed' effects, and so we can only get superpopulation standard deviations for batches of parameters fit with adaptive partial pooling (i.e. 'random' effects). The authors note that (p. 464):

> "The superpopulation [σ] and finite-population [s] standard deviations are not two different statistical “estimators” of a common quantity; rather, they are two different quantities that can both be estimated from the multilevel model. We can get a point estimate and uncertainty intervals for both. In general, the point estimates of σ and s will be similar to each other, but s will have less uncertainty than σ. That is, the variation is more precisely estimated for the finite population than the superpopulation. This makes sense because we have more information about the units we observe than the full population from which they are sampled."

Gelman and colleagues suggest the following general process, that can be referred to as a **Bayesian analysis of variance**, or **BANOVA**:

  1) Fit the model with the structure you think is required to capture the variation in the data. 
  
  2) Calculate the superpopulation and finite-population standard deviations for predictors or groups of predictors. 
  
  3) Make a plot comparing the magnitudes of different predictors, and of the uncertainty in the estimates. The authors refer to this as an **ANOVA plot**.
  
  4) Us the ANOVA plot to make inferences about the relative importance of your predictors, and to guide your analysis. 
  
We could potentially add a fifth step: 5) refit reduced model if some components show little to no importance or variation across clusters, and if you have some compelling reason to do so. This step is not strictly necessary, and may sometimes be a bad idea, but we will consider it as a possibility. 
  
### Getting the standard deviations from our models 'manually'

The superpopulation standard deviation is our model's estimate of the standard deviations of different batches of parameters. We can extract the superpopulation standard deviations using the `VarCorr` function. This function returns all sort of information about the variance and correlation parameters estimated by our model. In the second line below we specify that we only want information related to the standard deviation (`"sd"`) of our listener effects (`"L"`).

```{r, cache = TRUE, collapse = TRUE, eval = FALSE, cache = TRUE}
brms::VarCorr(model_height_vtl_f0)

brms::VarCorr(model_height_vtl_f0)[["L"]][["sd"]]
```

We can also get information regarding our error standard deviation using `Varcorr` as seen below:

```{r, cache = TRUE, collapse = TRUE, cache = TRUE}
brms::VarCorr(model_height_vtl_f0)$residual$sd
```

The `bmmb` package contains a function called `getsds` that collects estimates of all standard deviations estimated by the model, including the error:

```{r}
getsds (model_height_vtl_f0)
```

[@@ SB] from here on down is a problem, I need to come up with how to get the fixed effects and also be more consistent in saying rms vs sd, also probably providing formulas for there.]

Getting the finite-sample standard deviations is a bit trickier. First, we should note that although they are called standard deviations, they are actually calculated by finding the root-mean squared error. The reason for this is that since we assume a mean of 0 for each batch of parameters we do not need to subtract the sample mean from our observations when calculating the standard deviation. If one uses sum coding, this means that the parameter values can be squared, their mean can be found, and then the square root of this is our estimate. To calculate the standard deviations of different batches of parameters you need to calculate the root-mean squared deviation for each batch, *for each sample*. This means you end up with an estimate of the finite-sample standard deviation for each set of posterior samples. The code below shows how to calculate the finite-population standard deviations for item based on the random effects parameter estimates. 

```{r, cache = TRUE, collapse = TRUE, eval = TRUE}
# extract matrix representing all random effects from our model
listener_intercepts = ranef(model_height_vtl_f0, summary = FALSE)[["L"]][,,"Intercept"]

# the output is a 2d matrix. dimensions are: (rows) samples, (columns) parameters
str (listener_intercepts)

# we find the standard deviation across each row in the matrix
listener_intercepts_finite = apply (listener_intercepts,1,bmmb::rms)

# we summarize the output 
listener_intercepts_finite = posterior_summary (listener_intercepts_finite)

listener_intercepts_finite
```

For the fixed effects, we can use the absolute value of the parameters when these are each a single 'degree of freedom' (i.e. a single parameter). The code below shows how to get the fixed effects standard deviation estimates assuming that all parameters are unrelated. 

```{r, cache = TRUE, collapse = TRUE, eval = TRUE, cache = TRUE}
# get individual parameter samples
fixefs_finite = fixef(model_height_vtl_f0, summary = FALSE)

# summarize absolute value
fixefs_finite = posterior_summary (abs (fixefs_finite))

fixefs_finite
```

In cases where we have batches of fixed effect predictors, the finite-sample standard deviation of these can be estimated using the same approach used for the random effects shown above: You need a matrix where each column is a batched parameter and then you find the standard deviation across each row. Finally, we can estimate the finite-sample error by getting the model residuals, and then calculating the standard deviation of the residuals for each set of samples as shown below.

```{r, cache = TRUE, collapse = TRUE, eval = TRUE}
# get residuals
sigma_finite = residuals (model_height_vtl_f0, summary = FALSE)

# find standard deviation for each set of samples
sigma_finite = apply (sigma_finite, 1, sd)

# summarize
sigma_finite = posterior_summary (sigma_finite)

# name row, because it has no name by default
row.names(sigma_finite) = 'sigma'
```

### Using the `banova` function

[@@ SB - need to fix it so that it has some way of dealing with clusters of fixed effects ]

The `bmmb` package contains a function called `banova` that can get the finite-sample or superpopulation standard deviations for you. The output is a single data frame that contains a summary of the standard deviations for fixed and random effects, and the error term if the model contains one. Below we use the function to get both kinds of standard deviation, compared in 

```{r, cache = TRUE, include = FALSE, cache = TRUE}
banova_height_vtl_f0_finite = banova (model_height_vtl_f0, superpopulation = FALSE)
banova_height_vtl_f0_super = banova (model_height_vtl_f0, superpopulation = TRUE)
```

[@@ Explain anova plots properly somewhere]

The output of the `banova` function can be used to make a Bayesian ANOVA plot of our model using the `banovaplot` function in the `bmmb` package. If we were to do this right after fitting the model, we would have a pretty good idea of what matters and what doesn't in our data. 

```{r F11-7, fig.height = 4.5, fig.width = 8, fig.cap='(top) Finite sample BANOVA plot for `model_height_vtl_f0`. (bottom) The superpopulation BANOVA plot for the same model.', echo = FALSE}

################################################################################
### Figure 11.7
################################################################################

par (mar = c(.125,4,.125,.125), mfrow = c(2,1), oma = c(6.5,0,1,0))
banovaplot (banova_height_vtl_f0_finite[-2,], las = 2,line=FALSE,labels = "",
            yaxs="i", ylim = c(0,10))
abline (h=0)
box()
banovaplot (banova_height_vtl_f0_super[-2,], las = 2, line=FALSE,yaxs="i",
            ylim = c(0,10))
abline (h=0)
box()

```

[@@ SB - get rid of this once issues above are fixed]

The process Gelman proposes is potentially more complicated that what we've done above. For example, consider the random effects for a factor like vowel category. Imagine there are four categories, so four levels, for each of 50 listeners. The process described treats each of the 50 random effects for each vowel separately (i.e., 4 groups of 50 vowel random effects). The process described by Gelman would treat the 200 vowel random effects (across the four vowels) as one 'batch' of coefficients. This single batch would reflect all of the $Listener \colon Vowel$ interaction. The way we are approaching instead separates each $Listener \colon Vowel$ 'simple effect' and treats it separately. The main reason to do it the way way I've shown above is because it can be done easily for all models, and you get roughly equivalent information from the analysis. If you do want to investigate the variation associated with entire clusters of multiple predictors at a time, please see the Gelman articles linked to above, as there are a few important details that are not discussed here (e.g., the need to 'recover' missing parameters, the need to calculate 'degrees of freedom', etc.).

### Fitting and comparing the reduced model

Our initial model formula was:

`height ~ f0*vtl*A*G + (f0*vtl*A*G|L) + (f0*vtl*A*G|S)`

Because we include all interactions and listener 'random effects' of these, we're basically saying that we think its possible for every predictor to affect every other predictor, and for all of this to vary in a listener-dependent manner. We might instead consider the following model formula which includes only those effects we (arbitrarily) deemed 'large enough' based on our inspection of the Bayesian ANOVA above. 

`height ~ f0 + vtl + A*G + vtl:A1 + (f0 + vtl + A*G + vtl:A1|L) + (1|S)`

This model includes only the predictors that varied most between listeners and also affected our dependent variable the most. In some situations we may be justified in fitting a 'final' model that includes only the important components. We fit the reduced model below:

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))

priors = c(brms::set_prior("student_t(3,160, 12)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 12)", class = "b"),
           brms::set_prior("student_t(3,0, 12)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 12)", class = "sigma"))

model_height_vtl_f0_reduced =  
  brms::brm (height ~ f0+vtl+A*G+vtl:A + (f0+vtl+A*G+vtl:A|L) + (1|S),
             data = exp_data, chains = 4, cores = 4, warmup = 1000, 
             iter = 5000, thin = 4, prior = priors, family = "student")
```
```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_height_vtl_f0_reduced = bmmb::get_model ('11_model_height_vtl_f0_reduced.RDS')
```
```{r, include = FALSE, eval = TRUE}
# saveRDS (model_height_vtl_f0_reduced, '../models/11_model_height_vtl_f0_reduced.RDS')
model_height_vtl_f0_reduced = readRDS ('../models/11_model_height_vtl_f0_reduced.RDS')
```

We can see that the fixed effects shared in common are very similar, as are the estimates of the superpopulation random effects for the listener-dependent parameters (although intervals are wider for the full model).  

```{r F11-8, fig.height = 3, fig.width = 8, fig.cap='(left) A comparison of the means and 95% credible intervals for shared fixed effect parameters in our full and reduced models. (right) A comparison of the means and 95% credible intervals for shared superpopulation standard deviations in our full and reduced models.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.8
################################################################################

sds_reduced = getsds(model_height_vtl_f0_reduced)
sds = getsds(model_height_vtl_f0)
use = rownames(sds) %in% rownames(sds_reduced)
sds = sds[use,]

par (mfrow = c(1,2), mar = c(4,4,1,1))
layout (m = t(c(1,2)),widths = c(.55,.45))
brmplot (fixef (model_height_vtl_f0)[c(2,3,4,5,7,11),], ylim = c(-6,10),las=2,
         ylab = "Centimeters", col = bmmb::cols[2])
brmplot (fixef (model_height_vtl_f0_reduced)[c(3,2,4,5,7,6),],
         add = TRUE, nudge = 0.2, col = bmmb::cols[12],labels = "")
abline (h=0)

legend (4, 10, legend = c("Full","Reduced"),bty='n',pch=16,
        col=bmmb::cols[c(2,12)], pt.cex = 1.3)

brmplot (sds, las = 2, ylab = "Centimeters", col = bmmb::cols[2])
brmplot (sds_reduced, add = TRUE, col = bmmb::cols[12], nudge = .2, labels = "")

```

We can use cross-validation to compare the expected model out-of-sample prediction: 

```{r, cache = TRUE}
model_height_vtl_f0 = add_criterion (model_height_vtl_f0, "loo")
model_height_vtl_f0_reduced = add_criterion (model_height_vtl_f0_reduced, "loo")
```

And see that the reduced model has a lower $\mathrm{elpd}$, but that the differences is only about 1 standard error, making it not at all reliable. 

```{r}
loo_compare (model_height_vtl_f0,model_height_vtl_f0_reduced) 
```

We can also consider the variance explained ($R^2$) by each model:

```{r}
bmmb::r2_bayes(model_height_vtl_f0)
bmmb::r2_bayes(model_height_vtl_f0_reduced)
```

And see that all of the extra complexity included in our full model gains us less than 1% additional variance explained. All of this suggests that a researcher would be justified in fitting and interpreting the reduced model for their research. However, we are inclined to agree with Gelman and colleagues that there is value in fitting a model that includes all predictors and interactions supportable by our data, as long as these relate to potentially interesting research hypotheses. This is because being able to say "these parameters are near zero/show no meaningful variation" is useful information that we lose when we rely only on the reduced model. Of course, we could say something like "we fit a larger model that showed all these effects are zero but we are not presenting it here". However, if we do want to say something like this it may make sense to present the full model to the reader.

## Fitting a multivariate logistic regression model

We're now going to fit a logistic regression model with two quantitative predictors and an interaction between them. We're also going to inspect the model using the principle of Bayesian ANOVA (henceforth **BANOVA**) outlined in the previous section. Before continuing we want to talk briefly about the geometry of the models we'll be fitting. 
  A logistic regression model with two quantitative predictors specifies planes (or saddles) along a third dimension ($z$) representing the logit of the probability of observing a 'success' (a value of 1 for the dependent variable). We will refer to these surfaces as **response surfaces** because these surfaces specify the value of the response variable based on the value of our quantitative predictors. When the value on the response surface is negative, the model predicts a response of 0 at this location, and when the value of the plane is positive the model predicts a 1. When we want to know the probability expected for any given $x$ and $y$ axis location, we can transform the value on response surface using the inverse logit function. When the predicted logits are transformed to probabilities, our model defines a curved shape rather than a flat plane as seen in the bottom row of figure \@ref(fig:11.9). 

```{r F11-9, fig.height = 4, fig.width = 8, fig.cap='(top) Threee perspectives on a plane that specifies logits along its z axis. (bottom) The same plane after undergoing to antilogit transform, now expressing probabilities along the z axis.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11.9
################################################################################

f1 = function (a,b,c) a * 1 + b *1  
x <- y <- seq(-3, 3, length= 12)

par (mfrow = c(2,3), mar = c(1,1,0,0), oma = c(0,0,0,0))

z <- outer(x, y, f1)
persp(x, y, z, col = bmmb::cols[3],theta = 0,zlim = c(-6.5,6.5))
persp(x, y, z, col = bmmb::cols[3],theta = -90,zlim = c(-6.5,6.5))
persp(x, y, z, col = bmmb::cols[3],theta = 45,zlim = c(-6.5,6.5))

z <- outer(x, y, f1)
persp(x, y, ztop(z), col = bmmb::cols[4],theta = 0,zlim = c(0,1))
persp(x, y, ztop(z), col = bmmb::cols[4],theta = -90,zlim = c(0,1))
persp(x, y, ztop(z), col = bmmb::cols[4],theta = 45,zlim = c(-0,1))

```

When our models include interactions between quantitative predictors (i.e. terms representing their cross-products), our predicted values no longer vary along planes. Instead, the surface will resemble a saddle shape based on the sign of the parameter and its magnitude relative to the slopes of the relevant quantitative predictors in the model. When we convert these saddle shapes to represent percentages using the antilogit function, the resulting shape can be strange, as with the bottom row of figure \@ref(fig:11-10) which shows two non-contiguous sections associated with 'successes'. 

```{r F11-10, fig.height = 4, fig.width = 8, fig.cap='(top) Threee perspectives on a saddle shape that specifies logits along its z axis. (bottom) The same saddle shape after undergoing to antilogit transform, now expressing probabilities along the z axis.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-10
#######################\#########################################################

f1 = function (a,b,c) a * 1 + b *1 + a*b*1  

x <- y <- seq(-3, 3, length= 12)

par (mfrow = c(2,3), mar = c(1,1,0,0), oma = c(0,0,0,0))

z <- outer(x, y, f1)
persp(x, y, z, col = bmmb::cols[5],theta = 0,zlim = c(-9.5,15.5))
persp(x, y, z, col = bmmb::cols[5],theta = -90,zlim = c(-9.5,15.5))
persp(x, y, z, col = bmmb::cols[5],theta = 45,zlim = c(-9.5,15.5))

z <- outer(x, y, f1)
persp(x, y, ztop(z), col = bmmb::cols[6],theta = 0,zlim = c(0,1))
persp(x, y, ztop(z), col = bmmb::cols[6],theta = -90,zlim = c(0,1))
persp(x, y, ztop(z), col = bmmb::cols[6],theta = 45,zlim = c(-0,1))

```

### Data and research questions

In the code below we load our packages, set our contrasts, and load our experimental data. We also add our dependent variable, `F`, to our data frame. This variable equals 1 when the listener indicated hearing a female speaker and 0 when the listener indicated hearing a male speaker. As with our previous model we center our quantitative variables and divide f0 by 100 to make the expected regression coefficients for VTL and f0 more similar in magnitude.


```{r, warning=FALSE, message=FALSE}
library (brms)
library (bmmb)
options (contrasts = c('contr.sum','contr.sum'))

data (exp_data)

# our dependent variable
exp_data$F = ifelse(exp_data$G == 'f', 1, 0)

exp_data$vtl_original = exp_data$vtl
exp_data$vtl = exp_data$vtl - mean (exp_data$vtl)

exp_data$f0_original = exp_data$f0 
exp_data$f0 = exp_data$f0 - mean(exp_data$f0)
exp_data$f0 = exp_data$f0 / 100
```

Our research questions are:

Q1) Do listeners use VTL, f0, and the interaction between them to determine speaker gender?

Q2) Does apparent speaker age influence the use of the above mentioned acoustic cues?

Q3) Is there a lot of between-speaker variation in perceptual behavior?

### Description of the model

Our model formula is very much like the one we used for our model investigating the perception of femaleness in the last chapter, save for the addition of f0 and its interaction with the other predictors. Our formula will be:

`F ~ vtl * f0 * A + (vtl * f0 * A|L) + (1|S)`

We'll use the same priors we used for our logistic models last chapter. Since we scaled f0 so that a change of 1 corresponds to 100 Hz, a 1 unit change in f0 this value represents about the average difference in f0 between adult males and females. For this reason, we think it's reasonable to use a prior of the same magnitude as we used for our VTL parameter. We will omit our model specification since it is quite large and very similar to the presented in chapter 10 for `model_gender_vtl`. 

### Fitting and the model and applying a Bayesian ANOVA

Below we fit our model:

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))

model_gender_vtl_f0 =
  brm (F ~ vtl*f0*A + (vtl*f0*A|L) + (1|S), data=exp_data, 
       chains=4, cores=4, family="bernoulli", 
       warmup=1000, iter = 5000, thin = 4,  
       prior = c(set_prior("student_t(3, 0, 3)", class = "Intercept"),
                 set_prior("student_t(3, 0, 3)", class = "b"),
                 set_prior("student_t(3, 0, 3)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))
```
```{r, include = TRUE, eval = FALSE}
# Or download it from the GitHub page:
model_gender_vtl_f0 = bmmb::get_model ('11_model_gender_vtl_f0.RDS')
```
```{r, include = FALSE, eval = TRUE}
# saveRDS (model_gender_vtl_f0, '../models/11_model_gender_vtl_f0.RDS')
model_gender_vtl_f0 = readRDS ('../models/11_model_gender_vtl_f0.RDS')
```

We can consider the model fixed effects using `brmplot` in figure \@ref(fig:F11-11). and beside that we make a BANOVA plot that compares the fixed effects, the finite-sample standard deviations for 'batches' our random effects parameters, and the residual error.

```{r, include = FALSE}
banova_gender_vtl_f0 = bmmb::banova (model_gender_vtl_f0)
```

```{r F11-11, fig.height = 3, fig.width = 8, fig.cap='(left) A plot showing the fixed effects estimates for `model_gender_vtl_f0`. (right) a BANOVA plot of the same model.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-11
################################################################################

par ( mfrow = c(1,2), mar = c(5.5,3,1,1))
layout (mat= t(c(1,2)), widths = c(.4,.6))
bmmb::brmplot (brms::fixef(model_gender_vtl_f0), line = TRUE, las = 2)
bmmb::banovaplot (banova_gender_vtl_f0, line = TRUE, las = 2)
```

In the figure above we see a relatively complex model, with an important role for both f0 and VTL. We also see that unlike for our model predicting apparent height above, this model features a prominent f0 by VTL interaction (`vtl:f0`), and an interaction between this and apparent age (`vtl:f0:A1`). Just as with our last model, we are going to leave the interpretation of this model for chapter 14. Instead, we're going to focus on using our models to understand the classification of speakers into apparent females and apparent males.

### Categorization in two dimensions

Imagine a horizontal plane such that $z=0$ for all values of $x$ and $y$ in figures \@ref(fig:11-9) and \@ref(fig:11-10). This plane would represent an expected probability of 0.5, meaning it is the boundary between successes and failures along the $z$ axis. Our response surface will intersect with the horizontal $z=0$ plane, forming a curve at this intersection. These curves represent the division of the response surface into sections with values greater than 0 (expected response 1) and sections with values less than 0 (expected response 0). Thus, the curves formed by the intersection of our response surface and the $z=0$ plane represent the *category boundary* as defined by our model.
  The intersection of two planes forms a straight line. Although these surfaces are not flat, their intersection with the plane at $z=0.5$ (i.e. the plane where logit = 0) will still form straight lines. Although the surfaces in the bottom row of figure \@ref(fig:11-9) are not planes, the intersection of these surfaces with planes still form straight lines. However, the intersection between a saddle shape and the plane where $z=0$ will not be a straight line. Instead, it will be a **hyperbola** a shape that resembles a pair of parabolas that approach, but never cross an asymptote. Since saddle shapes can fall and then rise again, a surface of this kind may intersect with the plane at $z=0$ in more than one location (seen in figure \@ref(fig:F11-10)). 
  To find the intersection of the surfaces defined by our models and the plane at $z=0$, we can use some basic algebra. We take the equation defining the general shape, including the cross-product of $x$ and $y$:

$$
\begin{equation}
z =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{c} \times y) + (\mathrm{d} \times xy)
(\#eq:F11-5)
\end{equation}
$$

First, we consider the intersection of the plane at $z=0$ and a plane defined by our model. To do this we set both $z=0$ and $d=0$, and solve for $y$ as shown below. 

$$
\begin{equation}
\begin{split}
0 =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{c} \times y) + (0 \times xy) \\
y =  -(\mathrm{b} \times x - \mathrm{a}) / \mathrm{c}
\end{split}
(\#eq:F11-5)
\end{equation}
$$

Second, we consider the intersection of the plane at $z=0$ and the saddle shape defined by our model. To do this we set only $z=0$, and solve for $y$ again as shown below. 

$$
\begin{equation}
\begin{split}
0 =  \mathrm{a} + (\mathrm{b} \times x) + (\mathrm{c} \times y) + (d \times xy) \\
y =  (-\mathrm{b} \times x - \mathrm{a}) / (d \times x + c)
\end{split}
(\#eq:F11-5)
\end{equation}
$$

The $a, b, c$ and $d$ parameters represent the model coefficient for the intercept, VTL slope, f0 slope, and cross-product parameter respectively. We can get these for our model by adding up the correct combination of fixed effects parameters. 
  Below we get the samples for our fixed effects parameters from our model and add up the appropriate parameters to calculate our $a, b, c$ and $d$ parameters for apparent adults, for apparent children, and overall. In each case, we combine the necessary parameters first and then find the average across the samples of the parameter. 

```{r, collapse = TRUE}
# get fixed effect parameters
samples = brms::fixef (model_gender_vtl_f0, summary = FALSE)

# get a,b,c,d coefficients for overall surface
a_all = mean (samples[,"Intercept"])
b_all = mean (samples[,"vtl"])
c_all = mean (samples[,"f0"])
d_all = mean (samples[,"vtl:f0"])

# get a,b,c,d coefficients for adult surface
a_adult = mean (samples[,"Intercept"] + samples[,"A1"])
b_adult = mean (samples[,"vtl"] + samples[,"vtl:A1"])
c_adult = mean (samples[,"f0"] + samples[,"f0:A1"])
d_adult = mean (samples[,"vtl:f0"] + samples[,"vtl:f0:A1"])

# get a,b,c,d coefficients for child surface
a_child = mean (samples[,"Intercept"] - samples[,"A1"])
b_child = mean (samples[,"vtl"] - samples[,"vtl:A1"])
c_child = mean (samples[,"f0"] - samples[,"f0:A1"])
d_child = mean (samples[,"vtl:f0"] - samples[,"vtl:f0:A1"])
```

We can use these parameters to generate curves representing the boundaries between expected female and expected male responses. We do this for our hyperbolic (equation X) boundaries in figure \@ref(fig:F11-12). You may notice small lines in the top left corner of the left plot in the figure. When we 'zoom out' in the middle plot the figure \@ref(fig:F11-12), we see that this model actually predicts male classifications in the top left corner as well as in the bottom right corner. This is a result of the fact that our hyperbolic boundaries come in pairs, resulting from the bimodal shape seen in the bottom row of figure \@ref(fig:F11-10).
  In the right plot of figure \@ref(fig:F11-12) we present a *territorial map* of our stimulus space as defined by speaker VTL and f0, for apparent adults. This map tells us the expected classification of speakers into males and females given their voice f0 and VTL, and assuming that they are perceived as being adults. Unlike the territorial maps presented in the previous chapter (in figure X), the map below is defined along a two-dimensional stimulus space. In addition, although we only present one below we can make three separate maps, one for apparent children, one for apparent adults, and an overall map. 

```{r F11-12, fig.width = 8, fig.height = 3, fig.cap = "(left) Each point represents a single speaker, labels indicate most common group classification. Curves indicate male/female boundaries for adults (green), children (orange), and overall (blue). (middle) The same as the left figure but zoomed out more. (right) Territorial maps showing expected classifications for apparent adults in different regions of the f0 by VTL stimulus space.", echo = FALSE, cache = FALSE}

################################################################################
### Figure 11-12
################################################################################

# y = (-b*x - a - z) / (dx+c)
# fixef(model_gender_vtl_f0)

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
mu_vtl = mean (tmp$vtl_original)
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
mu_f0 = mean (tmp$f0_original)
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

aggd = aggregate (cbind ( height, A=="a", G=="f", vtl,f0, vtl) ~ S + C_v, 
                      data = tmp, FUN = mean)
aggd$C = ""
aggd$C[aggd[,4]>= 0.5 & aggd[,5]>= 0.5] = "w"
aggd$C[aggd[,4]>= 0.5 & aggd[,5]<= 0.5] = "m"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]>= 0.5] = "g"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]<= 0.5] = "b"
#table(aggd$C)

tab = table (tmp$S, tmp$C)
mod_cat = apply (tab, 1,which.max)

par (mfrow = c(1,3), mar = c(4,.25,.5,.25), oma = c(0,4,0,0))

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      pch=16,lwd=2, xlab = "",ylab="Height (inches)")
grid()
points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = bmmb::cols[c(2:5)][factor(aggd$C)])

curve ((-b_all*x - a_all) / (d_all*x+c_all), from = -6, to = -c_all/d_all, 
       add = TRUE,lwd=2,col=bmmb::cols[7])
curve ((-b_all*x - a_all) / (d_all*x+c_all), from = -c_all/d_all, 
       to = 6, add = TRUE,lwd=2,col=bmmb::cols[7])

curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -6, to = -c_adult/d_adult-.05, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[10])
curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -c_adult/d_adult+.05, to = 6, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[10])

curve ((-b_child*x - a_child) / (d_child*x+c_child), from = -6, to = -c_child/d_child, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[8])
curve ((-b_child*x - a_child) / (d_child*x+c_child), from = -c_child/d_child, to = 6, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[8])

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      pch=16,lwd=2, xlab = "",ylab="Height (inches)",xlim = c(-4,4),ylim = c(-2,2),
      yaxt='n')
grid()
points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = bmmb::cols[c(2:5)][factor(aggd$C)])

curve ((-b_all*x - a_all) / (d_all*x+c_all), from = -6, to = -c_all/d_all, add = TRUE,lwd=2,col=bmmb::cols[7])
curve ((-b_all*x - a_all) / (d_all*x+c_all), from = -c_all/d_all, to = 6, add = TRUE,lwd=2,col=bmmb::cols[7])

curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -6, to = -c_adult/d_adult-.05, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[10])
curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -c_adult/d_adult+.05, to = 6, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[10])

curve ((-b_child*x - a_child) / (d_child*x+c_child), from = -6, to = -c_child/d_child, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[8])
curve ((-b_child*x - a_child) / (d_child*x+c_child), from = -c_child/d_child, to = 6, add = TRUE,
       lwd=2, lty=2,col=bmmb::cols[8])

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-4,4), ylim = c(-2,2),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()

rect (-5,-3,5,3,col=2)

x = seq(-c_adult/d_adult+.05,5,0.1)
y = (-b_adult*x - a_adult) / (d_adult*x+c_adult)
polygon (c(-5,x,5),c(-3,y,-3),col="seagreen")

x = seq(-5 , -c_adult/d_adult-.05,0.1)
y = (-b_adult*x - a_adult) / (d_adult*x+c_adult)
polygon (c(-5,x,5),c(3,y,3),col="seagreen")

points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = 0)

```

The high-frequency male region suggests that speakers with a very high f0 and a very short VTL are likely to be identified as males, even for adults. Note that we don't have any speakers in these regions and so we can only guess what listeners might have done. However, common sense suggests that for adult speakers at least, speakers with a very short VTL and a very high f0 are not very likely to be identified as male. Does this matter? Yes and no. Does it matter that the earth isn't flat but our maps mostly act like it is? If it's *flat enough* in the area described by our map, then maybe not. If you're trying to find the shortest flight time between two cities, it probably does. 
  So, we have to decide what our model is for. If we only want to understand behavior in the regions where we mostly have a lot of data, then the high-frequency male region probably doesn't matter. On the other hand, if we want to understand how listeners might be using this acoustic information in general, we might worry that the model predicts behavior that we think is very unlikely to be correct. In other words, we might want our model to 'make sense' even for data we don't have. If this is the case then maybe we need to drop the cross-product from our model because it results in a model that can't possibly capture what listener are doing in general. Below we fit a reduced model without the interaction between f0 and VTL. 

```{r, eval = FALSE}
# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))

model_gender_vtl_f0_reduced =
  brm (F ~ (vtl+f0)*A + ((vtl+f0)*A|L) + (1|S), data=exp_data, 
       chains=4, cores=4, family="bernoulli", 
       warmup=1000, iter = 5000, thin = 4,  
       prior = c(set_prior("student_t(3, 0, 3)", class = "Intercept"),
                 set_prior("student_t(3, 0, 3)", class = "b"),
                 set_prior("student_t(3, 0, 3)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))
```
```{r, include = FALSE, eval = FALSE}
# Or download it from the GitHub page:
model_gender_vtl_f0_reduced = bmmb::get_model ('11_model_gender_vtl_f0_reduced.RDS')
```
```{r, include = FALSE, eval = TRUE}
# saveRDS (model_gender_vtl_f0_reduced, '../models/11_model_gender_vtl_f0_reduced.RDS')
model_gender_vtl_f0_reduced = readRDS ('../models/11_model_gender_vtl_f0_reduced.RDS')
```

And find our $a,b,c$ and $d$ parameters for our model. There is no $d$ parameter this time since there is no cross-product term. 

```{r, collapse = TRUE}
# get fixed effect parameters
samples = brms::fixef (model_gender_vtl_f0_reduced, summary = FALSE)

# get a,b,c coefficients for overall plane
a_all_reduced = mean (samples[,"Intercept"])
b_all_reduced = mean (samples[,"vtl"])
c_all_reduced = mean (samples[,"f0"])

# get a,b,c coefficients for adult plane
a_adult_reduced = mean (samples[,"Intercept"] + samples[,"A1"])
b_adult_reduced = mean (samples[,"vtl"] + samples[,"vtl:A1"])
c_adult_reduced = mean (samples[,"f0"] + samples[,"f0:A1"])

# get a,b,c coefficients for child plane
a_child_reduced = mean (samples[,"Intercept"] - samples[,"A1"])
b_child_reduced = mean (samples[,"vtl"] - samples[,"vtl:A1"])
c_child_reduced = mean (samples[,"f0"] - samples[,"f0:A1"])
```

We use these parameters to plot the linear category boundaries in our stimulus space. Figure \@ref(fug:11-13) compares the linear boundaries implied by our reduced model to those implied by our full model, ignoring the $d$ coefficients. We can see that in the absence of the cross-product, the boundaries for children and adults differ substantially in their slope. The change in slope indicates that f0 differences matter less for children than they do for adults. We can also see that the linear boundaries do not differ much in slope for our model that *does* include a cross-product. This is likely because either the cross-product, or the interaction of this with apparent age, is able to capture whatever difference our reduced model represents with differences in the effect of f0.  

```{r F11-13, fig.width = 8, fig.height = 3, fig.cap = "(left) Each point represents a single speaker, labels indicate most common group classification. Lines indicate male/female boundaries for adults (green), children (orange), and overall (blue) implied by the full model, ignoring the cross-product term. (middle) The same as the left figure but for the reduced model. (right) Territorial maps showing expected classifications for apparent adults in different regions of the f0 by VTL stimulus space, for the reduced model.", echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-13
################################################################################


fixef_1 = brms::fixef (model_gender_vtl_f0)
fixef_2 = brms::fixef (model_gender_vtl_f0_reduced)

# y = (-b*x - a - z) / (dx+c)
# fixef(model_gender_vtl_f0)

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
mu_vtl = mean (tmp$vtl_original)
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
mu_f0 = mean (tmp$f0_original)
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

aggd = aggregate (cbind ( height, A=="a", G=="f", vtl,f0, vtl) ~ S + C_v, 
                      data = tmp, FUN = mean)
aggd$C = ""
aggd$C[aggd[,4]>= 0.5 & aggd[,5]>= 0.5] = "w"
aggd$C[aggd[,4]>= 0.5 & aggd[,5]<= 0.5] = "m"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]>= 0.5] = "g"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]<= 0.5] = "b"
#table(aggd$C)

tab = table (tmp$S, tmp$C)
mod_cat = apply (tab, 1,which.max)

par (mfrow = c(1,3), mar = c(4,.25,.5,.25), oma = c(0,4,0,0))

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",
      ylab="Height (inches)")
grid()
points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = bmmb::cols[c(2:5)][aggd$C])

legend (1,300, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = bmmb::cols[2:5], bty='n',pch=16,pt.cex=1.5)


curve ((-b_all*x - a_all) / (c_all), from = -3, to = 3, add = TRUE,lwd=2,col=bmmb::cols[7])
curve ((-b_adult*x - a_adult)/ (c_adult), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[10])
curve ((-b_child*x - a_child)/ (c_child), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[8])

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()
points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = bmmb::cols[c(2:5)][aggd$C])

legend (1,300, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = bmmb::cols[2:5], bty='n',pch=16,pt.cex=1.5)

curve ((-b_all_reduced*x - a_all_reduced) / (c_all_reduced), from = -3, to = 3, add = TRUE,lwd=2,col=bmmb::cols[7])
curve ((-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[10])
curve ((-b_child_reduced*x - a_child_reduced)/ (c_child_reduced), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[8])


plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()

x = seq(-3,4,0.1)
y = (-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced)
polygon (c(-3,x,4),c(-3,y,-3),col="seagreen")

y = (-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced)
polygon (c(-3,x,1),c(3,y,3),col=2)

points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = 0)

```

### Model selection and misspecification

We can use cross-validation to investigate which of our two models is preferable.  

```{r, eval = FALSE, cache = TRUE}
model_gender_vtl_f0 = brms::add_criterion(model_gender_vtl_f0,"loo")
model_gender_vtl_f0_reduced = brms::add_criterion(model_gender_vtl_f0_reduced,"loo")
```

The comparison suggests that the model that includes the interaction does a better job of explaining our data, but the differences between the two models is not reliable (about two standard errors).

```{r, eval = TRUE}
brms::loo_compare (model_gender_vtl_f0, model_gender_vtl_f0_reduced)
```

However, when adding the `loo` criterion to our models we got the following error message :

`Warning: Found 201 observations with a pareto_k > 0.7 in model 'model_gender_vtl_f0'. It is recommended to set 'moment_match = TRUE' in order to perform moment matching for problematic observations.`

When we add the `loo` criterion to our model, `brms` (and stan) don't actually fit a new model for every left our data point (i.e. a 'real' leave-one-out cross validation). Instead, as discussed in section X , there is a way to approximate what the model would look like it been refit without each data point. This approximation is does using something called **Pareto smoothed importance sampling** (**PSIS**). The Pareto-$\hat{k}$ (`pareto_k`) statistic is a diagnostic for this method that helps you check the assumptions underlying the leave-one-out approximation (Vehtari, Gelman and Gabry, 2017). 
  Essentially, the $\hat{k}$ statistic is a measure of how unusual a given observation is. A very unusual observation is highly influential on your parameter estimates and will have a higher value of $\hat{k}$. If an observation is *too* unusual, then the estimate of $\mathrm{elpd}$ associated with that observation may not be reliable. The general rule of thumb is that values of $\hat{k}$ that are less than 0.5 are 'good', values between 0.5 and 0.7 are 'ok' (not so great but not bad either), values greater than 0.7 are 'bad' and values greater than 1.0 are 'very bad'. Aki Vehtari (cite webpage) outlines three general situations that cause Pareto $\hat{k}$ statistics to be large:

1) p_loo, the 'effective number of parameters' (see section X) is *much smaller* than number of parameters: The model is likely to be **misspecified**. A misspecified model is one whose structure contains important differences compared to the processes being modeled.

2) p_loo is *less than* the number of parameters: If the number of parameters is relatively large relative to the number of observations (e.g. p > n/5) then the model may be too flexible, or your priors may be too weak. 

3) p_loo is *greater than* the number of parameters: The model is likely to be *'badly'* misspecified. There are two situations related to this. 
  
In each case, a posterior predictive check may help understand the issues, as can inspecting the distribution of Pareto $\hat{k}$ statistics. Below, we print the actual and effective number of parameters for our model, as well as our number of observations.

```{r}
# number of estimated parameters
ncol (bmmb::get_samples(model_gender_vtl_f0))

# number of observations
nrow (model_gender_vtl_f0$data)

# information related to loo creiterion
model_gender_vtl_f0$criteria$loo$estimates
```

Based on the information above, we believe that our model falls into the second case. Our `p_loo` is only about 30% as large as our number of parameters. Although we do have more than 5 times as many observations (2085) as estimated parameters (306), the ratio of 6.81 is not much greater than 5. In addition, we *do* think there is a possibility that the model is *misspecified*. We have said repeatedly that no model can really 'prove' itself to be the 'real' model, and even that it's not totally clear what it would mean for a formalism like regression to represent the 'real' process underlying our observations. Given this, it seems that every model is 'misspecified' to some extent, making the critique of one specific model model as being misspecified somewhat odd. 
  However, we can say that in this context, misspecification refers to a difference between your model and reality that causes noticeable 'misbehavior' in your model. What might this misbehavior look like? We think an excellent example of this 'odd' behavior is in our hyperbolic category boundaries seen in figure \@ref(fig:F11-12). We noted at the time that these boundaries defied logic and were unlikely to represent listener behavior in this area of the parameter space. In other words, the model did not correctly reflect the underlying process we are trying to understand: Listener judgments of apparent gender based on speech acoustics and apparent speaker age.
  To investigate the Pareto $\hat{k}$ estimates for our initial model, we get these using the code below:

```{r, cache = TRUE}
pareto_k = model_gender_vtl_f0$criteria$loo$diagnostics$pareto_k
```

We plot these in figure \@ref(fig:F11-14), and compare them to the same values for our reduced model. When plotting these, we noted that there was a pattern such that $\hat{k}$ values were highest for adult male speakers.  

```{r F11-14, fig.width = 8, fig.height = 3, fig.cap = "(left) Pareto k estimates for data points in `model_gender_vtl_f0`. (right) Pareto k estimates for data points in `model_gender_vtl_f0_reduced`.", echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-14
################################################################################
point_col = tapply (exp_data$C_v, exp_data$S,findmode)
point_col = as.numeric(factor(point_col))

par (mfrow = c(1,2), mar = c(1,.5,1,.5), oma = c(0,4,0,0))

plot (model_gender_vtl_f0$criteria$loo$diagnostics$pareto_k, ylab="", 
      ylim = c(0,1.5),col=bmmb::cols[1+point_col],pch=16, xaxt='n')
abline (h = c(0.5,.7,1), col = deeppurple, lwd = 2, lty=2)
plot (model_gender_vtl_f0_reduced$criteria$loo$diagnostics$pareto_k, ylab="", 
      ylim = c(0,1.5),col=bmmb::cols[1+point_col],pch=16, yaxt='n', xaxt='n')
abline (h = c(0.5,.7,1), col = deeppurple, lwd = 2, lty=2)
mtext (side = 2, "Pareto k", outer = TRUE, line= 2.5, cex = 1.3)

legend (10, 1.4, legend=(c("boy","girl","man","woman")), col = bmmb::cols[2:5],pch=16,
        horiz = TRUE, bty = 'n',x.intersp = .8)
```

We made posterior predictions for each model, taking the average across all posterior samples: 

```{r, cache = TRUE}
p_preds = predict (model_gender_vtl_f0)
```

And plot these in the left plot of figure \@ref(fig:F11-15). It's clear that the main issue seems to be for males who are predicted to be female 0%, or nearly 0%, of the time. In fact, several men were identified as women in 0% of cases. Since a probability of 0 implies, in the limit, a logit of negative infinity, the model appears to have a hard time finding reasonable values for some parameters in these cases. We considered a  boxplot of Pareto k values by speaker (not presented here) which showed that a small number of individual men were responsible for most high k values. We found that several of these men had the lowest f0 values from among our speakers. In the middle plot we present each speaker plotted according to their f0 and VTL, where point size reflects the average Pareto $\hat{k}$ value for each speaker.

```{r F11-15, fig.width = 8, fig.height = 3, fig.cap = "(left) Pareto k estimates for data points in `model_gender_vtl_f0` plotted against the predicted probability of a female response for each data point. (middle) Adult male and female speakers plotted according to voice characteristics. Point size reflects Pareto k values. (right) A 'topographic map' of our predicted surface for adult speakers. The same points as in the middle figure. Black lines indicate female/male category boundaries (i.e. logit = 0). Red lines indicate 2-logit decreases in expected values, green lines indicate 2-logit increases in expected values.", echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-15
################################################################################

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
mu_vtl = mean (tmp$vtl_original)
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
mu_f0 = mean (tmp$f0_original)
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

point_col = tapply (tmp$C_v, tmp$S,findmode)
point_col = as.numeric(factor(point_col))

par (mfrow = c(1,3), mar = c(4,4,1,1))

plot (p_preds[,1], model_gender_vtl_f0$criteria$loo$diagnostics$pareto_k,
      col=bmmb::cols[1+point_col],pch=16, ylim = c(0,1.5),
      xlab="Expected Probbility of a Female Response",ylab="Pareto k")

aggd = aggregate (pareto_k ~ f0 + vtl + S + A_v, data = tmp, FUN = mean)
aggd = aggd[aggd$A_v=="a",]

tmp = tmp[tmp$A_v=="a",]
point_col = tapply (tmp$C_v, tmp$S,findmode)
point_col = as.numeric(factor(point_col))+2

plot(aggd[,2:1], cex = aggd$pareto_k*4, col = cols[1+point_col], pch=16,lwd=2,
     xlab = "Vocal-tract length (cm)", xlim = c(-2.2,3), 
     ylim = c(-1.2,1.1), ylab = "f0 (Hz)")

curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -6, 
       to = -c_adult/d_adult-.01, add = FALSE, lwd=2, lty=1,col=1, xlim = c(-2.2,3), ylim = c(-1.2,1.1))
curve ((-b_adult*x - a_adult) / (d_adult*x+c_adult), from = -c_adult/d_adult+0.01, 
     to = 6, add = TRUE, lwd=2, lty=1,col=1)

colors = colorRampPalette(c("blue",bmmb::cols[4],bmmb::darkorange))(13)
for (i in seq(-62,-2,2)){
  curve ((-b_adult*x - a_adult - i) / (d_adult*x+c_adult), from = -6, 
         to = -c_adult/d_adult-0.01, add = TRUE, lwd=1, lty=1,col=bmmb::cols[5])
  curve ((-b_adult*x - a_adult - i) / (d_adult*x+c_adult), from = -c_adult/d_adult+0.01, 
       to = 6, add = TRUE, lwd=1, lty=1,col=bmmb::cols[5])
}

for (i in seq(2,62,2)){
  curve ((-b_adult*x - a_adult - i) / (d_adult*x+c_adult), from = -6, 
         to = -c_adult/d_adult-0.01, add = TRUE, lwd=1, lty=1,col=bmmb::cols[4])
  curve ((-b_adult*x - a_adult - i) / (d_adult*x+c_adult), from = -c_adult/d_adult+0.01, 
       to = 6, add = TRUE, lwd=1, lty=1,col=bmmb::cols[4])
  
}
points(aggd[,2:1], cex = 1, col = "grey40", pch=16,lwd=2)
```

Based on figure \@ref(fig:F11-15), we can see that the problematic male voices are those with extreme values of VTL and, in particular f0. If we think of our saddle shapes (see figure \@ref(fig:F11-10)), they can have areas where $z$ values (i.e., the dependent variable) can rise or fall extremely rapidly. For example, we can see in the right plot of figure \@ref(fig:F11-15) that the most problematic male voices were in a region that includes very high values that rise rapidly based on small differences in f0 and VTL. So, it appears that in this case we have some male speakers whose probability of being identified as female is very low, implying very negative logit values, in a region of our surface that is curved such that extremely negative predicted values are possible. Combining this with or relatively small number of observations (given the complexity/flexibility of our model) results in the very high $\hat{k}$ values seen above.
  To resolve this situation we could fit a model with tighter priors, which may have the result of providing more constrained parameter estimates. However, it may be the case that our model is simply too flexible given the amount of data we have. Independently of all of this, we are still concerned with the very basic problem that the high-frequency region associated with male responses (presented in figure \@ref(fig:F11-12)) defies common sense and is probably wrong. As a result of this, we're not going to try to 'fix' the more complicated model but simply abandon it, relying instead on the reduced model that does not include the VTL by f0 interaction. 
  
### Answering our research questions

As a reminder, the questions we posed above in section X are: 

Q1) Do listeners use VTL, f0, and the interaction between them to determine speaker gender?

Q2) Does apparent speaker age influence the use of the above mentioned acoustic cues?

Q3) Is there a lot of between-speaker variation in perceptual behavior?

Based on our discussion above, we conclude that listeners use f0 and VTL, but the jury is still out on the interaction of the two. It seems that to really get a handle on the higher-frequency male region, we would need to run an experiment with more voices in the lower left and upper right corners of our stimulus space as arranged in the right plot of figure \@ref(fig:F11-15). Below we print a table of the fixed effects for our reduced model:

```{r T11-1}
knitr::kable(fixef(model_gender_vtl_f0_reduced), caption = "Fixed effect posterior means, error, and credible intervals for our reduced model.")
```

It seems that apparent speaker age appears to affect the use of both VTL and f0. In both cases, the effects of f0 and VTL changes are stronger for children compared to adults. In fact, the interaction between f0 and apparent age (`f0:A1`) has the effect of reducing the effect of f0 to nearly zero for adults (i.e. $f0 + (-A1) = 2.2 - 1.6 = 0.6$). We noted in chapter 10 that slopes of larger magnitudes generally indicate steeper category boundaries and less uncertainty between categories along that dimension. In this case, the larger magnitudes of the slopes for apparent adults for f0 and VTL likely reflect the fact that men and women were more discriminable along those dimensions than boys and girls. 
  We can think about between-listener variation in the use of acoustics in two ways. The first involves the inspection of the superpopulation standard deviation estimates for our listener-dependent effects, presented in table \@ref(tab:T11-2). Rather than only focusing on the estimates, we can also think about whether effects have 2.5% credible intervals (relatively) far from zero, meaning they are unlikely to be very small. Only three of the effects really meet that threshold: Listener-dependent variation in the use of VTL ($\sigma_{VTL \colon L}$), and the effect of apparent age ($\sigma_{A1 \colon L}$), and speaker-dependent variation in intercepts ($\sigma_{S}$). 

```{r T11-2}
knitr::kable(bmmb::getsds (model_gender_vtl_f0_reduced), caption = "Superpopulation standard deviation estimates for our 'random' effects.")
```

We can also think about the result of this variation in terms of how this might actually affect gender classification from speech. To do this, we get listener-specific intercepts, f0 slopes, and VTL slopes. We do this separately for apparent children and apparent adults using the code below (explained in detail in section X). 

```{r}
listener_coefficients_adult = 
  bmmb::short_hypothesis (model_gender_vtl_f0_reduced, scope="coef",group="L",
                          c("Intercept+A1=0","vtl+vtl:A1=0","f0+f0:A1=0"))
listener_coefficients_child = 
  bmmb::short_hypothesis (model_gender_vtl_f0_reduced, scope="coef",group="L",
                          c("Intercept-A1=0","vtl-vtl:A1=0","f0-f0:A1=0"))
```

Using these parameters, we can find boundaries for each listener using the equation above. We plot these in figure \@ref(fig:F11-16). We see that there is a general consensus between listeners in terms of an approximate acoustic boundary between adult males and adult females. However, there is much more variation in the boundary between boys and girls. This likely reflects the fact that the acoustic characteristics of adult men and women are generally more predictable and stable relative to children 10-12 years old. In addition, the listeners in our experiment (undergraduate University students) likely have substantially more recent experience interacting with adults than with children in that age group.   
  
```{r F11-16, fig.width = 8, fig.height = 5, fig.cap = "The first plot compares all category boundaries between men/women (orange) and boys/girls (blue). Subsequent plots present each listener's individual boundaries, and point colors indicate the listener's individual speaker classifications.", echo = FALSE, cache = FALSE}

################################################################################
### Figure 11-16
################################################################################

# y = (-b*x - a - z) / (dx+c)
# fixef(model_gender_vtl_f0)

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

aggd = aggregate (cbind ( height, A=="a", G=="f", vtl,f0, vtl) ~ S + A_v + G_v + C_v, 
                      data = tmp, FUN = mean)
#aggd = aggd[aggd$A_v=='a',]

a = listener_coefficients_adult[1:15,1]
b = listener_coefficients_adult[16:30,1]
c = listener_coefficients_adult[31:45,1]

ac = listener_coefficients_child[1:15,1]
bc = listener_coefficients_child[16:30,1]
cc = listener_coefficients_child[31:45,1]

par (mar = c(.2,.2,.2,.2), oma = c(3,3,.5,.5), mfrow = c(4,4))

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C_v)], 
      pch=16,lwd=2, xlab = "",ylab="f0 (cm)",xaxt = 'n',
        xlim=c(-2.5,2.7),ylim=c(-1.2,1.2))
grid()

for (i in 1:15)
  curve ((-b[i]*x - a[i]) / (c[i]), from = -3, to = 3, 
         add = TRUE,lwd=2,col=bmmb::cols[8],xaxt='n')

for (i in 1:15)
  curve ((-bc[i]*x - ac[i]) / (cc[i]), from = -3, to = 3, 
         add = TRUE,lwd=2,col=bmmb::cols[7],xaxt='n')

for (i in 1:15){
  xaxt = "n"
  yaxt = "n"
  if (i %in% 12:15) xaxt = "s"
  if (i %in% c(4,8,12)) yaxt = "s"
  tmpp = tmp[tmp$L==i,]
  plot (tmpp$vtl,tmpp$f0, cex =1.2, col = bmmb::cols[2:5][factor(tmpp$C)],
        pch=16,lwd=2, xlab = "",ylab="f0 (cm)",xaxt=xaxt,yaxt=yaxt,
        xlim=c(-2.5,2.7),ylim=c(-1.2,1.2))
  grid()
  curve ((-b[i]*x - a[i]) / (c[i]), from = -5, to = 3, add = TRUE,
         lwd=2,col=bmmb::cols[8])
  curve ((-bc[i]*x - ac[i]) / (cc[i]), from = -5, to = 3, add = TRUE,
         lwd=2,col=bmmb::cols[7])
  text (-1.5,-.75,i,cex=1.5)
}
```












