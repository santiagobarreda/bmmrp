\newpage
```{r}
knitr::opts_chunk$set(
  dpi = 300, dev = "jpeg", collapse=TRUE
)
```

# Multinomial and Ordinal regression

In chapter 10 we introduced models that can predict dichotomous categorical variables using logistic regression. Here, we extend many of the concepts introduced in that chapter to the modelling of dependent variables with any number of categories. First, we introduce ordinal models appropriate for the prediction of categorical variables with some inherent meaning. A simple example of this is first, second, and third place in a race. These values are categorical and not numerical. For example fourth place is not 'double' anything with respect to second place (i.e., $2 \times second \neq fourth$). However, there is clearly an inherent ordering in the categories such that first > second > third > fourth > and so on. A common experimental example of ordered categorical data arises from survey data that asks listeners to respond to questions using a small number of discrete, categorical choices (e.g., a scale from one to five). The reason this is not a truly quantitative variable is because there are a limited number of dicrete choices with nothing 'in between', and it is not necessarily the case that, for example, the second choice implied double the quantity of the second. 
  After discussing ordinal data, we will fit multinomial models that can be used for categorical variables *without* an inherent ordering. For example, we could use a model like this to perceive the perception of vowel sounds from speech acoustics, to predict the lexical category of a word (verb, noun, adjective, ...), or to predict a speaker's native language. Actually, these models can *also* be used for ordered data, they just do not inherently represent the ordering in the model. 

## Multinomial logistic regression

**Multinomial logistic regression** allows you to model data generated by a **multinomial distribution** with some unknown parameters. The multinomial distribution is the generalization of the binomial distribution (see section X) to any number of positive integer outcome categories. For example, the multinomial distribution will allow use to model the categorization of speakers into boys, girls, men, and women simultaneously rather than modeling this as two binary categorizations (male vs. female and adult vs. child). The multinomial distribution has three parameters:
  
  1) $n$: The number of trials, a positive integer.
  2) $J$: The number of possible outcomes, a positive integer.
  3) $p_1, \dots, p_J$: A vector of probabilities of observing each of the $j$ outcomes where $\sum p_j = 1$.
  
  Multinomial data arises in situations such as the following. You have a variable that can take on some number of possible values. For example, you ask people to tell you if the speaker seems to be a boy, a girl, a man, or a woman. This results in a variable, apparent speaker category, that can take on one of four discrete values. You observe a certain number of trials ($n$) under some set of conditions. You then count how many times listeners identified the speaker as belonging to one of the four categories. If you divide these counts by the total number of trials ($n$) you will get the probability of observing each outcome ($p_J$). We can think of our data as being generated by a multinomial distribution with some specific parameters as seen below. Our data is a vector of counts, $y_1, \dots, y_J$, representing the expected number of observations of each category for $n$ total outcomes. 

$$
y_1, \dots, y_J \sim \mathrm{multinomial}(p_1, \dots, p_J, n)
$$

  Below we draw 10 instances of a four-category multinomial variable with probabilities of 0.4, 0.1, 0.2, and 0.3 for the first, second, third and fourth categories respectively. Each of the variables below has $n=1$ so a single category is equal to 1 and the others must all be zero. 
  
```{r}
rmultinom (10, 1, c(.4,.1,.2,.3))
```

We sample ten more multinomial variables, this time each with $n=100$. We can see that we get a distribution of values across the four outcomes for each variable that resembles the values of $p$ we defined for each outcome. Note that the categories are numbered and could represent anything. This sort of variable only represents the relative frequency of outcomes with respect to each other and does not specify relationships between outcomes.

```{r}
set.seed (1)
multinomial_variable = rmultinom (10, 100, c(.4,.1,.2,.3))
multinomial_variable

rowMeans(multinomial_variable)/100
```

There are many ways to think about multinomial logistic regression. We'are going to present one that is consistent with the way we presented logistic regression in chapter 10 and the way we have been presenting regression models in general in this book. For a more complete treatment of the topic please see cite. Before beginning the explanation we can 'spoil' the conclusion:  The number of trials ($n$) and the number of possible outcomes ($J$) are aspects of your data and experimental design, and are known to you before you fit any model. Thus, multinomial logistic regression effectively consists of estimating $p_j$, or values analogous to it, for each category as a function of your dependent variables. 

Multinomial regression is a surprisingly 'simple' extension of the concepts underlying logistic regression (presented in chapter 10). When we discussed logistic regression in chapter 10, we introduced the antilogit function (also known as the *logistic* link function). In section X, we discussed the fact that this function converts log odds to probabilities by arbitrarily setting the 'number' of failures to 1 and modeling the number of outcomes (i.e. $e^z$). So, we see in equation below that the probability that $y=1$ (i.e. a 'success') is equal to the ratio of the 'number' of successes over the sum of the total number of outcomes.  

$$
\begin{equation}
\begin{split}
P(Y=1) = \frac{\mathrm{success}}{\mathrm{failure}+\mathrm{success}} = \frac{e^{z}}{1+e^{z}}
\end{split}
(\#eq:12-a)
\end{equation}
$$

The equation above works for when we have exactly two categories. However, it can be modified so that it can be applied to multiple categories. First, we assume that rather than exactly two possible outcomes, there are $J$, some integer larger than one. If we stick to our interpretation of the value of $e^{z}$ as an expected count for that category, then $e^{z_j}$ is the expected count for category $j$. To find the probability of observing category $j$ we find the ratio of the 'count' of $j$ over the sum of all possible outcomes (i.e., the sum of counts for all categories).

$$
\begin{equation}
\begin{split}
P(Y=j) = \frac{e^{z_j}}{\sum_{j=1}^{J} e^{z_j}}
\end{split}
(\#eq:12-a)
\end{equation}
$$

The function above is called the **softmax** function, and it is basically a generalization of the antilogit (logistic) function to more than two categories. When we did dichotomous logistic regression, we modeled only the 'count' of one variable, the one set to 'success'. The 'counts' for the category set to 'failure' was not modeled and was set to 1 (by setting $z=0$) for all cases. When we model multinomial data we have to follow the same convention and set one category as the 'reference'. This means the the equation above can be modified to pull one category out of the summation and set its count to 1 for all situations, as in the equation below. Notice that the count on the summation in the denominator of the fraction now begins at two.

$$
\begin{equation}
\begin{split}
P(Y=j) = \frac{e^{z_j}}{1 + \sum_{j=2}^{J} e^{z_j}}
\end{split}
(\#eq:12-a)
\end{equation}
$$

The equation above results in a set of probabilities which can serve as the multinomial parameters, $p_1, \dots , p_j$, for categories $1, \dots, J$. These probabilities are modeled by estimating $z_j$ as resulting from the linear combination of our predictor variables based on some unknown parameters ($\beta_j$). Each outcome has a different prediction equation for $z_j$ so that a multinomial regression has $J$ prediction equations, one for each outcome. Importantly, each prediction equation combines the same $x_k$ predictors, albeit in a category-specific way (based on the $\beta_j$ parameters for that category).  

$$
\begin{equation}
z_j = \beta_j + \beta_{j1} \times x_1 + \beta_{j2} \times x_2 + \dots + \beta_{kj} \times x_k
(\#eq:12-b)
\end{equation}
$$

The prediction equation for the 'reference' category (`brm` uses the first category) is fixed to have a value of 0. One way to accomplish this is to fix all coefficients for this outcome to equal 0 as in equation below. 

$$
\begin{equation}
z_1 = 0 \\
0 = \alpha_j + \beta_{j1} \times x_1 + \beta_{j2} \times x_2 + \dots + \beta_{kj} \times x_k \\
z_1 = 0 + 0 \times x_1 + 0 \times x_2 + \dots + 0 \times x_k \\
(\#eq:12-b)
\end{equation}
$$

At this point we have laid out the basics of multinomial regression and can present a summary of the information we just presented. In multinomial logistic regression we model the probability of observing each of the $J$ categorical outcomes. We fix the value of the score (or multinomial logit) of the reference category to 1, and estimate the scores of the other categories using a prediction equation specific for the category. The logit predicted for each category in a specific situation can then be converted to a probability using the softmax function presented in equation above. 

### Data and research questions

We load our packages and data:

```{r, warning=FALSE, message=FALSE}
library (brms)
library (bmmb)
data (exp_data)
```

We're going to add a new variable, $y$, representing our multinomial outcome. This variable is a vector of length four whose first, second, third, and fourth elements represent observed outcomes of 'boy', 'girl', 'man', and 'woman', respectively. We also print out the first six instances of this varialbe and compare this to the first six values of the `C` (category) variable in our data frame. 

```{r}
exp_data$y = cbind(b = as.numeric(exp_data$C=='b'),
                   g = as.numeric(exp_data$C=='g'),
                   m = as.numeric(exp_data$C=='m'),
                   w = as.numeric(exp_data$C=='w'))

head (exp_data$y)
head (exp_data$C)
```

We also add a variable called `size` that always equals 1 because each row in our data frame represents observation of a single outcome. 

```{r}
exp_data$size = 1
```

We will process our continuous predictors in the same way as in the preivous chapters: 

```{r}
exp_data$size = 1
exp_data$vtl_original = exp_data$vtl
exp_data$vtl = exp_data$vtl - mean (exp_data$vtl)

exp_data$f0_original = exp_data$f0 
exp_data$f0 = exp_data$f0 - mean(exp_data$f0)
exp_data$f0 = exp_data$f0 / 100
```

And use these to try to answer the following research questions: 

Q1) Can we use speaker f0 and VTL to predict their apparent speaker category?

Q2) Can the f0 by VTL space be divided into sections associated with the different categorizations (i.e. in a territorial map) in a useful way?

### Description of our model

Our model formula is largely similar to those we have seen before, with one minor change. Beside your dependent variable, you need to include a variable that indicates the integer number of trials for each set of observations. For us this will always be 1, but in many situations this can be a wider range of values. We want to predict our counts of categorizations with respect to value of speaker VTL and f0, and so the model formula we are going to use is seen below. 

`y|trials(size) ~ vtl+f0 + (vtl+f0|S) + (1|L)`

Since we're predicting category membership with two quantitative predictors, we know that the surfaces our model defines are planes. In the models with fit to this point, we had a single dependent variable and a single plane for a single condition. So, our model formulas previously were something like:

`y ~ vtl+f0 + (vtl+f0|S) + (1|L)`

However, in a multinomial regression we have one plane for each response category, so our formula above could really be thought of as:

`y_1 ~ vtl+f0 + (vtl+f0|S) + (1|L)`
`y_2 ~ vtl+f0 + (vtl+f0|S) + (1|L)`
`y_3 ~ vtl+f0 + (vtl+f0|S) + (1|L)`
`y_4 ~ vtl+f0 + (vtl+f0|S) + (1|L)`

This means that our multinomial model will involve four times as many coefficients as an equivalent Gaussian model, and $J$ times as many for $J$ outcome categories. Of course, we noted above that the coefficients of the reference category are set to zero and not estimated. This means that the formula above results in the estimation of three planes, resulting in three sets of analogous parameters. Our full model specification is given below:

$$
\begin{equation}
\begin{split}
y_{1[i]},y_{2[i]},y_{3[i]},y_{4[i]} \sim \mathrm{multinomial}(p_{1[i]},p_{2[i]},p_{3[i]},p_{4[i]}, n_{[i]}) \\ \\
\mathrm{for} \; j = 1, \dots, 4\\ \\ 
p_{j[i]} = \frac{e^{z_j}}{\sum_{j=1}^{J} e^{z_j}} \\ 
\\
z_{j[i]} = \mathrm{a}_j + b_{j[i]} \times \mathrm{vtl}_{[i]} + c_{j[i]} \times \mathrm{f0}_{[i]}  \\ 
a_{j[i]} = \mathrm{Intercept}_j + L_{j[L_{[i]}]} + S_{j[S_{[i]}]} \\ 
b_{j[i]} =  VTL_j + VTL_j \colon L_{j[L_{[i]}]} \\
c_{j[i]} =  f0_j + f0_j \colon L_{j[L_{[i]}]} \\ 
\\
\mathrm{for} \; j = 2, \dots, 4\\ \\ 

\textrm{Priors:} \\
S_{j[\bullet]} \sim \mathrm{Normal}(0,\sigma_{S_j}) 
\\ 
\begin{bmatrix} L_{j[\bullet]} \\ VTL_j \colon L_{j[\bullet]} \\ f0_j \colon L_{j[\bullet]} \end{bmatrix}	
\sim \mathrm{MVNormal} \left(\, \begin{bmatrix} 0\\ 0 \\ 0 \\ \end{bmatrix}, \Sigma_j \right) \\ \\
Intercept_j \sim t(3, 0, 3) \\
VTL_j, f0_j \sim t(3, 0, 3) \\
\sigma_{L_j}, \sigma_{VTL_j \colon L_j}, \sigma_{f0_j \colon L_j} \sim t(3, 0, 3) \\ R_j \sim \mathrm{LKJCorr} (2)

\end{split}
(\#eq:12-1)
\end{equation}
$$

We're going to take some time to unpack this definition, top to bottom, because it is at the same time very familiar and very different. First, note that our data-generating distribution is assumed to be the multinomial distribution. This function has a $p_j$ parameter indicating the probability of observing each of the $J$ categories, and an $n$ parameter specifying the total number of trials for that observation. The next line indicates that the next few lines applies individually for all four categories. For each category, the probability of observing that outcome ($p_j$) is found by combining the score for each category ($z_j$) using the softmax link function. For each category, the score is equal to a trial-dependent combination of an intercept, an effect fot VTL, and an effect for f0. The intercept varies according to an overall model intercept and speaker and listener-dependent variations from this. The VTL and f0 effect vary according to listener-dependent effects. 

After that we move on to the model priors. Unlike for the above lines, we do not specify priors for any parameters related to the reference category. This is because all of these parameters are set to 0 for this category so that the score for this category is always 0 and the 'expected count' (as discussed above) is always equal to 1. Again we may note that the specification of priors is very similar to the models we have discussed to this point, save for the proleferation of $j$ subscripts. 

### Fitting and interpreting our models

There is one major difference in how we need to specify priors for multinomial models: We need to specify priors individually for each response category with modeled parameters. This is done by passing the name of the categorical variable to the `dpar` parameter. Above, we named our response variables according to the letters we have been using throughout this text. We can see this below.

```{r}
colnames (exp_data$y)
```

The name passed to `dpar` will be `muCategory` where `Category` corresponds to the category name. This means we need to specify priors for `mug`, `mum`, and `muw`, but not `mub`. We specify our priors below:

```{r}
multinomial_prior = 
  c(brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="muw"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="muw"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="muw"),
    brms::set_prior("lkj_corr_cholesky (2)", class = "cor"))
```

And here is the code to fit our model, using the `multinomial()` family for the first time: 

```{r, eval = FALSE}
model_multinomial = 
  brms::brm (y|trials(size) ~ vtl+f0 + (vtl+f0|L) + (1|S), data=exp_data, 
             family=multinomial(), chains=4, cores=4, warmup=1000, iter = 5000, 
             thin = 4, prior = multinomial_prior)
```
```{r}
model_multinomial = bmmb::get_model ("12_model_multinomial.RDS")
```
```{r}
#  saveRDS (model_multinomial, "../../models/12_model_multinomial.RDS")
model_multinomial = readRDS ("../../models/12_model_multinomial.RDS")
```

Below we plot the model fixed effects, the intercept, VTL slope, and the f0 slope for each category with estimated parameters. Each group of parameters defines a plane whose value along the $z$ dimension can be predicted based on the values of f0 and VTL. We have four planes an four values of $z_j$ for any given location in the two dimensional space defined by f0 and VTL. For each point, we can select as the most probable category the one whose value of $z_j$ is highest at the point in the space. Another way to look at this is that we select the highest plane at any given location in the space. The intersection between two planes forms the category between them, and this boundary will be along straight lines as long as as our planes are planes (i.e. as long as no cross product terms are involved).

```{r}
par(mar=c(8,4,1,1))
brmplot (fixef (model_multinomial)[c(1,4,5,2,6,7,3,8,9),], las=2)
```

- find boundaries and compare to data
- show they dont make sense
- show intercepts and explain what we think is going on
- refit model without intercepts
-explain downsides to both approaches, no perfect solution
- compare intervals, compare loo?

$$
\begin{equation}
\begin{split}

\end{split}
(\#eq:12-2)
\end{equation}
$$


```{r}
# y = (-bx + ex - a + d + z) / (c-f)
```



```{r}

################################################################################
### Figure 12.X
###############################################################################


find_intersection = function (coeffs1, coeffs2, z = 0){

  # y = (-bx + ex - a + d + z) / (c-f)
  # y =(- a + d + z) / (c-f) + (-bx + ex)/ (c-f) 
  # intercept =(- a + d + z) / (c-f) 
  # slope = (-bx + ex)/ (c-f) 
  
  if (length(coeffs1) == 3 & length(coeffs1) == 3){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[3]-coeffs2[3]) 
    slope = (-coeffs1[2] + coeffs2[2]) / (coeffs1[3]-coeffs2[3]) 
    return (c(intercept=intercept, slope=slope))
  }
  if (length(coeffs1) == 2 & length(coeffs1) == 2){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[2]-coeffs2[2]) 
    return (intercept=intercept)
  }
}

params = fixef (model_multinomial)
params = fixef (model_multinomial, summary = FALSE)
head (params)
params = cbind (colMeans (params[,1:3]), 
                colMeans (params[,c(4,6,8)]),
                colMeans (params[,c(5,7,9)]))

params = rbind (c(0,0,0), params[1:3,])

tab = table (exp_data$S, exp_data$C)
mod_cat = apply (tab, 1,which.max)

cats = apply(multi_pred[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat2 = apply (tab2, 1,which.max)

cats = apply(multi_pred_re[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat3 = apply (tab2, 1,which.max)

par (mfrow = c(2,2), mar = c(4,4,1,1))

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[1,], params[2,])) # b vs g
abline (find_intersection (params[1,], params[3,])) # b vs m
abline (find_intersection (params[1,], params[4,])) # b vs w

abline (find_intersection (params[2,], params[3,])) # g vs m
abline (find_intersection (params[2,], params[4,])) # g vs w

abline (find_intersection (params[3,], params[4,])) # m vs w

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[3,], params[4,])) # m vs w

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[1,], params[2,])) # b vs g

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[1,], params[4,])) # b vs w
```



```{r}

################################################################################
### Figure 12.X
###############################################################################

find_intersection = function (coeffs1, coeffs2, z = 0){

  # y = (-bx + ex - a + d + z) / (c-f)
  # y =(- a + d + z) / (c-f) + (-bx + ex)/ (c-f) 
  # intercept =(- a + d + z) / (c-f) 
  # slope = (-bx + ex)/ (c-f) 
  
  if (length(coeffs1) == 3 & length(coeffs1) == 3){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[3]-coeffs2[3]) 
    slope = (-coeffs1[2] + coeffs2[2]) / (coeffs1[3]-coeffs2[3]) 
    return (c(intercept=intercept, slope=slope))
  }
  if (length(coeffs1) == 2 & length(coeffs1) == 2){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[2]-coeffs2[2]) 
    return (intercept=intercept)
  }
}

params = fixef (model_multinomial)
params = fixef (model_multinomial, summary = FALSE)
head (params)
params = cbind (colMeans (params[,1:3]), 
                colMeans (params[,c(4,6,8)]),
                colMeans (params[,c(5,7,9)]))

params = rbind (c(0,0,0), params[1:3,])

tab = table (exp_data$S, exp_data$C)
mod_cat = apply (tab, 1,which.max)

cats = apply(multi_pred[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat2 = apply (tab2, 1,which.max)

cats = apply(multi_pred_re[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat3 = apply (tab2, 1,which.max)

par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat2])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat3])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

```


```{r}
model_multinomial = brms::add_criterion(model_multinomial,"loo")
```

```{r, cache = TRUE}
multi_pred = predict (model_multinomial, re_formula = NA)
multi_pred_re = predict (model_multinomial)
```


```{r}
table (apply(multi_pred[,1,],1,which.max))
table (exp_data$C)
```


```{r}

sranef = ranef(model_multinomial)$S

par (mfrow = c(4,1), mar =c(1,4,.5,.5))
brmplot((sranef[,,1]+sranef[,,2]+sranef[,,3])/3)
brmplot(sranef[,,1])
brmplot(sranef[,,2])
brmplot(sranef[,,3])

```


```{r}

################################################################################
### Figure 12.X
###############################################################################

pt_size = sranef[,1,3]
pt_size = 1 + (pt_size - min(pt_size))/3

plot(model_multinomial$data$vtl,model_multinomial$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 1, col = bmmb::cols[2:5][mod_cat],
     cex = pt_size)


```



```{r}
multinomial_prior = 
  c(brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="mug"),
    brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="mum"),
    brms::set_prior("student_t(3, 0, 3)", class = "Intercept",dpar="muw"),
    brms::set_prior("student_t(3, 0, 3)", class = "b",dpar="muw"),
    brms::set_prior("student_t(3, 0, 3)", class = "sd",dpar="muw"),
    brms::set_prior("lkj_corr_cholesky (2)", class = "cor"))
```

And here is the code to fit our model, using the `multinomial()` family for the first time: 

```{r, eval = FALSE}
model_multinomial_noS = 
  brms::brm (y|trials(size) ~ vtl+f0 + (vtl+f0|L), data=exp_data, 
             family=multinomial(), chains=4, cores=4, warmup=1000, iter = 5000, 
             thin = 4, prior = multinomial_prior)
```
```{r}
#  saveRDS (model_multinomial_noS, "../../models/12_model_multinomial_noS.RDS")
model_multinomial_noS = readRDS ("../../models/12_model_multinomial_noS.RDS")
```



```{r, cache = TRUE}
multi_pred_noS = predict (model_multinomial_noS, re_formula = NA)
multi_pred_re_noS = predict (model_multinomial_noS)
```


```{r}
mean (apply(multi_pred[,1,],1,which.max) == as.numeric(as.factor(exp_data$C)))
mean (apply(multi_pred_re[,1,],1,which.max) == as.numeric(as.factor(exp_data$C)))

mean (apply(multi_pred_noS[,1,],1,which.max) == as.numeric(as.factor(exp_data$C)))
mean (apply(multi_pred_re_noS[,1,],1,which.max) == as.numeric(as.factor(exp_data$C)))
```



```{r}

################################################################################
### Figure 12.X
###############################################################################

find_intersection = function (coeffs1, coeffs2, z = 0){

  # y = (-bx + ex - a + d + z) / (c-f)
  # y =(- a + d + z) / (c-f) + (-bx + ex)/ (c-f) 
  # intercept =(- a + d + z) / (c-f) 
  # slope = (-bx + ex)/ (c-f) 
  
  if (length(coeffs1) == 3 & length(coeffs1) == 3){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[3]-coeffs2[3]) 
    slope = (-coeffs1[2] + coeffs2[2]) / (coeffs1[3]-coeffs2[3]) 
    return (c(intercept=intercept, slope=slope))
  }
  if (length(coeffs1) == 2 & length(coeffs1) == 2){
    intercept =(- coeffs1[1] + coeffs2[1] + z) / (coeffs1[2]-coeffs2[2]) 
    return (intercept=intercept)
  }
}

params = fixef (model_multinomial_noS)
params = fixef (model_multinomial_noS, summary = FALSE)
head (params)
params = cbind (colMeans (params[,1:3]), 
                colMeans (params[,c(4,6,8)]),
                colMeans (params[,c(5,7,9)]))

params = rbind (c(0,0,0), params[1:3,])

tab = table (exp_data$S, exp_data$C)
mod_cat = apply (tab, 1,which.max)

cats = apply(multi_pred_noS[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat2 = apply (tab2, 1,which.max)

cats = apply(multi_pred_re_noS[,1,],1,which.max)
table (exp_data$C, cats)

tab2 = table (exp_data$S, cats)
mod_cat3 = apply (tab2, 1,which.max)

par (mfrow = c(1,3), mar = c(4,4,1,1))

plot(model_multinomial_noS$data$vtl,model_multinomial_noS$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

plot(model_multinomial_noS$data$vtl,model_multinomial_noS$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat2])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

plot(model_multinomial_noS$data$vtl,model_multinomial_noS$data$f0, ylim = c(-1.5,1.5), 
     xlim = c(-3,3), pch = 16, col = bmmb::cols[2:5][mod_cat3])
abline (find_intersection (params[1,], params[2,]))
abline (find_intersection (params[1,], params[3,]))
abline (find_intersection (params[1,], params[4,]))

abline (find_intersection (params[2,], params[3,]))
abline (find_intersection (params[2,], params[4,]))

abline (find_intersection (params[3,], params[4,]))

```


```{r}
################################################################################
### Figure 12.X
###############################################################################

brmplot (fixef(model_multinomial), las = 2)
brmplot (fixef(model_multinomial_noS), add = TRUE, col = 2, nudge = 0.1, labels = "")
```



### Answering our research questions

We can use our model to answer the questions we posed above:

Q1) Can we use speaker f0 and VTL to predict their apparent speaker category?

Q2) Can the f0 by VTL space be divided into sections associated with the different categorizations (i.e. in a territorial map) in a useful way?

In terms of the first question, yes, we can use speaker f0 and VTL to predict apparent speaker accuracy. If we consider a baseline correctness by chance along of 25%, then our classification of 74% from two predictors isn't bad at all. 



## Ordinal (logistic) regression

Ordinal logistic regression involves the prediction of ordered categories. Although in some senses ordinal regression is 'simpler' than multinomial regression, we decided to present it after multinomial regression. This is because multinomial regression is just a generalization of logistic regression, which was discussed in detail in chapter 10. In contrast, ordinal regression, although related to (dichotomous) logistic regression, is in many ways conceptually its own thing. We will present one way of thinking about logistic regression however, as usual, there are many others. 

To discuss ordinal regression we need to talk about **latent variables**. Latent variables are variables that are inferred mathematically using some statistical model, but are not measured directly. For example, in our logistic regression model in chapter 10 we predicted the perception of femaleness. We did this by predicting variation in the logit of the probability of observing a female response. The logit of this probability can be thought of as a latent variable representing something like 'femininity' in the mind of the listener. The listener hears a voice, assigns it some femininity score, and based on this produces the surface variable that we do measure: The classification of a speaker as female. So, in this view the realization of the response variable (a female classification) is the result of a secret, *latent* variable that is not directly observed by us, but which we assume underlies the process. 




- weird latent variable
- unpredictable decision based on some choice. the distribution is the range of outcomes. 
- slide distribution like in regular regression
- except e assume logistic. 
- cumulative distribution function

- rather than one intercept, we have n-1 for n levels
- intercepts are fixed aspects of the model
- we model mu, and this changes responses as mu slides across intercepts
- you can also model the sd parameter
- there is also a normal model, but we will not be discussing here
- in practice little to no difference just like probit and logit for logistic


### Data and research questions


```{r, warning=FALSE, message=FALSE}
library (brms)
library (bmmb)
options (contrasts = c('contr.sum','contr.sum'))

data (exp_data)
exp_data = exp_data[exp_data$R=='a',]
```

We create a variable called `F`, which will equal our dependent variable. This variable equals 1 when listeners indicated hearing a female speaker and 0 when listeners indicated a male speaker. We will predict this using a single quantitative predictor, speaker VTL, which we center below. 

```{r}
exp_data$F = as.numeric (exp_data$G == 'f')
exp_data$vtl_original = exp_data$vtl
exp_data$vtl = exp_data$vtl - mean (exp_data$vtl)

exp_data$f0_original = exp_data$f0 
exp_data$f0 = exp_data$f0 - mean(exp_data$f0)
exp_data$f0 = exp_data$f0 / 100
```


```{r}
exp_data$SG = 0
exp_data$SG[exp_data$C=='m'] = 1
exp_data$SG[exp_data$C=='w'] = 2
exp_data$SG[exp_data$C=='g'] = 3
exp_data$SG[exp_data$C=='b'] = 3
```


```{r}
tmp = aggregate (height ~ C + L, data = bmmb::exp_data, FUN = mean)

plot(tmp[tmp$C=='b',"height"]-tmp[tmp$C=='g',"height"])
```



### Description of the model


### Fitting and interpreting the model


```{r}
# Fit the model yourself
set.seed (1)
options (contrasts = c('contr.sum','contr.sum'))

model_ordinal = brms::brm (SG ~ vtl+f0 + (vtl+f0|L) + (1|S), data=exp_data, family=cumulative("logit"), 
           chains=4, cores=4, warmup=1000, iter = 5000, thin = 4,
           prior = c(set_prior("student_t(3, 0, 3)", class = "Intercept"),
                     set_prior("student_t(3, 0, 3)", class = "b"),
                     set_prior("student_t(3, 0, 3)", class = "sd"),
                     brms::set_prior("lkj_corr_cholesky (2)", class = "cor")))

#  saveRDS (model_ordinal, "12_model_ordinal_3.RDS")
```


```{r}
model_ordinal = readRDS ("../models/12_model_ordinal.RDS")
```


```{r}
preds = predict (model_ordinal, re_formula = NA)
```

```{r}

fixed = brms::fixef(model_ordinal)

x = model_ordinal$data$vtl * fixed[3,1] + model_ordinal$data$f0  * fixed[4,1]

predds = cbind (round (plogis (fixed[1,1], x),5), 
                round ((plogis (fixed[2,2], x)-plogis (fixed[1,1], x)),5),
                round (1-plogis (fixed[2,2], x), 5))

head (predds)
head (preds)

       
```

```{r}
tapply (bmmb::exp_data$vtl, bmmb::exp_data$C, mean)
tapply (bmmb::exp_data$f0, bmmb::exp_data$C, mean)

par (mar = c(.2,.2,.2,.2), oma = c(3,3,.5,.5), mfrow = c(4,4))
curve (dlogis (x), from=-7,to=7)
abline (v = fixed[1:2])
```



### Answering our research questions









