\newpage
```{r, include = FALSE}
knitr::opts_chunk$set(
  dpi = 300, dev = "jpeg", collapse=TRUE
)
```

# Writing up experiments: An investigation of the perception of speaker category from speech acoustics

In this chapter we will present a complete analysis of our experimental data based on the models we've considered in the last few chapters. The chapter will be written in the general structure of an academic paper, but less formally, and will include a meta-discussion of what we are doing and why we are doing it. 

Writing a paper is like directing a short (maybe slightly dull) movie. A movie is made up of hundreds or thousands of discrete "shots", stuck together to give the impression of a single, continuous, coherent "story". Do you ever watch a movie and wonder, why is this scene here? What is this characters motivation? Where are characters right now and how did they get here? If so, this shows that either the director or the editor has been careless with the way they have stuck their shots together, resulting in problems with your understanding of the story. 

In the same way, the papers you write based on data analysis need to present the reader with dozens or hundreds of individual pieces of information, parameter settings, differences between conditions, and the like. You are in charge of stitching these pieces of information so that when the reader reads the paper, they feel like they are reading a coherent story regarding your research questions and what your experiment says about them. 

We're going to present examples of how what we've discussed so far in this book can be presented in the style of a research paper based on models similar to those fit in chapter 11. We're going to present two independent models: One predicts apparent height based on acoustics, apparent age and apparent gender, and the other predicts apparent gender based on speech acoustics and apparent age. There's a few strange things about this. First, we ran one and not two experiments so that treating these as unrelated dependent variables seems odd. Second, we know that apparent height, age, and gender are all related based on the analyses we have discussed to this point. 

Our intention in this chapter is simply to provide examples of analyses of both a quantitative variable and a dichotomous variable. We are effectively adopting the position of a set of researchers who ran the same experiments in two parallel dimensions where everything is the same except for these researchers decided on slightly different research questions. One of these researchers, the one prediction apparent height, treats apparent age and gender as given in order to understand height. The other researcher, the one predicting apparent gender, treats apparent age as given in order to understand apparent gender. We might consider a third researcher (in another dimension) who decides to consider both apparent age and apparent height simultaneously in a model with multiple dependent variables in some configuration. This sort of thing is possible with `Stan` and `brms`, however this being the last chapter and all, it's not something we'll be getting to in this book. Luckily, building these models is a modest extension of the things we *have* covered. For more information on models like this, please see (cite). 

We said that our intention is two present two examples of two separate experimental write-ups similar in form to an academic paper. However, because of the similarity of the research questions, much of the background information in the introduction of these two 'experiments' would be repetitive. In addition, since only one experiment was carried out there was only one set of methods. As a result, a single introduction and methods section will be presented for both analyses. After that, the experimental analyses and discussion of these diverge substantially. In order to avoid jumping back and forth between topics, we present contiguous results, discussion, and conclusion sections for each of the two experiments. 

## Introduction

The acoustic characteristics of the human voice vary in a systematic way between speakers with different indexical characteristics such as age and gender. Listeners are familiar with this co-variation and use this information to guess speaker indexical characteristics from their speech. Although listeners are often incorrect in their assessments they tend to be fairly consistent. In this way, listener judgments of speaker indexical characteristics can be thought of as incorrect but precise: People often make errors but their errors are fairly predictable. In this experiment we will investigate how listeners asses speaker characteristics from speech acoustics, and the way that our assumptions about speakers can affect the use of these cues. 

Our discussion of the perception of apparent speaker characteristics will focus on three of these: Apparent height, apparent age, and apparent gender. We will begin by talking about the perception of speaker size and height more generally. The main acoustic predictors of apparent speaker age, size and gender are the fundamental frequency, and the resonance properties of speech. Both of these acoustic characteristics will be introduced in the following sections, and the veridical relationships between these and apparent speaker characteristics will be discussed. After that, we will briefly review the current state of knowledge regarding listener perception of these characteristics. 


### Fundamental frequency and voice pitch

Your vocal folds, housed inside your larynx (in your neck) vibrate when you speak. For example, you will feel the front of your neck vibrate when you produce a 'zzzzzz' or a vowel sound. The rate of vibration of the vocal folds varies as a function of the mass and length of the vocal folds, and the size of the laryngeal structures in general. Think of guitar strings and the fact that the lower frequency strings tend to be thicker and higher strings tend to be thinner. In addition, the rate of vibration of the vocal folds is easily modifiable within a large range by speakers in the same way that a guitar player can adjust the tightness of a string to modify the note it plays. Rate of vibration is measured units of cycles per second, referred to as Hertz (Hz).

The repetitive vibration of the vocal folds results in a repetitive speech sound. Repetitive sounds can feel 'musical' or 'harmonic' to humans; these are terms that are difficult to define but that most people intuitively understand. A bell or flute is harmonic, a waterfall or the noise a fan makes is not. The repetition rate of sounds is refered to as the fundamental frequency (f0) of the sound. The f0 of a sound is the strongest determinant of perceived pitch. We say strongest because perceived pitch can also be (weakly) affected by the loudness of a sound, among other things, however it is still primarily dependent on f0.

Pitch is the internal *sensation* associated with sounds such that they can be ranked from 'low' to 'high'. Think of pitch like spicyness, it's the way something *feels*. You can rank foods from more to less spicy, you can rank sounds from lower to higher pitch. When you do this, in general, the resulting ranking will result in an orfering of the sounds according to f0: Sounds with higher f0s generally have higher pitches. So, when you hear a sound, such as human speech, and you hear someone with a 'high pitched voice', that is your auditory system (and brain) telling you, in its own way, that that person's vocal folds vibrating at a high rate (i.e. has a high f0). Because pitch is an important determiner of most apparent speaker characteristics (discussed in section X), f0 is an important acoustic characteristic for the prediction of these variables.   

### Variation in fundamental frequency between speakers


There are two main sources of average f0 differences between speakers. First, there is age related growth, meaning that f0 tends to decrease as humans age into their full adult size, as seen in figure \@ref(fig:F13-1). Second, there is the fact that there is substantial growth of langyngeal structures, including the vocal folds, during the typical course of male puberty. For this reason, most adult males produce speaking f0s that are lower than those of adult females of the same body size and age. This tendency is also clearly evident in figure \@ref(fig:F13-1). In summary, we can say that taller speakers tend to have lower f0s than shorter speakers, adults tend to have lower f0s than children, and adult males tend to have lower f0s than the rest of the population. 

```{r F13-1, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = "(left) Average height of males and females in the United states of America, organized by age (cite). (middle) Average f0 produced by male and female speakers between from 5 years of age until adulthood. (right) Estimated vocal-tract length for male and female speakers between from 5 years of age until adulthood, based on the acoustic data provided in Lee at al. (?)."}

################################################################################
### Figure 13.1
################################################################################

data(height_data, package="bmmb")
data(exp_data_all, package="bmmb")

par (mfrow = c(1,3), mar = c(4.1,4.1,1,1))

plot (height_data$age[height_data$gender=="f"]-.1,height_data$height[height_data$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(80,190),type='b', xlab="Age (years)",
      ylab = "Height (cm)",xlim=c(2,21),cex.axis=1.3,cex.lab=1.3)
lines (height_data$age[height_data$gender=="m"]+.1,height_data$height[height_data$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()
legend (11,120,legend = c("Female","Male"), col = c(2,4),pch=16,cex=1.2,pt.cex=1.5)


phonTools::errorbar(height_data$age[height_data$gender=="f"]-.1,
                    height_data$height[height_data$gender=="f"],
                    height_data$sd[height_data$gender=="f"],col=2,lwd=1,length=0.051)
phonTools::errorbar(height_data$age[height_data$gender=="m"]+.1,
                    height_data$height[height_data$gender=="m"],
                    height_data$sd[height_data$gender=="m"],col=4,lwd=1,length=0.051)

#ats = c(8,9.5,11,12.5,14, 15.5)
#vtls = (ats - 2.7)/0.068
#axis (side=4, at = vtls, labels = ats)
lee_etal_data
lee_etal_data$gbar = rowMeans (log(lee_etal_data[,4:6]))
lee_etal_data$vtl = exp(-((lee_etal_data$gbar)-min(lee_etal_data$gbar)))
lee_etal_data$vtl = 15 * lee_etal_data$vtl

agg = aggregate (cbind(f0,vtl) ~ age + gender, data = lee_etal_data, FUN = mean)
agg_sd = aggregate (cbind(f0,vtl) ~ age + gender, data = lee_etal_data, FUN = sd)
agg$vtl = agg$vtl * (15/max(agg$vtl))

rect (9.5,128,12.5,168,lwd=2,border="forestgreen",lty=1)
rect (17.5,150,20.5,190,lwd=2,border="forestgreen",lty=1)

plot (agg$age[agg$gender=="f"]-.1,agg$f0[agg$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(100,300),type='b', xlab="Age (years)",
      ylab = "f0 (Hz)",xlim=c(2,21),cex.axis=1.3,cex.lab=1.3)
lines (agg$age[agg$gender=="m"]+.1,agg$f0[agg$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()
phonTools::errorbar(agg$age[agg$gender=="f"]-.1,agg$f0[agg$gender=="f"],
                    agg_sd$f0[agg$gender=="f"],col=2,lwd=1,length=0.051)
phonTools::errorbar(agg$age[agg$gender=="m"]+.1,agg$f0[agg$gender=="m"],
                    agg_sd$f0[agg$gender=="m"],col=4,lwd=1,length=0.051)


#rect (9.5,220,12.5,270,lwd=3,border=3)
#rect (17.5,110,20.5,250,lwd=3,border=3)

points (c(11,11,19,19), tapply (exp_data$f0, exp_data$C, mean), pch=1,lwd=4, 
        cex = 3,col=bmmb::cols[2:5])

plot (height_data$height[height_data$gender=="f"][-c(1,2,3,19)],agg$f0[agg$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(100,300),type='b', xlab="Hieght (cm)",
      ylab = "f0 (Hz)",xlim=c(110,180),cex.axis=1.3,cex.lab=1.3)
lines (height_data$height[height_data$gender=="m"][-c(1,2,3,19)],agg$f0[agg$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()

#rect (9.5,220,12.5,270,lwd=3,border=3)
#rect (17.5,110,20.5,250,lwd=3,border=3)

points (height_data$height[c(29,10,38,19)],  tapply (exp_data$f0_original, exp_data$C, mean), pch=1,lwd=4,         cex = 3,col=bmmb::cols[2:5])

```

### Vocal tract length and voice resonance

Another way that speakers vary systematically in their speech acoustics is in their voice resonance. **Resonance** occurs when a sound wave bounces back and forth in an enclosed cavity so the energy can't leave the system easily, and builds up inside. Resonance forms the basis of most musical instruments and animal communication systems. So, dogs, cats, and birds, but also guitars and pianos, all make sounds by comprising structures that result in acoustic resonances. Most of these things make sounds by putting energy into a tube or enclosed space, and taking advantage of the resonance properties of that enclosed structure. For example, an acoustic guitar uses its strings to introduce energy into the cavity in its body, which serves to substantially amplify the sound. Without this amplification, the guitar strings alone would only make a very quiet and unappealing sound. In the same way, resonance is what allow us to produce very loud speech with relatively little effort. Think of the fact that babies that aren't strong enough to hold up their own heads can scream pretty loud. 

For human speech communication, the enclosed structure used to create acoustic resonances is the **vocal tract**, the space between the vocal folds and the lips. The vocal tract is an empty space comprising the oral cavity (your mouth) and the pharyngeal cavity (your throat). The vocal folds are like the guitar strings, they introduce energy into the vocal tract cavity by vibrating. Actually, as noted above, the vocal folds are more like the reed of a saxophone: The periodically open and close letting through (periodic) puffs of air. The puffs of air go from your larynx (in your neck) to your mouth and, even if your mouth is open, 'bounces' back and forth between the mouth and the vocal folds. These small amounts of energy reflected between the mouth and vocal folds add up and result in what is known as an acoustic resonance, a build up of acoustic energy inside of an enclosed space.

**Resonators**, that is, structures like the vocal tract that can resonate, usually have one or more preferred **resonance frequencies**. A complete explanation of this is beyond the scope of this chapter, but see cite for mor information on the topic. Here we will provide a not-entirely-right but conceptually simple explanation of the topic. The speed of sound is fixed for a given environment (temperature, pressure, gas composition, etc.). This means that the bouncing back and forth between your mouth and your larynx occurs at a fixed speed (i.e. the speed of sound). As a result, the number of times per second a sound wave can travel back and forth along the vocal tract, measured in Hertz, is determined by the length of that vocal tract. This is obvious: A longer travel time at a fixed velocity entails fewer repetitions per unit time. 

As a consequences of the above relationships, longer resonators have lower **resonance frequencies**, these are the frequencies they amplify the most. The resonance frequency of most sounds can be estimated using analysis software such as Praat (cite) or the phonTools package in R (Barreda cite). If a wave completed 50% fewer trips in a unit time, meaning the resonance frequency is 50% lower, then it traveled 50% longer to complete each trip (since speed is fixed). Based on this basic relation we can use resonance frequencies to estimate differences in speaker VTL. 

There are two extremely important caveats to our discussion of speaker VTL. The first is that we will almost always be referring to VTL as implied by acoustic measures. This can be thought of as an *effective* or *acoustic* VTL rather than a 'true' VTL. The issue is that the 'true' underlying VTL measured in units of length is not directly relevant for speech communication. If a speaker adopts a gesture that reduces their resonance frequencies by 10%, for the purposes of speech communication their VTL is effectively 10% shorter. Actually, it's not clear how to establish a 'true' average physical VTL for a speaker or what the direct relevance of this for speech communication would be. So, throughout this book just remember that when we discuss speaker VTL we are almost always discussing a VTL estimated given acoustic output. 

The second fact is that, when discussing acoustic VTL, differences in VTL can be established more readily than absolute VTL. This is because speakers can adopt a wide range of gestures to affect their acoustic VTL relative to any measure of true VTL, e.g. VTL during quiet nose breathing. As a result, the estimation of true VTL from speech acoustics is an underdetermined problem: There are an infinite number of possible different solutions. In addition, the differing gestures required to produce different speech sounds substantially affect the resonant frequencies of the vocal tract, making the connection between observed resonances and true VTL extremely complicated. Despite this, we can say the following: When two speakers say 'the same' thing, the differences in their vocal tract resonances should reflect differences in their effective/acoustic VTL. Thus, if two speakers saying 'the same' thing differ by 10% in their resonances, they differ by about 10% in their VTL.

The above has been in the service of getting to the following point: Listeners know that sounds with lower resonances represent larger resonators, and larger things more generally. The difference between a violin and a cello playing the same not is resonance. The difference between the barking sound of a tiny dog and a big dog is resonance. In the same way, speakers with lower resonances *sound* bigger to listeners. We are not alone in this. As noted, several animal species also appear to use the resonance information in calls to assess the size of other members of their species including red deer, koalas, and dogs. For a review of the potential importance of this to speech communication see Barreda (2020, cite). 

### Variation in vocal tract length between speakers

In figure \@ref(fig:F13-2) we see a comparison of age, height, and VTL. We can see that VTL increases as a function of age (and height) for both men and women. In addition, we see that male and female speakers are approximately the same in VTL until around 13 or 14 years of age, at which point males and females begin to diverge in VTL (and height). 

```{r F13-2, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = "(left) Average height of males and females in the United states of America, organized by age (cite). (middle) Average f0 produced by male and female speakers between from 5 years of age until adulthood. (right) Estimated vocal-tract length for male and female speakers between from 5 years of age until adulthood, based on the acoustic data provided in Lee at al. (?)."}

################################################################################
### Figure 13.2
################################################################################

data(height_data, package="bmmb")
data(exp_data_all, package="bmmb")

par (mfrow = c(1,3), mar = c(4.1,4.1,1,1))
plot (height_data$age[height_data$gender=="f"]-.1,height_data$height[height_data$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(80,190),type='b', xlab="Age (years)",
      ylab = "Height (cm)",xlim=c(2,21),cex.axis=1.3,cex.lab=1.3)
lines (height_data$age[height_data$gender=="m"]+.1,height_data$height[height_data$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()
legend (11,120,legend = c("Female","Male"), col = c(2,4),pch=16,cex=1.2,pt.cex=1.5)


phonTools::errorbar(height_data$age[height_data$gender=="f"]-.1,
                    height_data$height[height_data$gender=="f"],
                    height_data$sd[height_data$gender=="f"],col=2,lwd=1,length=0.051)
phonTools::errorbar(height_data$age[height_data$gender=="m"]+.1,
                    height_data$height[height_data$gender=="m"],
                    height_data$sd[height_data$gender=="m"],col=4,lwd=1,length=0.051)

rect (9.5,128,12.5,168,lwd=2,border="forestgreen",lty=1)
rect (17.5,150,20.5,190,lwd=2,border="forestgreen",lty=1)

#ats = c(8,9.5,11,12.5,14, 15.5)
#vtls = (ats - 2.7)/0.068
#axis (side=4, at = vtls, labels = ats)
data(lee_etal_data, package="bmmb")
lee_etal_data$gbar = rowMeans(log(lee_etal_data[,4:6]))

lee_etal_data
lee_etal_data$vtl = exp(-((lee_etal_data$gbar)-min(lee_etal_data$gbar)))
lee_etal_data$vtl = 15 * lee_etal_data$vtl

agg = aggregate (cbind(f0,vtl) ~ age + gender, data = lee_etal_data, FUN = mean)
agg_sd = aggregate (cbind(f0,vtl) ~ age + gender, data = lee_etal_data, FUN = sd)
agg$vtl = agg$vtl * (15/max(agg$vtl))

plot (agg$age[agg$gender=="f"]-.1,agg$vtl[agg$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(9,17),type='b', xlab="Age (years)",
      ylab = "VTL (cm)",xlim=c(2,21),cex.axis=1.3,cex.lab=1.3)
lines (agg$age[agg$gender=="m"]+.1,agg$vtl[agg$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()

phonTools::errorbar(agg$age[agg$gender=="f"]-.1,agg$vtl[agg$gender=="f"],
                    agg_sd$vtl[agg$gender=="f"],col=2,lwd=1,length=0.051)
phonTools::errorbar(agg$age[agg$gender=="m"]+.1,agg$vtl[agg$gender=="m"],
                    agg_sd$vtl[agg$gender=="m"],col=4,lwd=1,length=0.051)



#rect (9.5,220,12.5,270,lwd=3,border=3)
#rect (17.5,110,20.5,250,lwd=3,border=3)

points (c(11,11,19,19), tapply (exp_data$vtl, exp_data$C, mean), pch=1,lwd=4, 
        cex = 3,col=bmmb::cols[2:5])

plot (height_data$height[height_data$gender=="f"][-c(1,2,3,19)],agg$vtl[agg$gender=="f"],
      pch=16,col=2,lwd=2,cex=1.5, ylim = c(10,17),type='b', xlab="Height (cm)",
      ylab = "VTL (cm)",xlim=c(110,180),cex.axis=1.3,cex.lab=1.3)
lines (height_data$height[height_data$gender=="m"][-c(1,2,3,19)],agg$vtl[agg$gender=="m"],
      pch=16,col=4,lwd=2,cex=1.5,type='b')
grid()

#rect (9.5,220,12.5,270,lwd=3,border=3)
#rect (17.5,110,20.5,250,lwd=3,border=3)

points (height_data$height[c(29,10,38,19)],  tapply (exp_data$vtl_original, exp_data$C, mean), pch=1,lwd=4, 
        cex = 3,col=bmmb::cols[2:5])

```

There is good evidence that the VTL difference between adult male and females is a difference in height masquerading as a difference in VTL. That is, speaker height is strongly correlated to 'true' speaker VTL (measured using MRI) across the human population. The relationship is approximately the same between males and females and between-speaker variation is much larger than any seemingly systematic sex-based differences (Turner et al. 2009 cite ?). As a result, true VTL is strongly related to actual speaker height. Of course, because of the intervening influence of speech gestures into this, the relationship between true speakers height and acoustically-measured VTL may be substantially more complicated. For example, we see in the middle plot that there is a consistent different in VTL estimates for boys and girls. However, there is no corresponding predictable difference in body size between speakers of these ages. As a result, its been suggested that the apparent difference in VTL between boys and girls may be behavioral (cite).

### Perception of age, gender and size

- gross acoustic properties

There is substantial previous research on the perception of speaker size/height form speech. Experiments have repeatedly found that f0 and VTL cues individually affect the perception of size. The following general statement can be made: All other things being equal, listeners will identify a speaker with a lower f0 or longer apparent VTL as being larger. 

As noted in the previous section, there is a strong veridical associated between physical VTL and human height across the entire population. For this reason, listeners can generally guess the size of speakers with good accuracy, if the entire range of possible human height is considered. What we mean by this is that if you guess someone is 175 cm and they are 182 cm, that is somewhat inaccurate in that 7cm is a meaningful and noticeable error. However, considering that human speakers plausible range from about 100 to 200 cm, an error of 7 cm is not very large. The situation is more complicated for adult speakers. As we can see in the left plot above, height ranges vary much less for adult (>18) speakers. As a result of this, listeners are generally less accurate when estimating the heights of adults. 

The perception of age can be divided into two general situations: Before and after adulthood. As seen in figure above, until about 15 years of age or so age and height are almost perfectly correlated on average. After this age, the heights of females stop increasing much and those of males increase by gradually less until 18. So, for speakers below the age of 15 or so, identification of their age is effectively identification of their height, and vice versa. For example, the correlation between height and age for women from 2 to 15 in CDC cite is 0.992, indicating an almost perfect correlation. Effectively, if you can guess the height you can guess the age, and vice versa. Barreda and Assmann (cite) report that age can be estimated with reasonable accuracy from speech acoustics, and Assmann et all (cite cite) have found that both height and age can be estimated with good accuracy from speech. In both cases, speakers with longer VTLs and lower f0s were more likely to be identified as older and taller. 

We will not discuss the perception of age past adulthood in any detail, as variation in speech in adulthood does not have the simple anatomical explanations that it does in childhood. As a result, any ability to distinguish speakers who are, for example 30, from those who are 40, is necessarily a more subtle and complicated process than distinguishing a speaker who is 3 form one who is 13. 

Finally, we will discuss the identification of speaker gender from speech. First, we must make a distinction between sex and gender. Sex refers to the classification of humans into categories like male and female based wholly on anatomical/physiological considerations. Gender categories like male or female refer to a set of customs and behaviors that individuals engage in precisely to enforce gender differences in the population. For example, the most common 'scientific' definitions of biological sex center around reproductive capabilities. Such definitions are simply scientific objects, neither true nor false, and subject to change. Even if we take a reproductively-centered definition of 'biological women', there is little to no connection between many of the behavioral and social characteristics of 'biological women'. For example, there is no logical connection between physical characteristics of female sex and the stereotypical expectation that women tend to have longer hair than males. 

The most common way by which speakers (i.e. people) are assigned to gender categories is via a superficial inspection of the reproductive organs at birth. As a result of associations between reproductive organs, the endocrine system, and human development, most speakers assigned a male gender at birth are also likely to be relatively taller, and to have lower speaking f0, than female speakers. Both of the above result in adult males tending to have lower f0s and longer VTLs than female speakers within their same speaking group. This constraint is important because although men tend to be larger than women for a given population, there are large differences across global populations such that it's not the case that men everywhere are taller, than all groups of women. 

The most important predictors of apparent gender in adults are f0 and VTL (cite). Likely because adult men and women are well separated along these dimensions, listeners can identify the gender of adult speakers from voice in a large majority of cases (e.g. >95% in blank). One important thing to keep in mind that the perception of gender in voice is more complicated than what can be explained by f0 and VTL alone and involves performative aspects that cannot be explained solely due to average differences in body types (cite). 

For example, Hillenbrand et al. (2008?? cite) played listeners adult male and female voices and asked them to guess if they were men or women. They also manipulated stimuli to flip f0 and VTL between men and women: Adult males got female VTL and f0 and adult females got male VTL and f0. Even in this flipped condition, 20% (?) of women were still identified as women, and ?% of men were still identified as men. In addition, the tendency to identify speakers as their unflipped gender was greater for sentences than for syllables. So, although f0 and VTL are important for the perception of gender they do not entirely determine it, and more 'stylistic' information of the sort that is better conveyed by entire sentences also plays an important role for at least some speakers.  

The identification of gender in children's voices is more complicated, mostly due to the lack of many reliable anatomical differences between boys and girls, and in particular in atnaomy of the larynx and vocal tract, before puberty. This similarity can be seen by the almost complete overlap of speaker height, VTL and f0 seen in the figures above. Despite this however, previous studies have found that the gender of pre-pubescent children can be identified at an above than chance level (cite). For example, Barreda and Assmann (2021) presented listeners with the voices of boys and girls between the ages of four and 18 and asked them to guess the gender. They found that listeners can identify the gender of speakers as young as five and that this ability increased for sentences over syllables. 

These findings support those of Hillenbrand et al. (? cite) in two ways. First, they support the idea that although they are the most determiners of apparent gender and can predict a large majority of variation, there is more to apparent gender than just f0 and VTL. Second, they both show that given longer stretches of speech identification is better. This suggests that performative aspects of gender may come across better for longer stretches of speech. 

### Category-dependent behavior

In this section we will highlight two seeming cases of category-dependent use of acoustic cues: 1) The relationship between age, height, and gender, and 2) The age-dependent use of VTL cues in size perception. First, we will discuss the relationship between age, height, and gender. Consider figure \@ref(fig:F13-3) which presents the Leet et al. (cite) data. We can see in the left plot that males and females largely overlap in gross acoustics save for post-pubescent males. In the right plot we can see the age groups involved in this experiment in more detail, and we can compare this data to the characteristics of our stimuli (described in section X below). We can see that adult female acoustics overlap substantially with those of younger males. If a speaker makes a gender error for these voices, they will also make an age error. This suggests that the determination of these characteristics may be related. In fact, previous research has found that perception of the age of children between the ages of 5 and 18 is dependent on the perceived gender of the child (Barreda and Assmann 2018), and that the perception of the gender of children between the ages of 5 and 18 is dependent on the perceived age of the child (Barreda Assmann cite other)

```{r F13-3, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = "(left) Average f0 and vocal-tract length for each age group in the Lee at al. (?) data. (right) The same data as in the left plot, but only age groups similar to those included in our experiment are included. These are 10-12 year-old children, and adults 18-19 years of age."}

################################################################################
### Figure 13.3
################################################################################

data(height_data, package="bmmb")

data(exp_data_all, package="bmmb")

tmp_h_data = height_data
tmp_h_data = tmp_h_data[!(tmp_h_data$age %in% c(2,3,4,19)),]


agg = aggregate (cbind(f0,gbar) ~ age + gender, data = lee_etal_data, FUN = mean)

agg$vtls = exp(-((agg$gbar)-min(agg$gbar)))
agg$vtls = 15 * agg$vtls

par (mfrow = c(1,2), mar = c(4.1,4.1,1,1))


plot (agg$vtl,agg$f0, pch=16,col=2,lwd=2,cex=1.5, ylim = c(100,280),type='n', 
      xlab="Vocal-tract length (cm)", ylab = "f0 (Hz)",xlim=c(10,16),cex.axis=1.1,cex.lab=1.1)
grid()

text (agg$vtl,agg$f0, 5:19, col = rep(bmmb::cols[7:8], each = 15),cex=1.2)

legend (10,200,legend=c("female","male"), col=bmmb::cols[7:8],pch=16,bty='n')

use = agg$age %in% c(10,11,12,18,19)
plot (agg$vtl[use],agg$f0[use], pch=16,col=2,lwd=2,cex=1.5, ylim = c(100,280),type='n', 
      xlab="Vocal-tract length (cm)", ylab = "f0 (Hz)",xlim=c(10,16),cex.axis=1.1,cex.lab=1.1)
grid()

colss=bmmb::cols[c(3,3,3,5,5,2,2,2,4,4)]
text (agg$vtl[use],agg$f0[use], rep(5:19,2)[use], col = colss, cex=1.5)

points (tapply (exp_data$vtl_original, exp_data$C, mean),  
        tapply (exp_data$f0_original, exp_data$C, mean), pch=1,lwd=1, 
        cex = 6,col=bmmb::cols[2:5])

legend (10,200,legend=c("boy","girl","man","woman"), col=bmmb::cols[2:5],pch=1,bty='n',pt.cex=1.2)

```

In figure \@ref(fig:F13-2) we can see that average female height stops increasing substantially at about 14 years of age, and average male height stops increasing at about 18 years of age. We can also see that relatively modest changes in VTL, in the order of 2-3 cm, are associated with large changes in height (50 cm) between the ages of 5 to 15. However, we see that there is also approximately 2-3 cm of variation in VTL for adult speakers. This is backed up by existing data (cite pisanski, bird?). However, we do not expect adult speakers to vary from each other in height by 50 cm. As a result, Barreda (2017) has suggested that listener must use acoustics in a category dependent manner if they are to correctly guess the height of both adult and child speakers. Specifically, listeners must have a 'steep' relationship between VTL and apparent height when they think they are listening to children. This would allow them to relate given VTL differences with large differences in apparent height. They must also have a 'shallow' slope between VTL and apparent height for apparent adults, allowing them to relate the same VTL differences to smaller apparent height changes.  

### The current experiment

The experiment presented here is meant to investigate the relationship between speaker VTL, f0, and apparent height and gender. In each case, we are also interested in the role for apparent age in the use of speech acoustics and in the determination of height and gender. Listeners will be presented with a set of voices and asked to classify the speakers as male or female and as children and adults. Two separate analysis will be carried out. The first predicts apparent height based on speech acoustics and apparent age and gender, the second predicts apparent gender based on speech acoustics and apparent age. In general, we are interested in a few basic questions:

  Q1) What is the relationship between speech acoustics, specifically speaker f0 and VTL, and apparent height/gender?
  
  Q2a) Do apparent talker age and gender affect vary systematically with apparent height independently of speech acoustics?
  Q2a) Does apparent talker age vary systematically with apparent gender independently of speech acoustics?

  Q3) Do the effects for speaker f0 and VTL vary based on the apparent age or gender of the speaker? 

## Methods

In this section we will discuss the methods we used to collect the data. The goal is to provide enough information so that a person reading it should be able to replicate your experiment. You also need to keep in mind that the person reading your paper may have no idea what you did and things that seem obvious to you may not be obvious at all to your reader. Finally, you should try to keep in mind that readers may not be in your specialized area or even in your field. As a result, it is useful to either provide background information in highly-specialized subjects or methods, or at least provide adequate references for readers to figure things out for themselves. 

###  Participants

Listeners were 15 speakers of California English. Listeners were not all native speakers of English but all spoke English well enough to attend a university in the United States. All listeners participated in the experiment for partial course credit. Listeners were told that this was not a 'real' experiment, but rather that the resulting data would be analyzed in a book. Listeners were also told that the data would be shared publicly. Participating in the experiment and contributing their data to the book were both opt-in on the part of the student, for example, students could participate in the experiment for credit even if they did not wish to have their data included in the book data set. In addition, several other ways to get equivalent course credit were also offered. 

###  Stimuli

Stimuli where isolated productions of the word "heed" produced by the 139 speakers in the Hillenbrand et al. (1995) data set. Productions represented speech form x types of speakers. Speakers were all from Michigan and had the northeast cities dialect. In addition, speech samples were collected in 1995 meaning the local dialect may have changed substantially in the meantime. However, dialects of English do not usually exhibit much variation in the vowel sound in "heed" meaning that the speech should have sounded relatively "normal" to speakers of California English. In any case, any possible dialectal difference is not expected to have much effect on the important research questions we hope to address.

As noted above, the fundamental frequency (f0) of sounds is the repetition rate of the acoustic wave measured in Hertz, units of cycles per second. This means that the f0 of vowel sounds can estimated by measuring the rate at which the 'shape' at which the acoustic wave repeats as a function of time. For f0, we use the Hillenbrand et al. (1995) measurements for the stimuli, provided with the original data. These measurements were measured in the middle bla bla [@@ SB - check this]. 

Speaker vocal-tract length (VTL) was estimated based on the vocal-tract resonances provided in the Hillenbrand data. Since establishing a firm connection between acoustic resonances and a 'true' VTL in units of length is on empirically shaky ground, we're no going to bother with this. Instead, we are just going to focus on acoustic estimates of VTL based on the fact that a resonator that is 10% longer than another also produces resonance frequencies that are 10% lower than the first. 

So, to calculate speaker VTL we calculated the geometric mean resonance produced by each voice for their production of the vowel in the word 'heed'. We then found the speaker with the lowest overall resonances and set this speaker VTL to 16cm, basically appropriate for an adult male with a long VTL. This speaker served as the reference VTL. We then set each speaker's VTL to reflect the proportional difference between their resonances and those associated with the reference VTL. For example, imagine a speaker who produced resonances that were 1.04% higher than those of the reference speaker. This speaker would then have a VTL of 15.4 cm (16/1.04). 

What we did above is a reasonable estimate of the differences in *effective*, or *acoustic*, VTL. Differences in resonance frequencies are causally related to differences in VTL in a very direct manner. However, it should be clear that this does not reflect a 'real' physical VTL for these speakers, whatever that may be. We could have left the VTL predictor as a unit of frequency scaling, simply representing proportional differences in resonances between different speakers. However, we've found that readers find estimates of VTL in units of distance such as centimeters much more relatable.

We should note that all syllables were also manipulated using the "change gender" function in Praat (cite). This function allows you to scale the spectral envelope of speech sounds up/down according to uniform scaling, thereby changing resonance frequencies in a way that mimics the effects of differences in VTL. The spectral envelope of each syllable was scaled down by 8% (?), replicating a VTL increase of 8%. The fundamental frequency, duration, and other characteristics of the sound were not changed. Manipulated syllables were marked with a value of `b`, for 'big', in the `R` (resonance) column. These responses are not discussed or analyzed in the text as they are reserved for exercises, however, they are available in the `exp_data_all` data set included in the `bmmb` data.    

### Procedure

Listeners were instructed that they would hear the word "heed" produced by adult males (18+), adult females (18+), girls (10-12 years old), and boys (10-12 years old). They were asked to indicate how tall the speaker 'sounded' in feet and inches, and the category of the speaker. Responses could be provided in any order. Height responses were entered on a slider ranging form 4'0" to 6'6" (?). The slider displayed the selected height to the listener to the nearest tenth of an inch. Category responses were entered by selecting one of four category buttons labelled "women (18+ years old)", "man (18+ years old)", "girl (10-12 year old)", and "boy (10-12 years old)".

The experiment was conducted over Qualtrics, an online survey tool, with very little control over listening conditions. For example, listeners may have had headphones or speakers or very different qualities when carrying out the experiment. We have a pretty good excuse: Data collection was carried out during the covid-19 pandemic and both of the authors, in addition to the research participants, were mostly confined to their homes. However, if this were a 'real' experiment, we would have exerted more control over the listening conditions. Listeners were presented with syllables one at a time, randomized across stimulus dimensions. This means that all 276 (?) unique stimuli were presented in a completely unpredictable, random manner. This randomization was carried out independently for each listener. 

### Data screening

Data was collected from approximately 30 people, however, we only wanted 15 for the book in order to keep plots and model outputs manageable. As a result, we needed to eliminate about half the original listeners. Because the experiment was done online via Qualtrics and not in a controlled environment like a lab, the data was of varying quality between listeners. In addition, since listeners participated for partial course credit, in lieu of actual research participation, some may have been motivated by their grade rather than the love of science and may not have been as diligent in their responses as we might have liked. In addition, although idiosyncratic results do occur in real life, dealing with them in this text would have been a distracting digression from the goal of the text.

In light of this, when removing participants we tried to remove the "bad" data and tried to keep the "good" data. In general, this meant removing listeners that exhibited 'unusual' behavior, or behavior suggesting that they were not conducting the experiment in good faith. For example, some listeners provided the same average height for all categories of speakers or identified adult males at a low rate (despite the ease of this classification).

The above screening might be more difficult to justify in a 'real' experiment, and that's why we would like to stress that this is not a 'real' experiment. The data presented and analyzed here is 100% real, provided by actual listeners participating in the experiment described above. They did not know each other and did not co-ordinate their behavior in any way, now where they 'prepped' in any way regarding what sort of responses were expected for the experiment. However, the data has been 'curated' to make it suitable for its use as a pedagogical tool, and so any inferences taken from this data should be taken with a grain of salt. 

Normally, this section wouldn't be in a 'real' paper since we wouldn't have chosen our data in this way in a real experiment. However, if you *have* omitted listeners, individual observations, or modified your data in any way prior to analysis, you should definitely make that clear to the reader early on, and be prepared to explain your decisions and discuss the possible consequences for your analysis.

### Loading the data and packages

This section *definitely* wouldn't be in a real paper, but we want to load our data and packages in an easy-to-find place so that the code later on works for anyone following along. 

```{r, warning=FALSE, message=FALSE}
library (brms)
library (bmmb)
options (contrasts = c('contr.sum','contr.sum'))
data (exp_data)

exp_data$Female = ifelse(exp_data$G == 'f', 1, 0)

exp_data$vtl_original = exp_data$vtl
exp_data$vtl = exp_data$vtl - mean (exp_data$vtl)

exp_data$f0_original = exp_data$f0 
exp_data$f0 = exp_data$f0 - mean(exp_data$f0)
exp_data$f0 = exp_data$f0 / 100
```

### Statistical Analysis: Apparent height

We're going to re-fit the apparent height model we fit in chapter 11, without the f0 by VTL cross product. We use the code below to load the R packages we need, to set contrasts, and to load our data. What follows is something like what we would write in a paper or journal article describing the analyses we carried out.

Apparent height was treated as a quantitative variable coming from a t distribution with a trial-specific mean and fixed (but unknown) scale and nu parameters. Apparent height responses were converted to centimeters prior to analysis. Expected apparent trial was predicted based on the following 'fixed' effects: 1) Speaker VTL (`vtl`) measured in centimeters, 2) speaker fundamental frequency (`f0`) measured in hectohertz (1 hectohertz = 100 Hertz), 3) apparent speaker age, a factor `A` with levels adult (`a`) and child (`c`), and 4) apparent speaker gender, a factor `G` with levels female (`f`) and male (`m`). All possible interactions between fixed effects were included in the model. In addition, listener 'random effects' intercepts were calculated, as were listener-dependent effects for all fixed effects and their interactions. Speaker-dependent random intercepts were also included in the model. The model formula used is presented below.  

`height ~ (vtl+f0) * A * G + ((vtl+f0) * A * G | L) + (1 | S)`

Model 'fixed' effects were all estimated es coming from t distributions with a mean of zero, a scale of 5, and a nu of 3. The model intercept was given a t prior with a mean of 160, a scale of 5, and a nu parameter of 3. Speaker random intercepts were estimated as coming from a normal distribution with a mean of zero and a distribution-specific standard deviation estimated from the data. Listener random effects were estimated as coming from a 16-dimensional normal distribution. Each dimension was assumed to have a mean of zero and a standard deviation estimated from the data. An LKJ prior was used was a *concentration* parameter of 2, meaning our model would be somewhat skeptical of large correlations. Finally, the error term (`sigma`, $\sigma$) was given a half-t prior with a mean of 0, a scale of 12 and a nu of 3, and the nu parameter for our error distribution (i.e.this one $height \sim t(\nu, \mu, \sigma)$) was given a gamma prior with scale and rate parameters of 2 and 0.1 respectively. Data was analyzed using a multilevel Bayesian model fit with `Stan` (cite), using the `brms` (cite) package in R (cite). 

The above was a verbal description of the model presented in equation (last chapter). Here's the code we used to fit this model last chapter. We wouldn't include the code below in a paper, we would just like to note that the code below has the same information in it as the three paragraphs above. 

```{r, eval = FALSE}
priors = c(brms::set_prior("student_t(3,160, 5)", class = "Intercept"),
           brms::set_prior("student_t(3,0, 5)", class = "b"),
           brms::set_prior("student_t(3,0, 5)", class = "sd"),
           brms::set_prior("lkj_corr_cholesky (2)", class = "cor"), 
           brms::set_prior("gamma(2, 0.1)", class = "nu"),
           brms::set_prior("student_t(3,0, 5)", class = "sigma"))

height_model =  
  brms::brm (height ~ (vtl+f0)*A*G + ((vtl+f0)*A*G|L) + (1|S), data = exp_data, 
             chains = 4, cores = 4, warmup = 1000, iter = 5000, thin = 4, 
             prior = priors, family = "student")

```
```{r, include = TRUE, eval = FALSE}
# Download the height from the GitHub page:
height_model = bmmb::get_model ('13_model_height_vtl_f0.RDS')
```
```{r, include = FALSE, eval = TRUE}
height_model = readRDS ('../models/13_model_height_vtl_f0.RDS')
```

We would also like to note that it is good practice to do a prior predictive check *before* fitting your model in order to see that your prior generate reasonably plausible values of your dependent variables. Below is the code to carry our a prior predictive check for our model, and to plot the resulting predictions. You may have noted that above we use tighter priors than we have used to far in this book for our models with quantitative dependent variables (i.e. 5 cm as opposed to 12 cm). We decided to use the more constrained priors in our final analysis in our final model as this leads to predictions that are more concentrated in the 100-200 cm range that we are mostly interested, as can be seen by ruining the code below. 

```{r, eval = FALSE, warning=FALSE, message=FALSE}
height_model_priors =  
  brms::brm (height ~ (vtl+f0)*A*G + ((vtl+f0)*A*G|L) + (1|S), data = exp_data, 
             chains = 4, cores = 4, warmup = 1000, iter = 5000, thin = 4, 
             prior = priors, family = "student", sample_prior = "only")

p_check (height_model_priors)
```

### Statistical Analysis: Apparent gender

Apparent speaker femaleness (`F`) was treated as a binomial (dichotomous) variable coming from a Bernoullit distribution with a trial-specific p parameter. For the purposes of analysis responses of female were coded as 1 and responses of male were coded as 0. Expected apparent trial was predicted based on the following 'fixed' effects: 1) Speaker VTL (`vtl`) measured in centimeters, 2) speaker fundamental frequency (`f0`) measured in hectohertz (1 hectohertz = 100 Hertz), and 3) apparent speaker age, a factor `A` with levels adult (`a`) and child (`c`). All possible interactions between fixed effects were included in the model, save for the exclusion of the cross-product of f0 and VTL (and all related predictors). 

In addition, listener 'random effects' intercepts were calculated, as were listener-dependent effects for all fixed effects and their interactions. Speaker-dependent random intercepts were also included in the model. The model formula used is presented below.  

`F ~ (vtl + f0) * A + ((vtl + f0) * A | L) + (1 | S)`

Model 'fixed' effects, including the intercept, were all estimated es coming from t distributions with a mean of zero, a scale of 3, and a nu of 3. Speaker random intercepts were estimated as coming from a normal distribution with a mean of zero and a distribution-specific standard deviation estimated from the data. Listener random effects were estimated as coming from a 6-dimensional normal distribution. Each dimension was assumed to have a mean of zero and a standard deviation estimated from the data. An LKJ prior was used was a concentration parameter of 2. 

Since we are using exactly the same model we fit in chapter 11, we only load it below. 

```{r, include = TRUE, eval = FALSE}
# Download the gender model from the GitHub page:
gender_model = bmmb::get_model ('11_model_gender_vtl_f0_reduced.RDS')
```
```{r, include = FALSE, eval = TRUE}
gender_model = readRDS ('../models/11_model_gender_vtl_f0_reduced.RDS')
```

However, we include the code to carry our a prior predictive check on this model to remind the reader that is is a good idea to do this *before* you fit your final model. We are presenting these after the models are fit only due to the fact that these were described and discussed already in previous chapters. 

```{r, eval = FALSE}
# Fit the model yourself
gender_model_prior =
  brm (Female ~ (vtl+f0)*A + ((vtl+f0)*A|L) + (1|S), data=exp_data,
       family="bernoulli", sample_prior = "only",  
       chains = 4, cores = 4, warmup = 1000, iter = 5000, thin = 4, 
       prior = c(set_prior("student_t(3, 0, 3)", class = "Intercept"),
                 set_prior("student_t(3, 0, 3)", class = "b"),
                 set_prior("student_t(3, 0, 3)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))

preds = p_check (gender_model_prior)
```

## Results: Apparent height judgments

In this section we will outline a structured way to present large models to the reader. In general, we suggest figuring out what story you are trying to tell with your data, and making sure that everything you tell the reader is in service of helping them understand the story. This approach helps people understand your data better than loading up a cannon with numbers and blasting it at the reader. We will present and analysis of the height model first before moving onto the gender model. 

Before presenting the model results it's useful to talk about the results a bit, and to present some visual representation of the results. Although we've discussed our experimental data ad nauseam at this point, we will present this as we would the beginning of a results section and assume that the reader has never seen this data before. Figure below presents a boxplot showing all size responses across all listeners for speakers judged to be girls, women, men, and boys respectively. There are clear systematic differences in apparent height across the different apparent speaker categories. As seen in the middle and right plots of the same figure, it appears that these between-category differences may be associated with differing voice characteristics across speakers in different categories. 

```{r F13-4, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = " -- "}

################################################################################
### Figure 13.4
################################################################################

aggd = aggregate (cbind (height,f0,vtl) ~ S + C_v, data = exp_data, FUN = mean)

tab = table (exp_data$S, exp_data$C)
mod_cat = apply (tab, 1,which.max)

par (mfrow = c(1,3))
boxplot (height ~ A+G, data = exp_data, col = bmmb::cols[c(5,3,4,2)],ylim=c(110,195))
plot (aggd$f0, aggd$height, pch=16, col = bmmb::cols[1+mod_cat], cex=2,ylim=c(110,195))
plot (aggd$vtl, aggd$height, pch=16, col = bmmb::cols[1+mod_cat], cex=2,ylim=c(110,195))
```

The fixed effects for our height model are presented in figure below, and the standard deviations for all model random effects, as well as the model error term, are presented in the right plot of the same figure. As outlined in chapter 11, since our model has a large number of parameters we're going to focus on interpreting those that seem likely to result in 'meaningful' differences in apparent height (see ROPE, cite). For human height, apparent and veridical, we define meaningful differences as those that 1) are likely to be different from zero, *and* 2) have magnitudes of at least around 1 cm (about 0.5 inches). We see in the figure below that only a small number of these effects exceed that threshold. In addition, roughly the same subset of predictors exhibit the largest systematic between-listener variation in parameters. These predictors are: The main effect for VTL (`vtl`), the main effect for f0 (`f0`), the main effect for apparent age, and the interaction between apparent age and vtl. Our discussion below will focus on these effects.  

```{r F13-5, fig.height = 4, fig.width = 8, fig.cap='A comparison of fixed effect estimates provided by the brms (red) and lmer (black) models. The brms intervals are the 95% credible intervals, those for lmer are twice the standard error of the parameter estimate.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 13.5
################################################################################


par (mfrow = c(1,2), mar = c(4,6,1,1))

fixefs = fixef (height_model)[-1,]
good = (fixefs[,3] < -0.5 & fixefs[,4] < -0.5) | (fixefs[,3] > 0.5 & fixefs[,4] > 0.5) 
pchs = ifelse (good, 16,1)

brmplot (fixefs, xlim = c(-7,10),las=2, #ylim = c(-1,12),
               pch=pchs,lwd=2,horizontal=FALSE,xlab="Apparent Height (cm)")
abline (v = c(-0.5,0.5), lty = 3, col = bmmb::cols[6],lwd=2)

sds = bmmb::get_sds (height_model)
good = (sds[,3] < -0.5 & sds[,4] < -0.5) | (sds[,3] > 0.5 & sds[,4] > 0.5) 
pchs = ifelse (good, 16,1)

brmplot (sds, xlim = c(0,10),las=2, pch=pchs,lwd=2,xaxs='i',
               horizontal=FALSE,xlab="Apparent Height (cm)")
abline (v = c(0.5), lty = 3, col = bmmb::cols[6],lwd=2)

```

Table below presents information regarding our model fixed effects. We can see in the table (and figure) that f0 and VTL differences between speakers are associated with large and noticeable differences in apparent height. For example, the average difference of 100 Hz, and 2 cm difference in f0 and VTL respectively between adult males and females is expected to result in a difference of 11.9 cm (3.04 x 2 + 2.92 x 2) in apparent height. 

```{r T12-1, eval = FALSE}
knitr::kable(fixef(height_model), digits=2)
```

We can investigate the apparent age by VTL interaction by considering the simple effects of VTL across levels of apparent age. We can get these using the code below, and report it as follows. The simple effect for VTL when listeners thought the speaker was a child was 4.74 (s.d. = 0.78, 95% C.I = [3.24, 6.29]), meaning we expect an increase of 4.7 cm in apparent height for every 1 cm in increase in speaker VTL, on average. However, this value dropped all the way to (mean = 1.34, s.d. = 0.67, 95% C.I = [0.06, 2.72]) for apparent adults. So, differences in speaker VTL appear to be associated with much larger differences in apparent height when listeners think the speaker is a child. 

```{r}
age_vtl_slopes = bmmb::short_hypothesis(
  height_model, c("vtl + vtl:A1 = 0","vtl - vtl:A1 = 0"))

age_vtl_slopes
```

Given that we have a large difference in intercepts based on apparent age, this indicates that the lines relating speaker VTL to apparent height is substantially different for apparent children vs. apparent adults. We could get into this in more detail here, perhaps in a subsection within our results section. We could include several shorter subsections in the results that get into more detail regarding specific aspects of the results we have presented so far. Another option is to leave this for the discussion section, which is what we've chosen to do here. 

## Discussion: Apparent height

In the discussion section we can delve into any questions that arose but remained unresolved in the results section, and tell the 'story' of our data using more prose and in a more narrative fashion. Here is where you can explain your results to your reader and how they relate to your research questions. You can also develop interesting or unexpected findings in more detail, especially as they relate to previous findings in the field and possible *future* studies that might be suggested by your results. We're going to divide our discussion into thematic subsections, which can be a good idea since it allows the consideration of one topic/question at a time, and helps the reader find the information they may be looking for. 

### Age-dependent use of VTL cues on apparent height

In the results above, we found that the slope relating speaker VTL and apparent height was substantially different in apparent children and apparent adults. We calculated the age-dependent slopes for VTL above (`age_vtl_slopes`), and we calculate the age-dependent intercepts using the code below. 

```{r}
age_intercepts = bmmb::short_hypothesis(
  height_model, c("Intercept + A1 = 0","Intercept - A1 = 0"))

age_intercepts
```

Figure \@ref(fig:F13-6) presents the age-dependent intercepts (left plot), VTL slopes (middle plot), and a comparison of the lines implied by these parameters to our data (right plot). Age-dependent intercepts were 167.4 cm (s.d. = 1.27, 95% C.I = [164.88, 169.93]) for adults and 153.0 (s.d. = 1.86, 95% C.I = [149.28, 156.62]) for adults. Combined with the slope differences described above, the result is a higher but flatter line for adults and a lower but more steep line for children. 

bla bla. Given the same VTL there can be substantial differences in apparent height based on apparent age. 

```{r F13-6, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = " -- "}

################################################################################
### Figure 13.6
################################################################################


par (mar = c(4.2,4.2,1,1.5))
layout (mat=t(c(1:3)), widths=c(.27,.27,.46))
brmplot (age_intercepts, labels = c("A","C"),ylab="Apparent height (cm)",
         cex.lab=1.3,cex.axis=1.3)
brmplot (age_vtl_slopes, labels = c("A","C"),ylab="Apparent height (cm)",
         cex.lab=1.3,cex.axis=1.3)

tmp = exp_data$vtl - mean (exp_data$vtl)
aggd = aggregate (cbind (height,f0,tmp) ~ S + C_v, data = exp_data, FUN = mean)

plot (aggd$tmp, aggd$height, pch=16, col = bmmb::cols[1+mod_cat], 
      cex=2,ylim=c(120,185),xlab="Centered VTL (cm)", ylab = "Apparent Height (cm)",
         cex.lab=1.3,cex.axis=1.3)

abline (age_intercepts[1,1], age_vtl_slopes[1,1], lwd=3,col=2)
abline (age_intercepts[2,1], age_vtl_slopes[2,1], lwd=3, col=4)
```

### The effect for apparent gender on apparent height

In figure above we saw that the marginal (fixed) effect for apparent gender was effectively zero, however, the standard deviation of the listener-dependent gender effect ($\sigma_{A:L}$) was not. In figure below, we see the listener-dependent difference in apparent height across apparent genders. We can see that although many of these are not reliably different from zero, many are reliably different *from each other* and could reasonably affect apparent height judgments. As a result, we can say     


```{r F13-7, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = " -- "}

################################################################################
### Figure 13.7
################################################################################

listener_gender_effects = 
  bmmb::short_hypothesis(height_model, "-(G1+A1:G1)=0",scope="coef",group="L")


age_intercepts = bmmb::short_hypothesis(
  height_model, c("2*(A1+A1:G1) = 0","2*(A1-A1:G1) = 0",
                  "2*(G1+A1:G1) = 0","2*(G1-A1:G1) = 0"))

par (mar =c(4,4,1,1))
layout (mat = t(c(1:2)), widths = c(.6,.4))
brmplot (listener_gender_effects[,1:4]*2, col = bmmb::cols)
brmplot (age_intercepts)

```

Finally, before concluding our discussion of apparent height judgments we want to discuss the age by gender interaction. We include this even though its credible interval included values we consider practically meaningless because this interaction has a logical interpretation, and as a demonstration. 


## Conclusion: Apparent height judgments

apparent age is associated with taller heights given the same f0 and vtl. age is more subtle, as in communication of gender. evidence of category dependent effects for acoustics is strong and in line with previous reports. 


## Results: Apparent gender judgments

Figure \@ref(fig:F13-8) presents the probability with which individual speakers where identified as female, plotted against their gross acoustic characteristics. It's clear that there is a positive association between f0 and the probability of a female response, and a negative association between speaker VTL and the same probability. In addition, our figure suggests a possibly differing relationship between speech acoustics and female identifications for adult (women vs. men) compared to for children (girls vs. boys). 

```{r F13-8, echo = FALSE, fig.height = 3, fig.width = 8, echo = FALSE, fig.cap = " -- "}

################################################################################
### Figure 13.8
################################################################################

exp_data$F = as.numeric(exp_data$G=='f')

aggd = aggregate (cbind (exp_data$F,f0,vtl) ~ 
                    S + C_v, data = exp_data, FUN = mean)
colnames (aggd)[3]="Female"

tab = table (exp_data$S, exp_data$C_v)
mod_cat = apply (tab, 1,which.max)

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot (aggd$f0, aggd$Female, pch=16, col = bmmb::cols[1+mod_cat], cex=2,ylim=c(0,1))
legend (-1.1,0.9,legend=c("boy","girl","man","woman"), col=bmmb::cols[2:5],pch=16,bty='n',pt.cex=1.2)
plot (aggd$vtl, aggd$Female, pch=16, col = bmmb::cols[1+mod_cat], cex=2,ylim=c(0,1))
```

Although this analysis is focused on the relationships between predictors and responses, we may also want to consider how accurate listeners tended to be and the mistakes they tended to make. We can see that listeners were most accurate at identifying the gender of adults, and in particular adult males. Boys were confused with girls and women in 40% of cases, and girls were identified incorrectly in 57% of cases.

```{r}
# find cases where reported and veridical gender matches, across levels 
# of veridical category
tapply (exp_data$G==exp_data$G_v, exp_data$C_v,mean)

# table comparing reported to actual category. 
xtabs (~ exp_data$C + exp_data$C_v)
```

Figure \@ref(fig:F13-9) presents the model fixed effects and the standard deviation estimates of the random effects. Results indicate meaningful effects for f0, VTL, and apparent age, and meaningful interactions between both acoustic predictors and apparent age. The random effects standard deviations indicate meaningful variation in only three effects: Listener-dependent variation in the use of VTL ($\sigma_{VTL \colon L}$), and the effect of apparent age ($\sigma_{A1 \colon L}$), and speaker-dependent variation in intercepts ($\sigma_{S}$). 

```{r F13-9, fig.height = 3.5, fig.width = 8, fig.cap='A comparison of fixed effect estimates provided by the brms (red) and lmer (black) models. The brms intervals are the 95% credible intervals, those for lmer are twice the standard error of the parameter estimate.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 13.9
################################################################################

par (mfrow = c(1,2), mar = c(4,6,1,1))

fixefs = fixef (gender_model)
good = (fixefs[,3] < -0.5 & fixefs[,4] < -0.5) | (fixefs[,3] > 0.5 & fixefs[,4] > 0.5) 
pchs = 16

brmplot (fixefs, xlim = c(-4,4),las=2, #ylim = c(-1,12),
               pch=pchs,lwd=2,horizontal=FALSE,xlab="Logits")
#abline (v = c(-0.5,0.5), lty = 3, col = bmmb::cols[6],lwd=2)

sds = bmmb::get_sds (gender_model)
good = (sds[,3] < -0.5 & sds[,4] < -0.5) | (sds[,3] > 0.5 & sds[,4] > 0.5) 
#pchs = ifelse (good, 16,1)

brmplot (sds, xlim = c(0,2.5),las=2, pch=pchs,lwd=2,xaxs='i',
               horizontal=FALSE,xlab="Logits")
#abline (v = c(0.5), lty = 3, col = bmmb::cols[6],lwd=2)

```

Below we print a table of the standard deviations estimates presented in figure above. We don't present both tables in order to save space, but it is generally a good idea to provide actual numbers (even in an appendix or supplemental files) in addition to graphical representations of your results. 

```{r T11-1}
fixef_effects = fixef(gender_model)
standard_deviations = bmmb::getsds (gender_model)
knitr::kable(fixef_effects, caption = "Fixed effect estimaes, standard error, and 2.5% and 97.5% quantiles.")
```

The effects for the acoustic predictors are as expected: A lower f0 and longer VTL is associated with male responses. The magnitude of these effects is enough to make a meaningful difference for expected outcomes. For example, our results suggest that a 2 cm difference in VTL is associated with a logit difference of about 5.4, enough to change the probability of a female response from 6% ($\mathrm{logit}^{-1}(-2.7)=0.06$) to ($\mathrm{logit}^{-1}(2.7)=0.93$). Apparent adultness resulted in an increase in female responses, and the effects of f0 and VTL changes are *stronger* for adults compared to children. In terms of the random effects, it seems that there is substantial variation in only three parameters: Listener-dependent variation in the use of VTL ($\sigma_{VTL \colon L}$) and the effect of apparent age ($\sigma_{A1 \colon L}$), and speaker-dependent variation in intercepts ($\sigma_{S}$). 

## Discussion: Apparent gender judgments

We are going to focus on three aspects of our results: 1) The age dependent classification of speakers, 2) Between-listener variation in gender classifications, and 3) The speaker effects and more 'subtle' cues in gender perception.

### Effect of apparent age on the perception of femaleness

Apparent adultness increased the probability of observing a female response, while it increased the effects of VTL and f0. For example, the interaction between f0 and apparent age (`f0:A1`) has the effect of reducing the effect of f0 to nearly zero for children ($f0 + (-A1) = 2.2 - 1.6 = 0.6$) and nearly doubling it for adults ($f0 + A1 = 2.2 + 1.6 = 3.8$). This is seen in the top row of figure below which presents the predicted probability of apparent femaleness, in logits, plotted against f0. We can see that the line is much steeper for adults than for children, indicating that f0 is a stronger predictor of apparent femaleness for adults than for children. However, we can see that for both children and adults, the category boundary (i.e. the x intercept) between male and female responses moves leftward, towards lower values of f0, for apparent adults. 

```{r F13-10, fig.height = 3.5, fig.width = 8, fig.cap='.', echo = FALSE, cache = TRUE}

################################################################################
### Figure 13.10
################################################################################

tab = table (exp_data$S, exp_data$C_v)
mod_cat = apply (tab, 1,which.max)
aggd = aggregate (cbind(Female,f0,vtl) ~ S, FUN = mean, data = exp_data)

par (mfrow = c(2,3), mar = c(4,.1,1,.1), oma = c(0,4,0,1))

curve (f0_coefficients[1,1]+f0_coefficients[3,1]*x+vtl_coefficients[3,1]*-1,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered f0", ylim = c(-15,20),
       xaxt='s',yaxt='s')
curve (f0_coefficients[2,1]+f0_coefficients[4,1]*x+vtl_coefficients[4,1]*-1, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(f0_coefficients[1,1]+vtl_coefficients[3,1]*-1)/f0_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(f0_coefficients[2,1]+vtl_coefficients[4,1]*-1)/f0_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "VTL = -1")

curve (f0_coefficients[1,1]+f0_coefficients[3,1]*x+vtl_coefficients[3,1]*0,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered f0", ylim = c(-15,20),
       xaxt='s',yaxt='n')
curve (f0_coefficients[2,1]+f0_coefficients[4,1]*x+vtl_coefficients[4,1]*0, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(f0_coefficients[1,1]+vtl_coefficients[3,1]*0)/f0_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(f0_coefficients[2,1]+vtl_coefficients[4,1]*0)/f0_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "VTL = 0")

curve (f0_coefficients[1,1]+f0_coefficients[3,1]*x+vtl_coefficients[3,1]*1,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered f0", ylim = c(-15,20),
       xaxt='s',yaxt='n')
curve (f0_coefficients[2,1]+f0_coefficients[4,1]*x+vtl_coefficients[4,1]*1, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(f0_coefficients[1,1]+vtl_coefficients[3,1]*1)/f0_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(f0_coefficients[2,1]+vtl_coefficients[4,1]*1)/f0_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "VTL = 1")

curve (vtl_coefficients[1,1]+vtl_coefficients[3,1]*x+f0_coefficients[3,1]*-0.5,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered VTL", ylim = c(-10,15),
       xaxt='s',yaxt='s')
curve (vtl_coefficients[2,1]+vtl_coefficients[4,1]*x+f0_coefficients[4,1]*-0.5, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(vtl_coefficients[1,1]+f0_coefficients[3,1]*-0.5)/vtl_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(vtl_coefficients[2,1]+f0_coefficients[4,1]*-0.5)/vtl_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "f0 = -0.5")

curve (vtl_coefficients[1,1]+vtl_coefficients[3,1]*x+f0_coefficients[3,1]*0,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered VTL", ylim = c(-10,15),
       xaxt='s',yaxt='n')
curve (vtl_coefficients[2,1]+vtl_coefficients[4,1]*x+f0_coefficients[4,1]*0, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(vtl_coefficients[1,1]+f0_coefficients[3,1]*0)/vtl_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(vtl_coefficients[2,1]+f0_coefficients[4,1]*0)/vtl_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "f0 = 0")

curve (vtl_coefficients[1,1]+vtl_coefficients[3,1]*x+f0_coefficients[3,1]*0.5,ylab="Logits", 
       xlim = c(-3,3),lwd=6,col=bmmb::cols[9],xaxs='i',xlab="Centered VTL", ylim = c(-10,15),
       xaxt='s',yaxt='n')
curve (vtl_coefficients[2,1]+vtl_coefficients[4,1]*x+f0_coefficients[4,1]*0.5, 
       xlim = c(-3,3), add=TRUE,lwd=6,col=bmmb::cols[3])
abline (v = -(vtl_coefficients[1,1]+f0_coefficients[3,1]*0.5)/vtl_coefficients[3,1],
        col = bmmb::cols[9],lty=2,lwd=2)
abline (v = -(vtl_coefficients[2,1]+f0_coefficients[4,1]*0.5)/vtl_coefficients[4,1],
        col = bmmb::cols[3],lty=2,lwd=2)
abline (h = 0,lty=3, lwd=2)
text (1, -5, "f0 = 0.5")


```

In the bottom row of figure \@ref(fig:F13-10) we see a similar pattern of results, albeit with a negative relationship between VTL and apparent femaleness. Figure \@ref(fig:F13-10) also shows the effect of varying the *other* acoustic predictor on each of the relationships shown in the plot. For example, as we go from left to right in the bottom row we see the effect of f0 changing from -0.5, to 0, to 0.5 (i.e. from 50 Hz below the mean, to the mean, to 50 Hz above the mean). Overall, it is clear that that apparent age has a substantial effect on the mapping between gross acoustics and the perception of femaleness. 

We're just going to talk briefly about how we made those plots before moving on. The lines in figure above relate f0 and the probability of a female response *for a fixed value of VTL*. There is no 'ignoring' VTL as is not considering some value, it must be accounted for. Below we calculate intercepts and slopes for our lines, independently for each acoustic predictor and for both apparent adults and apparent children. 

```{r}
f0_coefficients = 
  bmmb::short_hypothesis (gender_model, 
                          c("Intercept+A1=0","Intercept-A1=0",
                            "f0+f0:A1=0","f0-f0:A1=0"))
vtl_coefficients = 
  bmmb::short_hypothesis (gender_model, 
                          c("Intercept+A1=0","Intercept-A1=0",
                            "vtl+vtl:A1=0","vtl-vtl:A1=0"))
```

For example, 
```{r}
f0_coefficients
```


```{r}
curve (f0_coefficients[1,1]+f0_coefficients[3,1]*x+vtl_coefficients[3,1]*-1)
curve (f0_coefficients[1,1]+f0_coefficients[3,1]*x+vtl_coefficients[3,1]*0)
```


```{r, collapse = TRUE}
# get fixed effect parameters
samples = brms::fixef (gender_model, summary = FALSE)

# get a,b,c coefficients for overall plane
a_all_reduced = mean (samples[,"Intercept"])
b_all_reduced = mean (samples[,"vtl"])
c_all_reduced = mean (samples[,"f0"])

# get a,b,c coefficients for adult plane
a_adult_reduced = mean (samples[,"Intercept"] + samples[,"A1"])
b_adult_reduced = mean (samples[,"vtl"] + samples[,"vtl:A1"])
c_adult_reduced = mean (samples[,"f0"] + samples[,"f0:A1"])

# get a,b,c coefficients for child plane
a_child_reduced = mean (samples[,"Intercept"] - samples[,"A1"])
b_child_reduced = mean (samples[,"vtl"] - samples[,"vtl:A1"])
c_child_reduced = mean (samples[,"f0"] - samples[,"f0:A1"])
```



```{r F11-13, fig.width = 8, fig.height = 3, fig.cap = "(left) Each point represents a single speaker, labels indicate most common group classification. Lines indicate male/female boundaries for adults (green), children (orange), and overall (blue) implied by the full model, ignoring the cross-product term. (middle) The same as the left figure but for the reduced model. (right) Territorial maps showing expected classifications for apparent adults in different regions of the f0 by VTL stimulus space, for the reduced model.", echo = FALSE, cache = TRUE}

################################################################################
### Figure 11-13
################################################################################


fixef_2 = brms::fixef (gender_model)

# y = (-b*x - a - z) / (dx+c)
# fixef(model_gender_vtl_f0)

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
mu_vtl = mean (tmp$vtl_original)
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
mu_f0 = mean (tmp$f0_original)
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

aggd = aggregate (cbind ( height, A=="a", G=="f", vtl,f0, vtl) ~ S + C_v, 
                      data = tmp, FUN = mean)
aggd$C = ""
aggd$C[aggd[,4]>= 0.5 & aggd[,5]>= 0.5] = "w"
aggd$C[aggd[,4]>= 0.5 & aggd[,5]<= 0.5] = "m"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]>= 0.5] = "g"
aggd$C[aggd[,4]<= 0.5 & aggd[,5]<= 0.5] = "b"
#table(aggd$C)

tab = table (tmp$S, tmp$C)
mod_cat = apply (tab, 1,which.max)

par (mfrow = c(1,3), mar = c(4,.25,.5,.25), oma = c(0,4,0,0))

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()
points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = bmmb::cols[c(2:5)][aggd$C])

legend (1,300, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = bmmb::cols[2:5], bty='n',pch=16,pt.cex=1.5)

curve ((-b_all_reduced*x - a_all_reduced) / (c_all_reduced), from = -3, to = 3, add = TRUE,lwd=2,col=bmmb::cols[7])
curve ((-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[10])
curve ((-b_child_reduced*x - a_child_reduced)/ (c_child_reduced), from = -3, to = 3, add = TRUE,lwd=2, lty=2,col=bmmb::cols[8])


plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()

xlim = par()$usr[1:2]
ylim = par()$usr[3:4]

x = seq(-3,4,0.1)
y = (-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced)
polygon (c(-3,x,4),c(-3,y,-3),col="seagreen")

y = (-b_adult_reduced*x - a_adult_reduced)/ (c_adult_reduced)
polygon (c(-3,x,1),c(3,y,3),col=2)

points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = 0)
box()

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C)], 
      xlim=c(-2.5,3),  pch=16,lwd=2, xlab = "",yaxt='n',
      ylab="Height (inches)")
grid()

xlim = par()$usr[1:2]
ylim = par()$usr[3:4]

x = seq(-3,4,0.1)
y = (-b_child_reduced*x - a_child_reduced)/ (c_child_reduced)
polygon (c(-3,x,4),c(-3,y,-3),col="cyan3")

y = (-b_child_reduced*x - a_child_reduced)/ (c_child_reduced)
polygon (c(-3,x,1),c(3,y,3),col="darkgoldenrod1")

points (aggd$vtl, aggd$f0, cex =1.2, pch=16,lwd=2,
      col = 0)
box()

```







### Between-listener variation in gender perception

We can also think about the result of this variation in terms of how this might actually affect gender classification from speech. To do this, we get listener-specific intercepts, f0 slopes, and VTL slopes. We do this separately for apparent children and apparent adults using the code below (explained in detail in section X). 

```{r}
listener_coefficients_adult = 
  bmmb::short_hypothesis (gender_model, scope="coef",group="L",
                          c("Intercept+A1=0","vtl+vtl:A1=0","f0+f0:A1=0"))
listener_coefficients_child = 
  bmmb::short_hypothesis (gender_model, scope="coef",group="L",
                          c("Intercept-A1=0","vtl-vtl:A1=0","f0-f0:A1=0"))
```

Using these parameters, we can find boundaries for each listener using the equation above. We plot these in figure \@ref(fig:F11-16). We see that there is a general consensus between listeners in terms of an approximate acoustic boundary between adult males and adult females. However, there is much more variation in the boundary between boys and girls. This likely reflects the fact that the acoustic characteristics of adult men and women are generally more predictable and stable relative to children 10-12 years old. In addition, the listeners in our experiment (undergraduate University students) likely have substantially more recent experience interacting with adults than with children in that age group.   

```{r F11-16, fig.width = 8, fig.height = 5, fig.cap = "The first plot compares all category boundaries between men/women (orange) and boys/girls (blue). Subsequent plots present each listener's individual boundaries, and point colors indicate the listener's individual speaker classifications.", echo = FALSE, cache = FALSE}

################################################################################
### Figure 11-16
################################################################################

# y = (-b*x - a - z) / (dx+c)
# fixef(model_gender_vtl_f0)

tmp = bmmb::exp_data
tmp = tmp[tmp$R=='a',]

tmp$vtl_original = tmp$vtl
tmp$vtl = tmp$vtl - mean (tmp$vtl)

tmp$f0_original = tmp$f0 
tmp$f0 = tmp$f0 - mean(tmp$f0)
tmp$f0 = tmp$f0 / 100

aggd = aggregate (cbind ( height, A=="a", G=="f", vtl,f0, vtl) ~ S + A_v + G_v + C_v, 
                      data = tmp, FUN = mean)
#aggd = aggd[aggd$A_v=='a',]

a = listener_coefficients_adult[1:15,1]
b = listener_coefficients_adult[16:30,1]
c = listener_coefficients_adult[31:45,1]

ac = listener_coefficients_child[1:15,1]
bc = listener_coefficients_child[16:30,1]
cc = listener_coefficients_child[31:45,1]

par (mar = c(.2,.2,.2,.2), oma = c(3,3,.5,.5), mfrow = c(4,4))

plot (aggd$vtl,aggd$f0, cex =1.2, col = bmmb::cols[c(2:5)][factor(aggd$C_v)], 
      pch=16,lwd=2, xlab = "",ylab="f0 (cm)",xaxt = 'n',
        xlim=c(-2.5,2.7),ylim=c(-1.2,1.2))
grid()

for (i in 1:15)
  curve ((-b[i]*x - a[i]) / (c[i]), from = -3, to = 3, 
         add = TRUE,lwd=2,col=bmmb::cols[8],xaxt='n')

for (i in 1:15)
  curve ((-bc[i]*x - ac[i]) / (cc[i]), from = -3, to = 3, 
         add = TRUE,lwd=2,col=bmmb::cols[7],xaxt='n')

for (i in 1:15){
  xaxt = "n"
  yaxt = "n"
  if (i %in% 12:15) xaxt = "s"
  if (i %in% c(4,8,12)) yaxt = "s"
  tmpp = tmp[tmp$L==i,]
  plot (tmpp$vtl,tmpp$f0, cex =1.2, col = bmmb::cols[2:5][factor(tmpp$C)],
        pch=16,lwd=2, xlab = "",ylab="f0 (cm)",xaxt=xaxt,yaxt=yaxt,
        xlim=c(-2.5,2.7),ylim=c(-1.2,1.2))
  grid()
  curve ((-b[i]*x - a[i]) / (c[i]), from = -5, to = 3, add = TRUE,
         lwd=2,col=bmmb::cols[8])
  curve ((-bc[i]*x - ac[i]) / (cc[i]), from = -5, to = 3, add = TRUE,
         lwd=2,col=bmmb::cols[7])
  text (-1.5,-.75,i,cex=1.5)
}
```


### Beyond gross acoustic cues in gender perception

Our model attempts to predict female responses given a small number of acoustic predictors


```{r}
tab = table (exp_data$S, exp_data$C_v)
mod_cat = apply (tab, 1,which.max)
brmplot (ranef(gender_model)$S[,,1], col = bmmb::cols[mod_cat+1])
```

## Conclusion: Apparent gender



## Overall conclusion





## References

Enver, N., Doruk, C., Kara, E., Kaali, K., Asliyuksek, H., & Basaran, B. (2021). Does size matter in laryngology? Relation between body height and laryngeal morphometry. Journal of Voice, 35(2), 291-299.


Sapienza, C. M., Ruddy, B. H., & Baker, S. (2004). Laryngeal structure and function in the pediatric larynx.



