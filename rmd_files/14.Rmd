\newpage
```{r, include = FALSE}
knitr::opts_chunk$set(
  dpi = 300, dev = "jpeg", collapse=TRUE
)
#options(knitr.duplicate.label = "allow")
```

# Conclusion

[@@ Noah, read this chapter its new]

This book is over, now what? We tried to provide thorough examples of working with different types of common model structures and data types. However, it is impossible to cover everything in one book, and in fact there are many important topics and methods that we didn't get to. We leave you with a very brief discussion on some of the topics we didn't get to or chose to not discuss. 

## Research design, variable selection, etc.

In this book we focused on introducing important concepts and putting them into practice. However, we *began* with a completed experiment and began at a point where we know what predictors we wanted in our model, how we wanted to use them in our model, and how we wanted to represent our dependent variable. In a typical case, the researcher must make all these decisions. 

For example, in the example regarding coffee and reading times mentioned sporadically throughout the book, the researcher has to decide which, if any, other manipulations (and associated predictors) to include in their experiment. They also need to decide which *observed* variables they want to include, these are things that were not manipulated such as reader age or gender. The famous statistician Ronald Fisher said: "To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of". Your experiment is like a cake you put in the oven. Once it comes out, its too late to add the eggs. All of this is to say that a successful statistical analysis hinges on a properly designed experiment and having a model that reasonably reflects the relations between the variables in your model. Mostly any decent introductory statistics book will feature a discussion of these topics. 

## Non-linear models

The models we have fit in this book are all *linear models*, meaning we have made predictions by adding together predictors after multiplying them by coefficients. A **non-linear regression model** is one in which predictions are *not* linearly combined. Since non-linear regression is simply characterized by not being a very specific combination of parameters, there are a very large number of possible non-linear models. For example, consider the following prediction equations:

$$
\begin{equation}
\begin{split}
\mu_{[i]} = x_{[1]} \cdot a_{[1]} \cdot x_{[2]} \cdot a_{[2]} \\ \\
\mu_{[i]} = \frac {x_{[1]} \cdot a_{[1]}}{x_{[2]} \cdot a_{[2]}} \\
\end{split}
(\#eq:14-1)
\end{equation}
$$

Although they are very 'simple', do not represent linear combinations of the terms and will not generate lines or planes for most values of $x$ and $a$. As noted above there are a wide variety of non-linear models, often highly-specific to different sub-fields. The good news is that both `brms` and *Stan* can be used to fit most non-linear models, and a majority of the content in this book will directly translate to working with these sorts of models. 

## Other data distributions

In this book we discussed some of the more 'basic' and common data distributions in use: Normal, t, Bernoulli, multinomial, etc. However, there are a large number of other distributions that can be used in `brm` (and *Stan*) simply by changing the `family` parameter in the function call. You can think of this parameter as below, where the distribution specified by the family parameter generates your data y given some parameter (or parameters) $\theta$.

$$
\begin{equation}
\begin{split}
y \sim \mathrm{family} (\theta)
\end{split}
(\#eq:14-2)
\end{equation}
$$

For example, setting `family=poisson` uses the Poisson distribution to model your data, effectively assuming that $y \sim \mathrm{Poisson} (\theta)$. The Poisson distribution is used to model discrete count data that is bounded by zero at the lower end but unbounded at the higher end. You can get more information about the link functions and data distributions included in `brms` using the command `?brms::brmsfamily`, and you should refer to the current *stan* manual for information about the link functions and distributions supported by *Stan*. You can get more information about the different distributions in a large number of textbooks and web pages online. In this book we modeled quantitative data (that we treated as unbounded and continuous), binomial, multinomial, and ordinal data. If you are modeling any other kind of data, it's worth thinking about what kind of probability distribution (and link function) would be best suited for that data.

## Multivariate analyses

The models in this book have almost all had univariate dependent variables. In other words, at all times we were predicting variables with a single dimension. However, we might imagine wanting to predict multivariate dependent variables, for example, from a multivariate normal distribution. Below, we see an example where we predict bivariate normal data ($y_1, y_2$) by predicting expected values for each dimension ($\mu_1, \mu_2$) and estimating the covariance matrix of the distribution ($\mathrm{\Sigma}$). Notice that in the prediction equations below, the predictors ($x$) are shared by both dimensions but the coefficients ($a$) are not.

$$
\begin{equation}
\begin{split}
\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \sim \mathrm{MVNormal} \left( \begin{bmatrix} \mu_1 \\ \mu_2 \\ \end{bmatrix}, \Sigma \right) \\ \\
\mu_1 = a_{1[1]} \cdot x_{[1]} + a_{1[2]} \cdot x_{[2]} + a_{1[3]} \cdot x_{[3]} \\
\mu_2 = a_{2[1]} \cdot x_{[1]} + a_{2[2]} \cdot x_{[2]} + a_{2[3]} \cdot x_{[3]} \\
\\\end{split}
(\#eq:14-2)
\end{equation}
$$

These sorts of models are actually very similar to the models we have fit in this book in that each dimension is represented by its own independent prediction equation. However, there are three main differences between fitting, for example, a single multivariate model predicting both variables at the same time, and fitting two independent models predicting each dimensions at a time. First, by fitting the model above as a multivariate model, we can test for differences between, for example, $\mu_1$ and $\mu_2$ or $a_{1[1]}$ and $a_{2[1]}$, that could only be made indirectly if the two models are fit independently. Second, treating our data as multivariate allows us to estimate the correlations of coefficients across dimensions, which is particularly useful when estimating 'random effects'. For example, we could use a multivariate model to see if there is a relationship between how listeners used one predictor for one dimension relative to for another dimension. Finally, when we treat data as univariate, under certain conditions we can also estimate the correlation of the residuals across dimensions. This will tell us, for example, whether our model overestimates one dimension when it underestimates the other, or if errors are unrelated. 

There are also other reasons you might want to build multivariate models, such as to carry out structural equation modeling or a path analysis. Those topics are beyond the scope of this book, however these, and other, multivariate models can fit with relative ease using `brms` or *Stan*.

## Causality and reaching conclusions

[@@ Noah - dont really know what to say here but seems like a good end]

As we've noted repeatedly in this book, finding a statistical association between variables is not the same thing as showing that there is a causal relationship between variables. It's up for debate whether *anything* can really be proven by *any* means, let alone using statistical methods. 
