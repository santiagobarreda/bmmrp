  
    
    
    
    
    
    
    
    
    
--- 
title: "Bayesian multilevel models for repeated measures data"
---


\newpage

Title: Bayesian multilevel models for repeated measures data
  
  
  
Subtitle: A conceptual and practical introduction in R
  
    
     
     
Santiago Barreda  
https://orcid.org/0000-0002-1552-083X  

Noah Silbert  
https://orcid.org/0000-0003-2515-384X  

\newpage

(Dedication)  
  
para mis chicas con pelo de nudo. 


\newpage

# Table of Contents {-}

| Preface
|   Bayesian multilevel models and repeated measures data
|   What’s missing from this book
|   Statistics as procedural knowledge
|     Practice vs brain power
|   How to use this book
|     Supplemental resources
|   Our Target Audience
|     The self starter
|     The convert
|     The instructor
|   What you need installed to use this book
|   Why go Bayesian?
|     Why brms?
|   It takes a village (of books)
|   Acknowledgments  
   
     
| 1	Introduction: Experiments and Variables  
|    1.1	Chapter pre-cap  
|     1.2	Experiments and effects  
|         1.2.1	Experiments and inference  
|    1.3	Our experiment  
|        1.3.1	Our experiment: Introduction  
|        1.3.2	Our experimental methods  
|        1.3.3	Our research questions  
|         1.3.4	Our experimental data  
|     1.4	Variables  
|         1.4.1	Populations and samples  
|         1.4.2	Dependent and Independent Variables  
|         1.4.3	Categorical variables and ‘factors’  
|         1.4.4	Quantitative variables  
|         1.4.5	Logical variables  
|     1.5	Inspecting our data  
|         1.5.1	Inspecting categorical variables  
|         1.5.2	Inspecting quantitative variables  
|         1.5.3	Exploring continuous and categorical variables together  
|     1.6	Exercises  
|     1.7	References  

| 2	Probabilities, likelihood, and inference  
|     2.1	Chapter pre-cap  
|     2.2	Data and research questions  
|     2.3	Empirical Probabilities  
|       2.3.1	Conditional and marginal probabilities  
|       2.3.2	Joint probabilities  
|     2.4	Probability distributions   
|     2.5	The normal distribution  
|       2.5.1	The sample mean  
|       2.5.2	The sample variance (or standard deviation)  
|       2.5.3	The normal density  
|       2.5.4	The standard normal distribution  
|     2.6	Models and inference  
|     2.7	Probabilities of events and likelihoods of parameters  
|       2.7.1	Characteristics of likelihoods  
|       2.7.2	A brief aside on logarithms  
|       2.7.3	Characteristics of likelihoods, continued  
|     2.8	Answering our research questions  
|     2.9	References  

| 3	Fitting Bayesian regression models with brms  
|     3.1	Chapter pre-cap  
|     3.2	What are regression models?  
|     3.3	What’s ‘Bayesian’ about these models?  
|       3.3.1	Prior probabilities  
|       3.3.2	Posterior distributions  
|       3.3.3	Posterior distributions and shrinkage  
|     3.4	Sampling from the posterior using Stan and brms  
|     3.5	Estimating a single mean with the brms package  
|       3.5.1	Data and Research Questions  
|       3.5.2	Description of the model  
|       3.5.3	Errors and residuals  
|       3.5.4	The model formula  
|       3.5.5	Fitting the model: Calling the brm function  
|       3.5.6	Interpreting the model: The print statement  
|       3.5.7	Seeing the samples  
|       3.5.8	Getting the residuals  
|     3.6	Checking model convergence  
|     3.7	Specifying prior probabilities  
|     3.8	The log prior and log posterior densities  
|     3.9	Answering our research questions    
|     3.10	‘Traditionalists’ corner    
|       3.10.1	One-sample t-test vs. intercept-only Bayesian models    
|       3.10.2	Intercept-only ordinary-least-squares regression vs. intercept-only  
|               Bayesian models  
|     3.11	Exercises  

| 4	Inspecting a ‘single group’ of observations using a Bayesian multilevel model  
|     4.1	Chapter pre-cap  
|     4.2	Repeated measures data  
|       4.2.1	Multilevel models and ‘levels’ of variation  
|     4.3	Representing predictors with many levels  
|     4.4	Strategies for estimating factors with many levels  
|       4.4.1	Complete pooling  
|       4.4.2	No pooling  
|       4.4.3	(Adaptive) Partial pooling  
|       4.4.4	Hyperpriors  
|     4.5	Estimating a multilevel model with brms  
|       4.5.1	Data and Research questions  
|       4.5.2	Description of the model  
|       4.5.3	Fitting the model  
|       4.5.4	Interpreting the model  
|     4.6	‘Random’ Effects  
|       4.6.1	Inspecting the random effects  
|     4.7	Simulating data using our model parameters  
|     4.8	Adding a second random effect  
|       4.8.1	Updating the model description  
|       4.8.2	Fitting and interpreting the model  
|     4.9	Investigating ‘shrinkage’  
|     4.10	Answering our research questions  
|     4.11	‘Traditionalists’ corner  
|       4.11.1	Bayesian multilevel models vs. lmer  
|     4.12	Exercises  
|     4.13	References  

| 5	Comparing two groups of observations: Factors and contrasts  
|     5.1	Chapter pre-cap  
|     5.2	Comparing two groups  
|     5.3	Distribution of repeated measures across factor levels  
|     5.4	Data and research questions  
|     5.5	Estimating the difference between two means with ‘brms’  
|         5.5.1	Fitting the model  
|         5.5.2	Interpreting the model  
|     5.6	Contrasts  
|         5.6.1	Treatment coding  
|         5.6.2	Sum coding  
|         5.6.3	Comparison of sum and treatment coding  
|     5.7	Sum coding and the decomposition of variation  
|        5.7.1	Description of the model  
|         5.7.2	Fitting the model  
|        5.7.3	Comparison of sum and treatment coding  
|     5.8	Inspecting and manipulating the posterior samples  
|         5.8.1	Using the hypothesis function  
|         5.8.2	Working with the random effects  
|     5.9	Making our models more robust: The (non-standardized) t distribution  
|     5.10	Re-fitting with t-distributed errors  
|         5.10.1	Description of the model  
|        5.10.2	Fitting and interpreting the model  
|     5.11	Simulating the two-group model  
|     5.12	Answering our research questions  
|     5.13	‘Traditionalists’ corner  
|         5.13.1	Bayesian multilevel models vs. lmer  
|     5.14	Exercises  

| 6	Variation in parameters (‘random effects’) and model comparison  
|     6.1	Chapter pre-cap  
|     6.2	Data and research questions  
|     6.3	Variation in parameters across sources of data  
|        6.3.1	Description of our model  
|         6.3.2	Correlations between random parameters  
|         6.3.3	Random effects and the multivariate normal distribution  
|         6.3.4	Specifying priors for a multivariate normal distribution  
|         6.3.5	Updating our model description  
|         6.3.6	Fitting and interpreting the model  
|     6.4	Model Comparison  
|         6.4.1	In-sample and out-of-sample prediction  
|        6.4.2	Out-of-sample prediction: Adjusting predictive accuracy  
|        6.4.3	Out-of-sample prediction: Cross validation  
|        6.4.4	Selecting a model  
|    6.5	Answering our research questions  
|     6.6	‘Traditionalists’ corner  
|         6.6.1	Bayesian multilevel models vs. lmer  
|     6.7	Exercises  
|     6.8	References  

| 7	Comparing many groups, interactions, and posterior predictive checks  
|     7.1	Chapter pre-cap  
|     7.2	Comparing four (or any number of) groups  
|         7.2.1	Data and research questions  
|         7.2.2	Description of our model  
|         7.2.3	Fitting and interpreting the model  
|     7.3	Investigating multiple factors simultaneously  
|         7.3.1	Data and research questions  
|         7.3.2	Description of the model  
|         7.3.3	Fitting and interpreting the model  
|     7.4	Posterior prediction: Using our models to predict new data  
|     7.5	Interactions and interaction plots  
|     7.6	Investigating interactions with a model  
|         7.6.1	Data and research questions  
|         7.6.2	Model formulas  
|         7.6.3	Description of our model  
|         7.6.4	Fitting and interpreting the model  
|         7.6.5	Caulculating group means in the presence of interactions  
|         7.6.6	Calculating simple effects in the presence of interactions  
|         7.6.7	Assessing model fit: Bayesian $R^2$  
|     7.7	Answering our research questions  
|     7.8	Factors with more than two levels  
|     7.9	‘Traditionalists’ corner  
|         7.9.1	Bayesian multilevel models vs. lmer  
|     7.10	Exercises  
|     7.11	References  

| 8	Varying variances, more about priors, and prior predictive checks  
|     8.1	Chapter pre-cap  
|     8.2	Data and Research questions  
|     8.3	More about priors  
|         8.3.1	Prior predictive checks  
|         8.3.2	More specific priors  
|     8.4	Heteroskedasticity and distributional (or mixture) models  
|    8.5	A ‘simple’ model: Error varies according to a single fixed effect  
|        8.5.1	Description of our model  
|        8.5.2	Prior predictive checks  
|         8.5.3	Fitting and interpreting the model  
|    8.6	A ‘complex’ model: Error varies according to fixed and random effects  
|         8.6.1	Description of our model  
|         8.6.2	Fitting and interpreting the model  
|     8.7	Answering our research questions  
|     8.8	Building identifiable and supportable models  
|         8.8.1	Collinearity  
|         8.8.2	Predictable values of categorical predictors  
|         8.8.3	Saturated, and ‘nearly-saturated’, models  
|     8.9	Exercises  
|     8.10	References  

| 9	Quantitative predictors and their interactions with factors  
|     9.1	Chapter pre-cap  
|     9.2	Data and research questions  
|     9.3	Modeling variation along lines  
|         9.3.1	Description of the model  
|         9.3.2	Centering quantitative predictors  
|         9.3.3	Fitting an interpreting the model  
|    9.4	Models with group-dependent intercepts, but shared slopes  
|        9.4.1	Description of the model  
|         9.4.2	Fitting and interpreting the model  
|        9.4.3	Interpreting group effects in the presence of shared (non-zero) slopes  
|    9.5	Models with group-dependent slopes and intercepts  
|         9.5.1	Description of the model  
|         9.5.2	Fitting and interpreting the model  
|         9.5.3	Interpreting group effects in the presence of varying slopes  
|     9.6	Answering our research questions: Interim discussion  
|     9.7	Data and research questions: Updated  
|     9.8	Models with intercepts and slopes for each level of a grouping factor 
|         (i.e. ‘random slopes’)  
|         9.8.1	Description of the model  
|         9.8.2	Fitting and interpreting the model  
|     9.9	Models with multiple predictors for each level of a grouping factor  
|         9.9.1	Description of the model  
|        9.9.2	Fitting and interpreting the model  
|         9.9.3	Model selection  
|     9.10	Answering our research questions: Updated  
|         9.10.1	A word on causality  
|     9.11	Exercises  
|     9.12	References  

| 10	Logistic regression and signal detection theory models  
|     10.1	Chapter pre-cap  
|     10.2	Dichotomous variables and data  
|     10.3	Generalizing our linear models  
|     10.4	Logistic Regression  
|         10.4.1	Logits  
|         10.4.2	The inverse logit link function  
|         10.4.3	Building intuitions about logits and the inverse logit function  
|     10.5	Logistic regression with one quantitative predictor  
|         10.5.1	Data and research questions  
|         10.5.2	Description of the model  
|         10.5.3	Fitting the model  
|         10.5.4	Interpreting the model  
|         10.5.5	Using logistic models to understand classification  
|         10.5.6	Answering our research question  
|     10.6	Measuring sensitivity and bias  
|         10.6.1	Data and research questions  
|         10.6.2	Description of the model  
|         10.6.3	Fitting and interpreting the model  
|         10.6.4	Answering our research questions  
|     10.7	Exercises  
|     10.8	References  
  
| 11	Multiple quantitative predictors, dealing with large models, and Bayesian ANOVA  
|     11.1	Chapter pre-cap  
|     11.2	Models with multiple quantitative predictors  
|     11.3	Interactions between quantitative predictors  
|         11.3.1	Data and research questions  
|         11.3.2	Description of the model  
|         11.3.3	Fitting the model  
|         11.3.4	Advantages of Bayesian multilevel models for large models  
|     11.4	Bayesian Analysis of Variance  
|        11.4.1	Getting the standard deviations from our models ‘manually’  
|         11.4.2	Using the banova function  
|        11.4.3	Fitting and comparing the reduced model  
|    11.5	A logistic regression model with multiple quantitative predictors  
|         11.5.1	Data and research questions  
|         11.5.2	Description of the model  
|         11.5.3	Fitting and the model and applying a Bayesian ANOVA  
|         11.5.4	Categorization in two dimensions  
|         11.5.5	Model selection and misspecification  
|     11.6	Exercises  
|     11.7	References  
  
      
| 12	Multinomial and Ordinal regression  
|     12.1	Chapter pre-cap  
|     12.2	Multinomial logistic regression  
|        12.2.1	Multinomial logits and the softmax function  
|        12.2.2	Comparison to logistic regression  
|        12.2.3	Data and research questions  
|         12.2.4	Description of our model  
|         12.2.5	Fitting the model  
|         12.2.6	Interpreting the model  
|         12.2.7	Multinomial models and territorial maps  
|         12.2.8	Refitting the model without speaker random effects  
|         12.2.9	Answering our research questions  
|     12.3	Ordinal (logistic) regression  
|         12.3.1	Cumulative distribution functions  
|         12.3.2	Data and research questions  
|         12.3.3	Description of the model  
|         12.3.4	Fitting and interpreting the model  
|         12.3.5	Listener-specific discrimination terms  
|         12.3.6	Answering our research questions  
|     12.4	Exercises  
|     12.5	References  
  
| 13	Writing up experiments: An investigation of the perception of apparent speaker |        characteristics from speech acoustics  
|     13.1	Introduction  
|         13.1.1	Fundamental frequency and voice pitch  
|         13.1.2	Variation in fundamental frequency between speakers  
|         13.1.3	Voice resonance and vocal-tract length  
|         13.1.4	Estimating vocal-tracts length from speech  
|         13.1.5	Variation in vocal-tract length between speakers  
|         13.1.6	Perception of age, gender and size  
|         13.1.7	Category-dependent behavior  
|         13.1.8	The current experiment  
|     13.2	Methods  
|         13.2.1	Participants  
|         13.2.2	Stimuli  
|         13.2.3	Procedure  
|         13.2.4	Data screening  
|         13.2.5	Loading the data and packages  
|         13.2.6	Statistical Analysis: Apparent height  
|         13.2.7	Statistical Analysis: Apparent gender  
|     13.3	Results: Apparent height judgments  
|     13.4	Discussion: Apparent height  
|         13.4.1	Age-dependent use of VTL cues on apparent height  
|         13.4.2	The effect for apparent gender on apparent height  
|     13.5	Conclusion: Apparent height judgments  
|     13.6	Results: Apparent gender judgments  
|     13.7	Discussion: Apparent gender judgments  
|         13.7.1	Effect of apparent age on the perception of femaleness  
|         13.7.2	Between-listener variation in gender perception  
|         13.7.3	Beyond gross acoustic cues in gender perception  
|    13.8	Conclusion: Apparent gender  
|    13.9 Conclusion  
|         13.9.1	Research design, variable selection, etc.  
|         13.9.2	Non-linear models  
|         13.9.3	Other data distributions  
|         13.9.4	Multivariate analyses  
|         13.9.5	Causality and reaching conclusions  
|         13.9.9	References  
  

\newpage
  
# Preface {-}

This book presents an introduction to the statistical analysis of repeated measures data using Bayesian multilevel regression models. Our approach is to fit these models using the `brms` package and the *Stan* programming language in R. This book introduces mathematical and modeling concepts in plain English, and focuses on understanding the visual/geometric consequences of different regression model structures rather than on rigorous mathematical explanations of these. 

Statistical modeling is as much a coding challenge as it is a mathematical challenge. As any programmer with some experience knows, copying existing scripts and modifying them slightly is an excellent way to learn to code, and often a new skill can be learned shortly after an understandable example can be found. To that end, rather than use a different toy data set for every new topic introduced, this book presents a set of fully worked analyses involving increasingly complicated models fit to the same experimental data. 

We were both trained as linguists and the experiment we analyze in this book is an experiment investigating a 'linguistic' research question (kind of). However, the sorts of models described in this book are useful for researchers in psychology, cognitive science, and many related (and unrelated) disciplines. In general, the information in this book will be useful to anyone that has similar sorts of data that they want to analyze regardless of the specifics of their research areas, and it should be straightforward to extend the concepts outlined in this book to models that predict different sorts of data.

## Bayesian Multilevel models and repeated measures data {-}

A more complete explanation of the following is presented in chapters 2 to 4, however, we can say something about this here. A Bayesian model is one which bases reasoning on *posterior probabilities* rather than *likelihoods*. A multilevel model is one in which you simultaneously model variation in your data, and in your parameters. There a 'multi(ple) levels' because there is variation in the data (conceptually 'below'), and in the parameters of the probability distributions generating this data (conceptually 'above'). A Bayesian multilevel model puts these concepts together and uses posterior probabilities to make inferences about variation in data and in parameters. 

Repeated measures data is data where multiple observations come from the same 'source' (discussed in more detail in section \@ref(c4-multilevel)). Basically, any time you have data with more than one observation from any given source, you have repeated measures data. Repeated measures data is very common in linguistics, and the norm in many areas of research. For example, you might ask participants in an experiment to hit a button as soon as they hear a buzz, and repeat this for 100 trials per participant. Repeated measures data can naturally lead to independent data-level variation (the lower level) and parameter level variation (the upper level). For example, reaction times to hit a button vary within-participants (the data/lower level), but average reaction times will also likely vary between participants (the upper/parameter level) in a systematic manner. Using multilevel models to analyze repeated measures data lets you independently model these two levels of variation. Using a *Bayesian* multilevel model allows you to build flexible models that provide you with all sorts of useful information. 

One obstacle to the proper analysis of repeated measures data is that this requires models that are relatively 'complicated' and therefore usually not taught at the beginner level. In order to learn how to model repeated measures data, a student is often first expected to learn ‘traditional’ statistical approaches and designs. After this foundation is laid, the student can then move to the sorts of models they can *actually* use for their work, mainly multilevel models that can handle repeated measures data. This approach has several drawbacks: It takes a long time, spends substantial energy on statistical approaches that most will rarely if ever use in their work, and front-loads difficult and abstract concepts before students can start working with data they really understand. As a result, students may become discouraged, become convinced that they’re ‘not good at math’, and may not realize how much they already intuitively understand about statistics.

This book starts with multilevel models and doesn't look back. It focuses on a realistic and complicated data set, and focuses from (nearly) the start (chapter 4) on realistic models that could actually be used in modern, publishable research.

## Whats missing from this book {-}

There was a time when every other Spider-Man movie started with Peter Parker being bitten by the spider and getting his powers, costume, etc. Filmmakers didn't trust that viewers know how Spider-Man got his powers, or that they could easily get this information somewhere, and so chose to spend precious movie minutes retelling what is perhaps the least interesting Spider-Man story. The problem is, the longer you spend on Peter Parker getting bit, the less time you can spend on Spider-Man swinging between buildings at high velocity. In the same way, many statistics books tell the statistics 'origin story' over and over, to the detriment of getting to the sorts of really interesting models people actually need. In other cases, when complicated models are discussed they are presented in a style and language only appropriate for an advanced reader, or with simple examples using toy data. 

This book tries to find the 'Goldilocks zone' between too much and too little information: We assume you know who Spider-Man is or can easily find his origin story. This book omits a basic introduction to R in addition to a detailed explanation of how the code used in the book works. It also omits a lot of explanation that is required to 'really' understand topics, for example correlation, which we introduce but only spend a few pages on. However, there are hundreds or thousands of excellent introductions to R, and even more places to find good information about basic statistical concepts like correlations. Rather than spend precious book pages on really getting into every topic that comes up, this book aims to spend as much time as possible flying between buildings at high velocity, or the statistical equivalent.

As a result, this book is narrowly focused on a specific subject: Introduction to Bayesian multilevel models for repeated measures data. It's not a general introduction to statistics nor to Bayesian models, or any number of other things. This narrow(ish) focus is both a strength and a weakness. We feel that it is a strength because it allows this book to cover material and provide examples of the sorts of models that are frequently needed in many disciplines, but also rarely discussed in statistics textbooks (often missing entirely or featuring in 1-2 chapters tucked away at the back). It's a weakness because it means the book is to some extent 'incomplete' in terms of providing a full introduction to the field of (Bayesian) statistics, as noted above. As a result, the ideal reader will know a little about statistics or have some resources on introductory statistics on hand (e.g. books, Wikipedia, etc.) to look up things they might not know or understand. Similarly, the book does not spend much time explaining how the things we do in R work, though examples of everything are given and the code is commented and made as transparent as we could. 

At the other end of the spectrum, this book is also missing many 'more complicated' but useful topics. For example, the book is entirely focused on linear modeling and does not discuss non-linear modeling. We also do not discuss missing data, multivariate dependent variables, or transformations of the dependent variables, among other topics. However, all of these things, and more, are easily doable using *Stan* and `brms`. 

## Statistics as Procedural knowledge {-}

Although statistical knowledge might seem like declarative knowledge, in many ways it is much more similar to procedural knowledge. You would never read a chapter from a French textbook once and expect to have memorized all the vocabulary and irregular forms. Similarly, you would never practice a piano piece a single time and assume that you are just 'bad at the piano' because you can't play it flawlessly. And yet a student may read a chapter from this book once and feel disappointed that they do not already understand the concepts.

We suggest thinking of acquiring statistical knowledge like learning a language, or musical instrument. It is normal, and in fact should be expected, that the reader will need to read some parts of the text multiple times, and *practice*, before being able to really *understand* all of the concepts presented here. We do not think getting good at statistics is about 'brain power' (whatever that is), as much as it is about a desire to learn, a genuine interest in the topic, and perseverance. In this way, learning statistics is very much like learning a language or an instrument. Buying a piano doesn't make you good at the piano, only practice does. Reading a grammar of Spanish or even moving to Spain will not teach you Spanish, only getting out there and talking to people will. To get good at statistics, you need to practice using statistics. 

To that end, this book provides examples of analyses and data that can be re-analyzed in many similar yet different ways. As a result, readers have an opportunity to fit several parallel models, interpret them, make sure they understand them, and so on. In doing so, and in returning to challenging content periodically to 'practice', we hope that readers will be able to support their understanding of the content in the book. 

### Practice vs brain power {-}

John von Neumann was perhaps the greatest mathematical mind the world has ever seen. A glance at his contributions to mathematics on his Wikipedia page reveals an astonishing breadth and depth of mathematical abilities. Some quotes from his contemporaries about von Neumann (from his Wikipedia page):

  * "I have sometimes wondered whether a brain like von Neumann's does not indicate a species superior to that of man" - Hans Bethe 

  * "Johnny was the only student I was ever afraid of. If in the course of a lecture I stated an unsolved problem, the chances were he'd come to me at the end of the lecture with the complete solution scribbled on a slip of paper." - George Pólya 

And yet when a graduate student complained about not *understanding* some mathematical abstraction, von Neumann is said to have replied:

  * "Young man, in mathematics you don't *understand* things. You just get *used to them*" - John von Neumann

This was von Neumann's experience, it has certainly been our experience, and it will likely be yours. Some things will make no sense the first, second, third, ..., maybe even the tenth time you see them. And then one day they will. It won't be clear when or why they changed from confusing to sensible, but all of the sudden a combination of repetition, practice and *time* will make the difference. It is likely that no amount of thinking and raw brain power alone will help you *understand* statistics right away on a first pass. 

That being said, many of the things we talk about in this book will come up in every chapter, so if things don't all make sense right away that's fine, since you will have plenty of chances to *get used to them*. Things will make more sense bit by bit as we learn how to use more and more complicated models. After reading a few chapters you should come back and read earlier chapters again (and again). You may notice that a lot of things are discussed in an earlier chapter that you did not notice the first time you read it. 

## How to use this book {-}

The chapters of this book, from chapter 3 on, are organized in terms of regression model components, e.g., intercepts, slopes, interactions, 'random' effects, and so on. We will discuss how these relate to experimental designs, statistical concepts, and the geometry of figures based on the data and model coefficients. In each chapter, we will learn how to use these components to ‘build’ progressively more complicated models to answer more complicated research questions. An analogy may be drawn to learning to be a carpenter. A chair is usually not carved out of a single block of wood, but rather assembled out of several discrete components (i.e., the flat seat, the cylindrical legs, supports for stability, etc.). As a result, learning to build a chair requires that the carpenter first learn to make the individual components and then learns to put them together in a specific way. 

The example analyses in each chapter are presented with the general structure of a lab report or academic write up. This is done as a pedagogical tool to help readers ‘copy’ the work they see in each chapter and modify this to suit their needs. The general structure for the presentation of new model components: 1) Introduce a type of research question (e.g., are the means of these groups the same?). Explain how this relates to model design concepts and give an example of real-world data associated with this sort of design. 2) Present the structure of a model that can be used to analyze the data, and to answer this sort of research question. Fit the model. 3) Interpret the model output and coefficients. Explain what all the information presented in the model output means, explain how coefficients relate to our research question. 4) Beyond coefficient values, discuss what the model ‘means’ and attempt to provide an answer to our research questions based on the model. These chapter components correspond approximately to the 1) introduction, 2) methods, 3) results, and 4) discussion/conclusion sections found in a typical research paper.  

Since we think learning statistics requires practice and repetition, this book is intended to allow readers to follow along with the analyses in the book, and to modify these to fit and interpret their own models. A possible sequence might be: 1) Repeat the exact analyses in a chapter, 2) Modify the model in the chapter slightly and interpret it, 3) Fit new data using a model structure like that included in each chapter. Of course, step two above can be extended indefinitely for most chapters because in each case we present only a very small set of the very large number of possible models that could be fit to the data in each chapter. 

### Supplemental Resources {-}

The code needed to follow along with all the analyses presented in the book is provided in each chapter. There is also a book website that presents an online version of the book, and the code necessary to make all the plots in the book. Finally, the book GitHub page contains `.Rmd` files containing all the code chunks, and the code to make the figures in the book. The book GitHub page also contains all the models referred to in the book, which can be downloaded using the `bmmb::get_models` function.

## Our target audience {-}

Although we think no statistical background is needed to use this book, readers with some statistical background will be in a better position to take full advantage. This book does assume a basic familiarity with R. However, the book provides fully worked examples of all analyses (including the scripts to generate all figures) so that readers only need to know enough R to follow along. We identify a few (non-exhaustive) types of people who might get good use out of this book: The self-starter, the convert, and the instructor. 

### The self-starter {-}

The self-starter is a person interested in multilevel models, who has little to no background in statistics, and perhaps little to no knowledge of R. However, the self-starter enjoys learning on their own, and is motivated to use Wikipedia, Stack overflow, Google, and so on in order to supplement the information in this book. In particular, self starters may benefit by 'going along for the ride' to some extent, and focusing on practicing and working through examples without expecting to *understand* everything the first time. 

### The convert {-}

Converts are readers who are already proficient with more 'traditional' analysis methods, and may want to ‘translate’ their skills over to a Bayesian framework. As much as possible, this book adopts the jargon of more 'traditional' methods, and we also provide explicit comparisons with other sorts of models at the end of several chapters. If this sort of reader is reasonably familiar with R, and in particular if they are familiar with the `lme4` package, the content and examples in this book should be very accessible.  

### The instructor {-}

Linguistics, and many disciplines with similar sorts of data, are in the early stages of a paradigm shift towards Bayesian statistical methods. Although we don't include many of the smaller exercises found in the typical statistics textbook, the book was written for use as a (semi) introductory book for a senior undergraduate or graduate statistics class. In addition, the data and scripts provided allow for a broad range of in class activities and out of class assignments to be easily created based on the topics covered in each chapter. The exercises suggested at the end of each chapter involve the analysis and interpretation of models similar in structure to what is presented in each chapter. The result of this is that the exercises we suggest resemble the actual analyses that students will need to carry out when they eventually analyze their own data. 

For example, students can be asked to replicate an analysis from a chapter but to make some modification, analyzing a different dependent variable or re-parameterizing the model in some way. Because of the open-ended nature of the data exploration and the incredible customizability of Bayesian multilevel models, assignments using the same data set and analysis scripts can easily vary from very simple to quite sophisticated. In addition, since usable models are presented from the fourth chapter on, students can use the course to analyze their own repeated measures data, building and interpreting progressively more complicated models as the course progresses. 

## What you need installed to use this book {-}

In order to use this book, you need to install R, which you can get for free online. If your R installation is more than 1 year old you may want to consider updating R right now, and will also want to update R periodically. That's because some of the packages we will use in this book sometimes don't play nice with 'older' (relatively speaking) versions of R. If you're trying to do something and R crashes for no apparent reason, it may be time to update R and all your packages. You will also need to install (minimally) the `brms`, `devtools`, and `bmmb` packages. You can get the first two by running `install.packages('packageName')`. You can get the `bmmb` package from the package GitHub by running: 

```{R, eval = FALSE}
devtools::install_github("santiagobarreda/bmmb")
```

After installing `devtools` of course. We also recommend installing RStudio, an integrated development environment (IDE) for R. This is basically just software that makes it more convenient to use R, and it honestly *does* make it very convenient. Installing RStudio is only recommend it but we recommend it like we recommend indoor plumbing: After trying it we think you will not want to live without it. 

## Why go Bayesian? {-}

We do not use Bayesian statistics because of an affinity for Bayesian arguments regarding the philosophy of knowledge gathering, although we do think these topics are very interesting. Instead, we use Bayesian statistics, and suggest you should also use them, because it lets you do things that are difficult to do with other approaches, gives you information that other approaches don't, and gives you a flexibility and resilience that may be difficult to find elsewhere. Even though we think Bayesian modeling has its advantages, we are not like the proverbial man with the hammer, we do not think that every problem requires a Bayesian solution. Instead, you may find that the models in this book are best for some situations, and other sorts of models are best for other situations. 

That being said, the flexibility provided by Bayesian modeling is an enormous advantage, and it is difficult to find through other approaches. Returning to the woodworking analogy, learning to 'build' Bayesian models from their components lets you build exactly the 'furniture' (model) you want. In contrast, working with some traditional models feels more like going to Ikea, you can pick from a set of predetermined models, but are often constrained in terms of how these pre-built pieces can be modified. 

### Why brms? {-}

The `brms` package is a useful way for us to use the *Stan* programming language, the real star of the show. We could write our own models for all of the analyses presented in this book and fit them directly in *Stan*. However, `brms` will do this for us more quickly, more easily, and with fewer mistakes. It also writes highly efficient models that fit quickly in *Stan*. In addition, the helper functions in the package make working with the posterior samples very simple and work well with a wide range of other packages related to *Stan*. However, despite the fact that this book uses `brms` exclusively for model building and fitting, we don't really think of this book as being *about* `brms`. The information presented in this book applies to modeling more generally, and is also useful for people that write their own models directly in *Stan*. The main difference is these readers would need to do a lot of things 'by hand' or find other solutions to many of the things the `brms` package makes it very simple to do. 

## It takes a village (of books) {-}

As noted above, this book omits some basic information that can be found in many places online. It also does not get into some more-complicated topics or in-depth explanations of some aspects of statistical modeling more generally. We recommend the following books to provide information that readers may want, but that is not present in this book.

*Before this book*

These books introduce more 'classic' (non-Bayesian) models and traditional statistical approaches. Such models are usually not multilevel models and often cannot handle repeated measures data.  

Myers, J. L., Well, A. D., & Lorch, R. F. (2013). Research design and statistical analysis. Routledge.

Winter, B. (2019). Statistics for linguists: An introduction using R. Routledge.

*During this book* 

These books provide a lot of additional information about Bayesian statistics and modeling in general, and discuss many things not covered in this book. However, they spend very little time on repeated measures data.

Kruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan.

Gelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press.

Gelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.

*After this book* 

If you finish this book and think most of it made sense, you may find these books useful.

McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.

Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis.


## Acknowledgements {-}

We would like to thank Sunny Zhou, Scott Perry, and Michelle Cohn, for their helpful comments and suggestions. I (Santiago) would also like to thank Terry Nearey for introducing Noah and me, and for always insisting that things be done properly. I would also like to thank my parents for too many things to list, my daughters for keeping me company while I worked on the book, and my wife for all her encouragement and support; the book wouldn't exist without her.  

